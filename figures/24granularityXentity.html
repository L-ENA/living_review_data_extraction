<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <title>EPPI-Mapper</title>
  <style>
    *, *:before, *:after {
  box-sizing: border-box;
}
*:focus-visible {
  outline: 4px solid #000000;
}
html {
  box-sizing: border-box;
  font-family: 'Roboto', sans-serif;
  font-size: 14px;
  text-rendering: optimizeLegibility;
  -moz-osx-font-smoothing: grayscale;
  line-height: 1.5;
}
body {
  background-color: #ffffff;
  margin: 0;
  overflow: hidden;
}
a {
  color: #0275d8;
}
.clearfix:after {
  visibility: hidden;
  display: block;
  font-size: 0;
  content: " ";
  clear: both;
  height: 0;
}
* html .clearfix {
  zoom: 1;
}
*:first-child + html .clearfix {
  zoom: 1;
}
.loader {
  position: absolute;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: #000000;
  z-index: 1000;
  transition: all 2s ease;
}
.loader span.spinner {
  animation: spin 1.2s linear infinite;
  border: 5px solid #ffffff;
  border-bottom-color: #000000;
  border-top-color: #000000;
  border-radius: 100%;
  display: inline-block;
  width: 40px;
  height: 40px;
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
}
.header {
  padding: 0 18px;
}
.header-basic {
  background: #ffffff;
  padding: 0 18px;
  display: flex;
  font-size: 1.5rem; 
  font-weight: bold;
  height: 108px;
  padding: 8px 8px 0;
  text-align: center;
}
.header-basic .header-title {
  flex-grow: 1;
}
.header-basic .header-title span {
  position: relative;
  top: 30%;
}
.header-basic img {
  height: 100px;
}
.header-basic img:nth-child(1) {
  float: left;
}
.header-basic img:nth-child(2) {
  float: right;
}
.wrapper {
  margin: 8px;
}
.pivot-table {
  font-size: 11px;
  font-weight: normal;
  width: 100%;
}
.pivot-table .top-head, .pivot-table .side-head, .pivot-table .body,
.pivot-table .text-wrapp {
  display: inline-block;
  overflow: hidden;
}
.pivot-table .side-head, .pivot-table .body {
  float: left;
}
.pivot-table table {
  border: 0;
  border-collapse: collapse;
}
.pivot-table table thead th {
  border: 1px solid #ffffff;
  color: #ffffff;
  font-weight: normal;
  max-width: 103px;
  min-width: 103px;
  width: 103px;
  padding: 4px;
  overflow: hidden;
  text-overflow: ellipsis;
  vertical-align: top;
  text-align: left;
}
.pivot-table.text-view table thead th:not(.collapsed) {
  max-width: 153px;
  min-width: 153px;
  width: 153px;
}
.pivot-table.text-view-large table thead th:not(.collapsed) {
  max-width: 203px;
  min-width: 203px;
  width: 203px;
}
.pivot-table table tbody th, .pivot-table table tbody td {
  border: 1px solid #fff;
}
.pivot-table table tbody th {
  color: #ffffff;
  font-weight: normal;
  min-height: 103px;
  height: 103px;
  max-width: 103px;
  min-width: 0;
  width: 0;
  padding: 4px;
  overflow: hidden;
  text-overflow: ellipsis;
  vertical-align: top;
  text-align: left;
}
.pivot-table.text-view table tbody th:not(.collapsed) {
  min-height: 153px;
  height: 153px;
  max-width: 153px;
}
.pivot-table.text-view-large table tbody th:not(.collapsed) {
  min-height: 203px;
  height: 203px;
  max-width: 203px;
}
.pivot-table table tbody td {
  min-width: 103px;
  width: 103px;
  min-height: 103px;
  height: 103px;
}
.pivot-table.text-view table tbody td {
  min-width: 153px;
  width: 153px;
  min-height: 153px;
  height: 153px;
}
.pivot-table.text-view-large table tbody td {
  min-width: 203px;
  width: 203px;
  min-height: 203px;
  height: 203px;
}
.body table tbody td.cell div.data-wrapper {
  position: relative;
  background-color: #eeeeee;
  cursor: pointer;
  justify-items: center;
  align-items: center;
  width: 100%;
  height: 100%;
  overflow: hidden;
  -webkit-transition: all 0.5s;
  -moz-transition: all 0.5s;
  -moz-transition: all 0.5s;
  -ms-transition: all 0.5s;
  -o-transition: all 0.5s;
  transition: all 0.5s;
  flex-wrap: wrap;
  align-items: center;
  justify-content: space-around;
}
.body table tbody td.cell div.data-wrapper div.break {
  flex-basis: 100%;
  height: 0;
}
.body table tbody td.cell div.pie-wrapper {
  position: relative;
  background-color: #eeeeee;
  cursor: pointer;
  display: none;
  width: 100%;
  height: 100%;
  overflow: hidden;
}
.body table tbody td.cell div.mosaic-wrapper {
  position: relative;
  background-color: #eeeeee;
  cursor: pointer;
  display: none;
  width: 100px;
  height: 100px;
  max-width: 100px;
  max-height: 100px;
  overflow: hidden;
}
.body table tbody td.cell div.pie-wrapper div.pie,
.body table tbody td.cell div.pie-wrapper div.pie-hole{
  position: absolute;
  top: 50%;
  left: 50%;
  background-size: cover;
  border-radius: 100%;
  transform: translate(-50%, -50%);
}
.body table tbody td.cell div.pie-wrapper div.pie-hole{
  background-color: #dddddd;
}
.body table tbody td.cell.none div.data-wrapper,
.body table tbody td.cell.none div.pie-wrapper,
.body table tbody td.cell.none div.mosaic-wrapper {
  cursor: not-allowed !important;
}
.pivot-table.text-view .body table tbody td.cell div.data-wrapper div.data {
  display: flex;
  gap: 4px;
  align-items: center;
  padding: 2px;
}
.pivot-table.text-view .body table tbody td.cell div.data-wrapper div.data span.count {
  background: #ffffff;
  border: 1px solid #000000;
  border-radius: 2px;
  color: #000000;
  padding: 2px;
}
.controls {
  display: inline-block;
  position: absolute;
}
.ui-segment {
  background-color: #ffffff;
  color: #0275d8;
  border: 1px solid #0275d8;
  border-radius: 4px;
  display: inline-block;
}
.ui-segment span.option.active {
  background-color: #0275d8;
  color: #ffffff;
}
.ui-segment span.option {
  font-size: 13px;
  padding-left: 23px;
  padding-right: 23px;
  height: 25px;
  text-align: center;
  display: inline-block;
  line-height: 25px;
  margin: 0;
  float: left;
  cursor: pointer;
  border-right: 1px solid #0275d8;
  transition: all 0.5s ease;
}
.ui-segment span.option:last-child {
  border-right: none;
}
.segment-select{
  display: none;
}
.footer {
  position: absolute;
  bottom: 0;
  background-color: #ffffff;
  color: rgba(0, 0, 0, 0.87);
  height: 36px;
  min-height: 36px;
  padding: 16px;
  width: 100%;
  -webkit-box-align: center;
  -ms-flex-align: center;
  align-items: center;
  display: -webkit-box;
  display: -ms-flexbox;
  display: flex;
  -webkit-box-flex: 0 !important;
  -ms-flex: 0 1 auto !important;
  flex: 0 1 auto !important;
}
.footer .inner svg {
  display: inline-block;
  vertical-align: middle;
}
.footer .legend .dot {
  border-radius: 100%;
  display: inline-block;
  height: 10px;
  width: 10px;
  margin: 2px 3px 0 6px;
}
.footer .legend .label {
  font-size: 1.0em;
}
.legend-tooltip {
  border: 1px solid #cccccc;
  display: block;
  position: absolute;
  width: 300px;
  background: #ffffff;
  color: #000000;
  padding: 6px 8px;
  border-radius: 4px;
  -webkit-box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  -moz-box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  z-index: 2;
}
.spacer {
  -webkit-box-flex: 1 !important;
  -ms-flex-positive: 1 !important;
  flex-grow: 1 !important;
}
.hide {
  display: none;
}
.tooltip {
  display: none;
  position: absolute;
  border: 1px solid #cccccc;
  background-color: #ffffff;
  border-radius: 3px;
  padding: 3px 6px 2px;
  -webkit-box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  -moz-box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  z-index: 2;
}
.tooltip .count {
  color: #0275d8;
  margin: 2px 3px;
}
.tooltip .count span {
  background-color: #0275d8;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  color: #ffffff;
  padding: 2px 4px;
}
.refs {
  font-size:  12px;
  color:  lightgrey;
  margin:  0;
  padding: 0 0 0 20px;
}
.veil {
  background: rgba(0, 0, 0, 0.7);
  display: none;
  position: absolute;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 99;
}
.veil.open {
  display: block;
}
.settings {
  position: absolute;
  top: 0;
  left: -600px;
  bottom: 0;
  background: #fff;
  color: #000000;
  padding: 0;
  width: 360px;
  z-index: 100;
}
.settings.open {
  left: 0;
}
.settings > div.title {
  color: #000000;
  font-size: 22px;
  padding: 20px;
  margin: 0 0 10px 0;
}
.settings > div.title > button.btnSettings {
  -webkit-border-radius: 36px;
  -moz-border-radius: 36px;
  border-radius: 36px;
  border: 1px solid #000000;
  background-color: inherit;
  color: #ffffff;
  background-color: #000000;
  display: inline-block;
  float: right;
  height: 36px;
  transition: all 0.25s ease;
  text-align: center;
  line-height: 36px;
  padding: 0 12px;
  letter-spacing: 1px;
}
.settings > div.title > button.btnSettings:hover {
  background-color: #ffffff;
  color: #000000;
  cursor: pointer;
}
.settings > div.title > button.btnSettings.busy{
  border: 3px solid #000000 !important;
  border-top-color: #ffffff !important;
  border-bottom-color: #ffffff !important;
  text-indent: -99999px;
  width: 36px;
  border-radius: 36px;
  animation: spin 1.2s linear infinite;
}
.settings > div.title > button.disabled {
  cursor: not-allowed;
  pointer-events: none;
  background: unset;
  color: #000000;
  opacity: 0.3;
  text-decoration: none;
}
.settings > div.title > button.right {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
  margin-left: 2px;
}
.settings > div.title > button.left {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.settings div.filter-type-wrapper {
  background-color: #efefef;
  position: absolute;
  top: 76px;
  overflow: auto;
  padding: 0 10px 0 0;
  width: 100%;
  border-top: 1px solid #000000;
}
.settings div.filter-wrapper {
  background-color: #efefef;
  position: absolute;
  top: 226px;
  bottom: 0;
  overflow: auto;
  padding: 0 10px 0 0;
  width: 100%;
}
.settings div.filter-type-wrapper h2,
.settings div.filter-wrapper h2 {
  margin-left: 20px;
  margin-bottom: 10px;
}
.settings div.filter-type-wrapper ul,
.settings div.filter-wrapper ul {
  list-style: none;
  margin: 0 0 0 16px;
  padding: 0;
}
.settings div.filter-type-wrapper ul li,
.settings div.filter-wrapper ul li {
  margin: 0;
  padding: 3px 5px;
}
.settings div.filter-type-wrapper ul li:hover,
.settings div.filter-wrapper ul li:hover {
  cursor: pointer;
}
.settings div.filter-type-wrapper ul li span,
.settings div.filter-type-wrapper ul li svg,
.settings div.filter-wrapper ul li span,
.settings div.filter-wrapper ul li svg {
  display: inline-block;
  vertical-align: middle;
}
.settings div.filter-type-wrapper ul li svg,
.settings div.filter-wrapper ul li svg {
  display: none;
}
.settings div.filter-type-wrapper ul li.checked svg#checked,
.settings div.filter-wrapper ul li.checked svg#checked {
  display: inline-block;
}
.settings div.filter-type-wrapper ul li.unchecked svg#unchecked,
.settings div.filter-wrapper ul li.unchecked svg#unchecked {
  display: inline-block;
}
.settings div.filter-type-wrapper ul li.indeterminate svg#indeterminate,
.settings div.filter-wrapper ul li.indeterminate svg#indeterminate {
  display: inline-block;
}
.reader,
.accessibility {
  position: absolute;
  top: 0;
  right: -1000px;
  bottom: 0;
  background: #fff;
  color: #000;
  padding: 0;
  width: 1000px;
  z-index: 100;
}
.reader.open,
.accessibility.open {
  right: 0;
}
.reader > div.title,
.accessibility > div.title {
  background-color: #ffffff;
  color: #000000;
  display: flex;
  justify-content: space-between;
  font-size: 22px;
  padding: 20px;
}
.reader > div.title > div,
.accessibility > div.title > div {
  display: flex;
}
.reader > div.title > div:nth-child(1),
.accessibility > div.title > div:nth-child(1) {
  flex-grow: 1;
}
.reader > div.title > div:nth-child(1) > button,
.accessibility > div.title > div:nth-child(1) > button {
  margin-right: 18px;
}
.reader > div.title > div > input {
  border: 1px solid #000000;
  background: #ffffff;
  padding: 8px 16px;
  color: #000000;
  float: right;
  font-size: 16px;
  transition: all 0.5s ease;
  margin-right: 6px;
}
.reader > div.title > div > input:focus {
  background: #ffffff;
  color: #000000;
  box-shadow: 0 0 10px #719ECE;
}
.reader > div.title > div > input::-webkit-input-placeholder,
.reader > div.title > div > input::-moz-placeholder,
.reader > div.title > div > input:-ms-input-placeholder,
.reader > div.title > div > input:-moz-placeholder { 
  color: #595959;
}
.reader > div.title > div > select {
  border: 1px solid #000000;
  background: #ffffff;
  padding: 7.5px 16px;
  color: #595959;
  font-size: 16px;
  transition: all 0.5s ease;
  cursor: pointer;
  margin-right: 6px;
}
#filter-options option {
  color: #000000;
}
#filter-options option[value=""][disabled] {
  display: none;
}
.reader > div.content,
.accessibility > div.content {
  background: #ffffff;
  display: flex;
  align-content: stretch;
  align-items: stretch;
  height: 92.4%;
}
.reader > div.content > div.reader-filter {
  background-color: #efefef;
  border-right: 1px solid #ffffff;
  overflow: auto;
  width: 200px;
  min-width: 200px;
  max-width: 200px;
}
.reader > div.content > div.filter-opts {
  background-color: #efefef;
  border-right: 1px solid #ffffff;
  overflow: auto;
  width: 200px;
  min-width: 200px;
  max-width: 200px;
  float: right;
}
.settings > div.filter-wrapper > div.controlTainer {
  align-content: center;
  padding: 10px;
}
.reader > div.content > div.reader-filter ul {
  list-style: none;
  margin: 0;
  padding: 0;
  width: 100%;
  overflow: hidden;
}
.reader > div.content > div.reader-filter > ul > ul {
  padding: 0 0 0 12px;
 }
.reader > div.content > div.reader-filter ul li:hover {
  cursor: pointer;
}
.reader > div.content > div.reader-filter ul li {
  padding: 3px 6px 2px;
  display: inline-flex;
  align-items: end;
  width: 100%;
}
.reader > div.content > div.reader-filter ul li span,
.reader > div.content > div.reader-filter ul li svg {
  display: inline-block;
  vertical-align: middle;
}
.reader > div.content > div.reader-filter ul li svg {
  display: none;
  width: 24px;
  min-width: 24px;
}
.reader > div.content > div.reader-filter ul li span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}
.reader > div.content > div.reader-filter ul li.checked svg#checked {
  display: inline-block;
}
.reader > div.content > div.reader-filter ul li.unchecked svg#unchecked{
  display: inline-block;
}
.reader > div.content > div.reader-filter ul li.indeterminate svg#indeterminate {
  display: inline-block;
}
.reader > div.content > div.nav {
  background-color: #efefef;
  border-right: 1px solid #ffffff;
  overflow: auto;
  width: 280px;
  min-width: 280px;
  max-width: 280px;
}
.reader > div.content > div.navTainer {
  background-color: #efefef;
  border-right: 1px solid #ffffff;
  overflow: auto;
  width: 280px;
  min-width: 280px;
  max-width: 280px;
}
.reader > div.content > div.navTainer .navGroupSelect {
  color: #000000;
  border-top: 1px solid #000000;
  padding: 6px 10px 4px;
}
.reader > div.content > div.navTainer .ref-sort-order {
  color: #000000;
  border-top: 1px solid #000000;
  border-bottom: 1px solid #000000;
  padding: 6px 10px 4px;
}
.reader > div.content > div.navTainer > div.nav > ul,
.reader > div.content > div.navTainer > div.nav > ul.segmented,
.reader > div.content > div.navTainer > div.nav > ul.segmented > li > ul {
  list-style: none;
  margin: 0;
  padding: 0;
}
.refMenuItem {
  padding: 0;
  margin: 0;
}
.refMenuItem > button {
  color: #000000;
  background-color: transparent;
  border: 1px solid transparent;
  border-bottom-color: #ffffff;
  display: block;
  font-size: 1rem;
  padding: 6px 16px 8px;
  text-align: left;
  max-width: 280px;
}
.refMenuItem > button:hover,
.refMenuItem > button:focus-visible {
  background-color: #ffffff;
  cursor: pointer;
}
.refMenuItem > button > div.title,
.refMenuItem > button > div.auth {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}
.refMenuItem > button > div.title {
  font-weight: bold;
}
.refMenuItem > button > div.auth,
.refMenuItem > button > div.date {
  color: #666666;
}
.refMenuItem > button > div.auth {
  font-style: italic;
}
.refMenuItem.selected > button {
  background-color: #000000;
  color: #ffffff;   
}
.refMenuItem.selected > button > div.auth,
.refMenuItem.selected > button > div.date {
  color: #cfcfcf;   
}
.reader > div.content > div.read,
.accessibility > div.content > div.read {
  flex-grow: 1;
  overflow: auto;
  padding: 10px 18px;
  border-top: 1px solid #000000;
}
.reader > div.content > div.read > h2,
.accessibility > div.content > div.read > h2 {
  margin-top: 0;
}
.reader > div.content > div.read > hr,
.accessibility > div.content > div.read > hr {
  background-color: #efefef;
  border: 0;
  height: 1px;
}
.reader > div.content > div.read > p,
.accessibility > div.content > div.read > p,
.accessibility > div.content > div.read > ul {
  font-size: 1.3rem;
}
.reader > div.content > div.read > div.meta-data {
  margin-top: 16px;
}
.reader > div.content > div.read > div.meta-data > div.meta-data-item > label {
  color: #aaaaaa;
  display: block;
  float: left;
  width: 140px;
  margin: 0 0 3px 0;
  padding: 3px 6px;
}
.reader > div.content > div.read > div.meta-data > div.meta-data-item > span {
  border: 1px solid #eeeeee;
  display: block;
  float: left;
  margin: 0 0 3px -140px;
  padding: 2px 4px 2px 144px;
  width: 100%;
}
.ref-sort-order {
  padding: 4px 8px;
}
.menu {
  background-color: #000000;
  border-bottom: 1px solid #000000;
  display: block;
  list-style: none;
  margin: 0;
  padding: 0;
}
.menu:after {
  content: "";
  display: table;
  clear: both;
}
.menu .menu-item {
  background-color: #000000;
  border: 0;
  color: #ffffff;
  display: flex;
  align-items: center;
  padding: 8px 12px;
  transition: all 0.2s ease-out;
  cursor: pointer;
  float: left;
  border: 0;
}
.menu .menu-item:active {
  background-color: #ffffff;
  color: #000000;
}
.menu .menu-item:active svg {
  fill: #000000;
}
.menu .menu-item:hover, .menu .menu-item:focus {
  background-color: #ffffff;
  color: #000000;
}
.menu .menu-item:hover svg, .menu .menu-item:focus svg {
  fill: #000000;
}
.menu .menu-item span {
  font-size: 14px;
  margin-left: 4px;
  overflow: hidden;
  word-break: keep-all;
}
.menu .menu-item svg {
  fill: #ffffff;
  display: inline-block;
  margin: 0 0 0 4px;
}
.menu .menu-item span.active-text,
.menu .menu-item svg.active-svg {
  display: none;
}
.menu .menu-item.active span.inactive-text,
.menu .menu-item.active svg.inactive-svg {
  display: none;
}
.menu .menu-item.active span.active-text,
.menu .menu-item.active svg.active-svg {
  display: block;
}
.menu .menu-item[aria-checked="true"] {
  background-color: #ffffff;
  color: #000000;
}
.menu .menu-item[aria-checked="true"] svg {
  fill: #000000;
}
.refMenuItemLegend {
  -webkit-border-radius: 100%;
  -moz-border-radius: 100%;
  border: 1px solid #ffffff;
  border-radius: 100%;
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 6px 3px 0;
  float: right;
}
.clickable-row,
.clickable-col {
  cursor: pointer;
}
.top-head th div,
.side-head th div {
  position: relative;
  width: 100%;
  height: 100%;
}
.top-head th div svg,
.side-head th div svg {
  display: none;
  background: rgba(255, 255, 255, 0.8);
}
.top-head th div svg,
.side-head th div svg {
  position: absolute;
  top: -4px;
  right: -4px;
}
.top-head th:not(.collapsed) div svg#arrowLeft,
.side-head th:not(.collapsed) div svg#arrowUp {
  cursor: pointer;
  display: inline-block;
}
.top-head th.collapsed div svg#arrowRight,
.side-head th.collapsed div svg#arrowDown {
  cursor: pointer;
  display: inline-block;
}
.top-head th.busy div svg#refresh,
.side-head th.busy div svg#refresh {
  border-radius: 100%;
  cursor: pointer;
  display: inline-block;
  animation: spin 1s linear infinite;
}
.btnColCollapse,
.btnRowCollapse {
  opacity: 40%;
}
.top-head th.collapsed {
  min-width: 0;
  max-width: 0;
  width: 0;
  padding: 4px 0;
}
.pivot-table .top-head th.collapsed.level-1 {
  min-width: 103px;
  width: 103px;
  padding: 4px;
}
.pivot-table.text-view .top-head th.collapsed.level-1 {
  min-width: 153px;
  width: 153px;
}
.pivot-table.text-view-large .top-head th.collapsed.level-1 {
  min-width: 203px;
  width: 203px;
}
.side-head th.collapsed {
  min-height: 0;
  max-height: 0;
  height: 0;
  padding: 0 4px;
}
.pivot-table .side-head th.collapsed.level-1 {
  min-height: 103px;
  height: 103px;
  padding: 4px;
}
.pivot-table.text-view .side-head th.collapsed.level-1 {
  min-height: 153px;
  height: 153px;
}
.pivot-table.text-view-large .side-head th.collapsed.level-1 {
  min-height: 203px;
  height: 203px;
}
.body td.collapsed-col {
  min-width: 18px;
  width: 18px;
}
.body td.collapsed-row {
  min-height: 18px;
  height: 18px !important;
}
.body td.collapsed-col,
.body td.collapsed-row {
  background: rgba(0, 0, 0, 0.2);
}
.body td.collapsed-col .mosaic-wrapper,
.body td.collapsed-row .mosaic-wrapper {
  width: auto !important;
  height: auto !important;
}
.body td.collapsed-col svg,
.body td.collapsed-row svg {
  display: none;
}
.body td.collapsed-col .pie-wrapper,
.body td.collapsed-col .mosaic-wrapper,
.body td.collapsed-col .data-wrapper,
.body td.collapsed-row .pie-wrapper,
.body td.collapsed-row .mosaic-wrapper,
.body td.collapsed-row .data-wrapper {
  opacity: 0 !important;
}
.top-head th.collapsed:not(.level-1) span,
.side-head th.collapsed:not(.level-1) span {
  display: none;
}
.top-head th.collapsed:not(.level-1),
.side-head th.collapsed:not(.level-1) {
  border-width: 0;
}
.top-head th.collapsed.first:not(.level-1) {
  border-left: 1px solid #ffffff !important;
}
.top-head th.collapsed.last:not(.level-1) {
  border-right: 1px solid #ffffff !important;
}
.side-head th.collapsed.first:not(.level-1) {
  border-top: 1px solid #ffffff !important;
}
.side-head th.collapsed.last:not(.level-1) {
  border-bottom: 1px solid #ffffff !important;
}
@keyframes spin {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}
.attribute-tooltip {
  position: absolute;
  border: 1px solid #cccccc;
  background-color: #ffffff;
  border-radius: 3px;
  display: none;
  padding: 3px 6px 2px;
  text-overflow: ellipsis;
  max-width: 350px !important;
  min-width: 200px !important;
  -webkit-box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  -moz-box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  box-shadow: 0 2px 10px 2px rgba(0, 0, 0, 0.3);
  z-index: 2;
}
.attribute-tooltip.show {
  display: block;
}
.attribute-tooltip .close-tooltip {
  cursor: pointer;
  float: right;
}
.attribute-tooltip .content {
  margin: 0 0 12px 0;
}
.attribute-tooltip h4 {
  margin: 0 0 8px 0;
}
.text-center {
  text-align: center;
}
.text-muted {
  color: #6c757d !important;
  font-size: 11px;
  margin: 10px 0 0 0;
}
#filterClearButton,
#codeFilterClearButton {
  float: right;
}
#codeFilterClearButton {
  width: 100%;
}
.btn {
  background-color: #000000;
  border: 1px solid #000000;
  color: #ffffff;
  display: block;
  font-weight: 400;
  padding: 8px 16px;
  font-size: 16px;
  line-height: normal;
  transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}
.btn:hover {
  color: #000000;
  background-color: inherit;
  border-color: #000000;
  cursor: pointer;
}
#status {
  display: none;
}
@keyframes spin {
  from {transform:rotate(0deg);}
  to {transform:rotate(360deg);}
}
.screenreader {
  position: absolute;
  left: -9999px;
}
.meta-data hr {
  border-top: 1px solid #000000;
}
#filterClearButton {
  padding: 6px 12px;
  border: 1px solid #000000;
  transition: all 0.25s ease;
  color: #ffffff;
  background-color: #000000;
}
#filterClearButton:hover {
  color: #000000;
  background-color: inherit;
}
#sgroup,
#RefSortOrder {
  border: 1px solid black;
  color: #595959;
}
#sgroup::placeholder,
#RefSortOrder::placeholder {
  color: #595959;
}
.style-picker {
  float: right !important;
}
  </style>
  <script type="text/javascript">
    var isIE = false || !!document.documentMode;

    if (isIE === true) {
      alert("Unfortunately this map cannot be used in Internet Explorer. You will need to use Microsoft Edge, Firefox or Google Chrome.");
    }
  </script>
</head>

<body>
  <p id="announce" class="screenreader" role="status" aria-live="polite"></p>
  <div class="attribute-tooltip">
    <span class="close-tooltip">X</span>
    <div class="content"></div>
  </div>
  <div class="loader">
    <span class="spinner"></span>
  </div>
  <div class="veil"></div>
  <div id="reader" class="reader" role="dialog" aria-labelledby="announce" style="display:none;"></div>
  <div id="accessibility" class="accessibility" role="dialog" aria-labelledby="announce" style="display:none;">
    <div class="title clearfix">
      <button class="btn close" aria-label="Close accessibility">X</button> <span>Accessibility</span>
    </div>
    <div class="content">
      <div class="read">
        <p>Web accessibility refers to the practice of designing and developing websites and web-based applications in a
          way that makes them usable and accessible to all individuals, including those with disabilities. This includes
          ensuring that web content can be easily understood, navigated, and interacted with by people with a wide range
          of abilities and disabilities, including those who use assistive technologies such as screen readers.</p>

        <p>This Eppi Mapper generated map conforms to the World Wide Web Consortium's Web Content Accessibility
          Guidelines (<a href="https://www.w3.org/WAI/standards-guidelines/wcag/" target="_blank">W3C WCAG</a>) Version
          2.1 Level AA to the maximum extent possible. These guidelines explain how to make web content more accessible
          for people with disabilities. The features in this map include:</p>

        <ul>
          <li>If the default settings have been used when generating this map, the overall colour design has been set to
            a more visually contrasting appearance.</li>
          <li>Links in the application have been converted to buttons to accommodate screen readers.</li>
          <li>Aria status updates now inform screen readers when modals open and selected records change.</li>
          <li>Tab focus is contained within opened modals.</li>
          <li>The style selection functionality has been moved to the top menu for easier access.</li>
          <li>A new style, Textual view, has been added to the map. This view presents the record counts within each
            node in a text readable format.</li>
          <li>URL links for individual records have been made descriptive.</li>
          <li>The map generation tool will inform the user if the selected background and font colour contrast meet
            accessibility requirements.</li>
        </ul>
        <br>
        <p><b>Limitations</b></p>
        <p>At the present time, while the keyboard focus does stay within each opened modal, the focus cannot be moved
          to individual checkboxes or radio buttons or, in the View Records modal, to all fields in the full record.
          This means that those items, along with the nodes in the map, can only be selected using a mouse or pointing
          device. In future updates we hope to address this limitation.</p>
      </div>
    </div>
  </div>
  <div class="menu" id="menu">
    <button class="menu-item menu-settings" role="button">
      <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18" viewBox="0 0 24 24">
        <path d="M0 0h24v24H0z" fill="none" />
        <path
          d="M19.43 12.98c.04-.32.07-.64.07-.98s-.03-.66-.07-.98l2.11-1.65c.19-.15.24-.42.12-.64l-2-3.46c-.12-.22-.39-.3-.61-.22l-2.49 1c-.52-.4-1.08-.73-1.69-.98l-.38-2.65C14.46 2.18 14.25 2 14 2h-4c-.25 0-.46.18-.49.42l-.38 2.65c-.61.25-1.17.59-1.69.98l-2.49-1c-.23-.09-.49 0-.61.22l-2 3.46c-.13.22-.07.49.12.64l2.11 1.65c-.04.32-.07.65-.07.98s.03.66.07.98l-2.11 1.65c-.19.15-.24.42-.12.64l2 3.46c.12.22.39.3.61.22l2.49-1c.52.4 1.08.73 1.69.98l.38 2.65c.03.24.24.42.49.42h4c.25 0 .46-.18.49-.42l.38-2.65c.61-.25 1.17-.59 1.69-.98l2.49 1c.23.09.49 0 .61-.22l2-3.46c.12-.22.07-.49-.12-.64l-2.11-1.65zM12 15.5c-1.93 0-3.5-1.57-3.5-3.5s1.57-3.5 3.5-3.5 3.5 1.57 3.5 3.5-1.57 3.5-3.5 3.5z" />
      </svg>
      <span>Filters</span>
    </button>
    <button class="menu-item menu-expand" role="button">
      <svg class="inactive-svg" xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18"
        viewBox="0 0 24 24">
        <path
          d="M15 21h2v-2h-2v2zm4 0h2v-2h-2v2zM7 21h2v-2H7v2zm4 0h2v-2h-2v2zm8-4h2v-2h-2v2zm0-4h2v-2h-2v2zM3 3v18h2V5h16V3H3zm16 6h2V7h-2v2z" />
        <path d="M0 0h24v24H0z" fill="none" />
      </svg>
      <svg class="active-svg" xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18"
        viewBox="0 0 24 24">
        <path
          d="M13 7h-2v2h2V7zm0 4h-2v2h2v-2zm4 0h-2v2h2v-2zM3 3v18h18V3H3zm16 16H5V5h14v14zm-6-4h-2v2h2v-2zm-4-4H7v2h2v-2z" />
        <path d="M0 0h24v24H0z" fill="none" />
      </svg>
      <span class="inactive-text">Hide Headers</span>
      <span class="active-text" aria-hidden="true">Show Headers</span>
    </button>
    <button class="menu-item menu-fullscreen" role="button">
      <svg class="inactive-svg" xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18"
        viewBox="0 0 24 24">
        <path d="M0 0h24v24H0z" fill="none" />
        <path d="M7 14H5v5h5v-2H7v-3zm-2-4h2V7h3V5H5v5zm12 7h-3v2h5v-5h-2v3zM14 5v2h3v3h2V5h-5z" />
      </svg>
      <svg class="active-svg" xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18"
        viewBox="0 0 24 24">
        <path d="M0 0h24v24H0z" fill="none" />
        <path d="M5 16h3v3h2v-5H5v2zm3-8H5v2h5V5H8v3zm6 11h2v-3h3v-2h-5v5zm2-11V5h-2v5h5V8h-3z" />
      </svg>
      <span class="inactive-text">Fullscreen</span>
      <span class="active-text" aria-hidden="true">Exit Fullscreen</span>
    </button>
    <button class="menu-item menu-about" role="button">
      <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18" viewBox="0 0 24 24">
        <path d="M0 0h24v24H0z" fill="none" />
        <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-6h2v6zm0-8h-2V7h2v2z" />
      </svg>
      <span>About</span>
    </button>
    <button class="menu-item menu-studysubmit" role="button">
      <svg xlmns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18" viewBox="0 0 24 24">
        <path d="M0 0h24v24H0z" fill="none" />
        <path
          d="M8.016 15l3.984-3.984 3.984 3.984-1.406 1.453-1.594-1.594v4.125h-1.969v-4.125l-1.594 1.547zM18 20.016v-11.016h-5.016v-5.016h-6.984v16.031h12zM14.016 2.016l6 6v12q0 0.797-0.609 1.383t-1.406 0.586h-12q-0.797 0-1.406-0.586t-0.609-1.383l0.047-16.031q0-0.797 0.586-1.383t1.383-0.586h8.016z">
        </path>
      </svg>
      <span>Submit a Study</span>
    </button>
    <button class="menu-item menu-reader" role="button">
      <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18" viewBox="0 0 24 24">
        <path fill="none" d="M-74 29h48v48h-48V29zM0 0h24v24H0V0zm0 0h24v24H0V0z" />
        <path
          d="M13 12h7v1.5h-7zm0-2.5h7V11h-7zm0 5h7V16h-7zM21 4H3c-1.1 0-2 .9-2 2v13c0 1.1.9 2 2 2h18c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 15h-9V6h9v13z" />
      </svg>
      <span>View Records<span class="record-count"></span></span>
    </button>
    <button class="menu-item menu-accessibility" role="button">
      <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="18" height="18" viewBox="0 0 18 18">
        <path
          d="M8 4.143A1.071 1.071 0 1 0 8 2a1.071 1.071 0 0 0 0 2.143Zm-4.668 1.47 3.24.316v2.5l-.323 4.585A.383.383 0 0 0 7 13.14l.826-4.017c.045-.18.301-.18.346 0L9 13.139a.383.383 0 0 0 .752-.125L9.43 8.43v-2.5l3.239-.316a.38.38 0 0 0-.047-.756H3.379a.38.38 0 0 0-.047.756Z" />
        <path d="M8 0a8 8 0 1 0 0 16A8 8 0 0 0 8 0ZM1 8a7 7 0 1 1 14 0A7 7 0 0 1 1 8Z" />
      </svg>
      <span>Accessibility</span>
    </button>
    <span class="style-picker" role="group">
      <button class="menu-item bubble-map selected" aria-checked="true" role="switch" aria-label="Bubble map"
        data-tippy-content="Bubble map view" data-style="bubble">
        <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="16" height="16" viewBox="0 0 16 16">
          <circle cx="8" cy="8" r="8" />
        </svg>
      </button>
      <button class="menu-item heat-map" aria-checked="false" role="switch" aria-label="Heat map view"
        data-tippy-content="Heat map view" data-style="heat">
        <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="16" height="16" viewBox="0 0 16 16">
          <path
            d="M8 15V1h6a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H8zm6 1a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12z" />
        </svg>
      </button>
      <button class="menu-item mosaic-map" aria-checked="false" role="switch" aria-label="Mosaic map view"
        data-tippy-content="Mosaic map view" data-style="mosaic">
        <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="16" height="16" viewBox="0 0 16 16">
          <path d="M0 2a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V2z" />
        </svg>
      </button>
      <button class="menu-item donut-map" aria-checked="false" role="switch" aria-label="Donut map view"
        data-tippy-content="Donut map view" data-style="donut">
        <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="16" height="16" viewBox="0 0 16 16">
          <path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0zm-8 3a3 3 0 1 0 0-6 3 3 0 0 0 0 6z" />
        </svg>
      </button>
      <button class="menu-item text-map" aria-checked="false" role="switch" aria-label="Text map view"
        data-tippy-content="Text map view" data-style="text">
        <svg xmlns="http://www.w3.org/2000/svg" fill="#eeeeee" width="16" height="16" viewBox="0 0 16 16">
          <path
            d="M14.5 3a.5.5 0 0 1 .5.5v9a.5.5 0 0 1-.5.5h-13a.5.5 0 0 1-.5-.5v-9a.5.5 0 0 1 .5-.5h13zm-13-1A1.5 1.5 0 0 0 0 3.5v9A1.5 1.5 0 0 0 1.5 14h13a1.5 1.5 0 0 0 1.5-1.5v-9A1.5 1.5 0 0 0 14.5 2h-13z" />
          <path
            d="M3 5.5a.5.5 0 0 1 .5-.5h9a.5.5 0 0 1 0 1h-9a.5.5 0 0 1-.5-.5zM3 8a.5.5 0 0 1 .5-.5h9a.5.5 0 0 1 0 1h-9A.5.5 0 0 1 3 8zm0 2.5a.5.5 0 0 1 .5-.5h6a.5.5 0 0 1 0 1h-6a.5.5 0 0 1-.5-.5z" />
        </svg>
      </button>
    </span>
  </div>
  <div id="settings" class="settings" role="dialog" tabindex="-1" aria-labelledby="announce" style="display:none;">
  </div>
  <div class="header clearfix">
    
  </div>
  <div class="wrapper primary-view">
    <div class="pivot-table clearfix">
      <div class="top-head-wrapper">
        <div class="top-head">
          <table>
            <thead>
            </thead>
          </table>
        </div>
      </div>
      <div class="side-head">
        <table>
          <tbody>
          </tbody>
        </table>
      </div>
      <div class="body">
        <table>
          <tbody>
          </tbody>
        </table>
      </div>
    </div>
  </div>
  <div class="footer">
    <div class="legend"></div>
    <div class="spacer"></div>
    <div class="inner">
      Generated using v.2.3.0 of the EPPI-Mapper
      powered by <a href="https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=2914" target="_blank">EPPI Reviewer</a>
      and created with
      <svg fill="#c62828" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
        <path d="M0 0h24v24H0z" fill="none" />
        <path
          d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z" />
      </svg>
      by the
      <a href="http://www.digitalsolutionfoundry.co.za/" target="_blank">Digital Solution Foundry</a> team.
    </div>
  </div>
  <script type="text/javascript">/*! jQuery v3.3.1 | (c) JS Foundation and other contributors | jquery.org/license */
!function(e,t){"use strict";"object"==typeof module&&"object"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error("jQuery requires a window with a document");return t(e)}:t(e)}("undefined"!=typeof window?window:this,function(e,t){"use strict";var n=[],r=e.document,i=Object.getPrototypeOf,o=n.slice,a=n.concat,s=n.push,u=n.indexOf,l={},c=l.toString,f=l.hasOwnProperty,p=f.toString,d=p.call(Object),h={},g=function e(t){return"function"==typeof t&&"number"!=typeof t.nodeType},y=function e(t){return null!=t&&t===t.window},v={type:!0,src:!0,noModule:!0};function m(e,t,n){var i,o=(t=t||r).createElement("script");if(o.text=e,n)for(i in v)n[i]&&(o[i]=n[i]);t.head.appendChild(o).parentNode.removeChild(o)}function x(e){return null==e?e+"":"object"==typeof e||"function"==typeof e?l[c.call(e)]||"object":typeof e}var b="3.3.1",w=function(e,t){return new w.fn.init(e,t)},T=/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g;w.fn=w.prototype={jquery:"3.3.1",constructor:w,length:0,toArray:function(){return o.call(this)},get:function(e){return null==e?o.call(this):e<0?this[e+this.length]:this[e]},pushStack:function(e){var t=w.merge(this.constructor(),e);return t.prevObject=this,t},each:function(e){return w.each(this,e)},map:function(e){return this.pushStack(w.map(this,function(t,n){return e.call(t,n,t)}))},slice:function(){return this.pushStack(o.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},eq:function(e){var t=this.length,n=+e+(e<0?t:0);return this.pushStack(n>=0&&n<t?[this[n]]:[])},end:function(){return this.prevObject||this.constructor()},push:s,sort:n.sort,splice:n.splice},w.extend=w.fn.extend=function(){var e,t,n,r,i,o,a=arguments[0]||{},s=1,u=arguments.length,l=!1;for("boolean"==typeof a&&(l=a,a=arguments[s]||{},s++),"object"==typeof a||g(a)||(a={}),s===u&&(a=this,s--);s<u;s++)if(null!=(e=arguments[s]))for(t in e)n=a[t],a!==(r=e[t])&&(l&&r&&(w.isPlainObject(r)||(i=Array.isArray(r)))?(i?(i=!1,o=n&&Array.isArray(n)?n:[]):o=n&&w.isPlainObject(n)?n:{},a[t]=w.extend(l,o,r)):void 0!==r&&(a[t]=r));return a},w.extend({expando:"jQuery"+("3.3.1"+Math.random()).replace(/\D/g,""),isReady:!0,error:function(e){throw new Error(e)},noop:function(){},isPlainObject:function(e){var t,n;return!(!e||"[object Object]"!==c.call(e))&&(!(t=i(e))||"function"==typeof(n=f.call(t,"constructor")&&t.constructor)&&p.call(n)===d)},isEmptyObject:function(e){var t;for(t in e)return!1;return!0},globalEval:function(e){m(e)},each:function(e,t){var n,r=0;if(C(e)){for(n=e.length;r<n;r++)if(!1===t.call(e[r],r,e[r]))break}else for(r in e)if(!1===t.call(e[r],r,e[r]))break;return e},trim:function(e){return null==e?"":(e+"").replace(T,"")},makeArray:function(e,t){var n=t||[];return null!=e&&(C(Object(e))?w.merge(n,"string"==typeof e?[e]:e):s.call(n,e)),n},inArray:function(e,t,n){return null==t?-1:u.call(t,e,n)},merge:function(e,t){for(var n=+t.length,r=0,i=e.length;r<n;r++)e[i++]=t[r];return e.length=i,e},grep:function(e,t,n){for(var r,i=[],o=0,a=e.length,s=!n;o<a;o++)(r=!t(e[o],o))!==s&&i.push(e[o]);return i},map:function(e,t,n){var r,i,o=0,s=[];if(C(e))for(r=e.length;o<r;o++)null!=(i=t(e[o],o,n))&&s.push(i);else for(o in e)null!=(i=t(e[o],o,n))&&s.push(i);return a.apply([],s)},guid:1,support:h}),"function"==typeof Symbol&&(w.fn[Symbol.iterator]=n[Symbol.iterator]),w.each("Boolean Number String Function Array Date RegExp Object Error Symbol".split(" "),function(e,t){l["[object "+t+"]"]=t.toLowerCase()});function C(e){var t=!!e&&"length"in e&&e.length,n=x(e);return!g(e)&&!y(e)&&("array"===n||0===t||"number"==typeof t&&t>0&&t-1 in e)}var E=function(e){var t,n,r,i,o,a,s,u,l,c,f,p,d,h,g,y,v,m,x,b="sizzle"+1*new Date,w=e.document,T=0,C=0,E=ae(),k=ae(),S=ae(),D=function(e,t){return e===t&&(f=!0),0},N={}.hasOwnProperty,A=[],j=A.pop,q=A.push,L=A.push,H=A.slice,O=function(e,t){for(var n=0,r=e.length;n<r;n++)if(e[n]===t)return n;return-1},P="checked|selected|async|autofocus|autoplay|controls|defer|disabled|hidden|ismap|loop|multiple|open|readonly|required|scoped",M="[\\x20\\t\\r\\n\\f]",R="(?:\\\\.|[\\w-]|[^\0-\\xa0])+",I="\\["+M+"*("+R+")(?:"+M+"*([*^$|!~]?=)"+M+"*(?:'((?:\\\\.|[^\\\\'])*)'|\"((?:\\\\.|[^\\\\\"])*)\"|("+R+"))|)"+M+"*\\]",W=":("+R+")(?:\\((('((?:\\\\.|[^\\\\'])*)'|\"((?:\\\\.|[^\\\\\"])*)\")|((?:\\\\.|[^\\\\()[\\]]|"+I+")*)|.*)\\)|)",$=new RegExp(M+"+","g"),B=new RegExp("^"+M+"+|((?:^|[^\\\\])(?:\\\\.)*)"+M+"+$","g"),F=new RegExp("^"+M+"*,"+M+"*"),_=new RegExp("^"+M+"*([>+~]|"+M+")"+M+"*"),z=new RegExp("="+M+"*([^\\]'\"]*?)"+M+"*\\]","g"),X=new RegExp(W),U=new RegExp("^"+R+"$"),V={ID:new RegExp("^#("+R+")"),CLASS:new RegExp("^\\.("+R+")"),TAG:new RegExp("^("+R+"|[*])"),ATTR:new RegExp("^"+I),PSEUDO:new RegExp("^"+W),CHILD:new RegExp("^:(only|first|last|nth|nth-last)-(child|of-type)(?:\\("+M+"*(even|odd|(([+-]|)(\\d*)n|)"+M+"*(?:([+-]|)"+M+"*(\\d+)|))"+M+"*\\)|)","i"),bool:new RegExp("^(?:"+P+")$","i"),needsContext:new RegExp("^"+M+"*[>+~]|:(even|odd|eq|gt|lt|nth|first|last)(?:\\("+M+"*((?:-\\d)?\\d*)"+M+"*\\)|)(?=[^-]|$)","i")},G=/^(?:input|select|textarea|button)$/i,Y=/^h\d$/i,Q=/^[^{]+\{\s*\[native \w/,J=/^(?:#([\w-]+)|(\w+)|\.([\w-]+))$/,K=/[+~]/,Z=new RegExp("\\\\([\\da-f]{1,6}"+M+"?|("+M+")|.)","ig"),ee=function(e,t,n){var r="0x"+t-65536;return r!==r||n?t:r<0?String.fromCharCode(r+65536):String.fromCharCode(r>>10|55296,1023&r|56320)},te=/([\0-\x1f\x7f]|^-?\d)|^-$|[^\0-\x1f\x7f-\uFFFF\w-]/g,ne=function(e,t){return t?"\0"===e?"\ufffd":e.slice(0,-1)+"\\"+e.charCodeAt(e.length-1).toString(16)+" ":"\\"+e},re=function(){p()},ie=me(function(e){return!0===e.disabled&&("form"in e||"label"in e)},{dir:"parentNode",next:"legend"});try{L.apply(A=H.call(w.childNodes),w.childNodes),A[w.childNodes.length].nodeType}catch(e){L={apply:A.length?function(e,t){q.apply(e,H.call(t))}:function(e,t){var n=e.length,r=0;while(e[n++]=t[r++]);e.length=n-1}}}function oe(e,t,r,i){var o,s,l,c,f,h,v,m=t&&t.ownerDocument,T=t?t.nodeType:9;if(r=r||[],"string"!=typeof e||!e||1!==T&&9!==T&&11!==T)return r;if(!i&&((t?t.ownerDocument||t:w)!==d&&p(t),t=t||d,g)){if(11!==T&&(f=J.exec(e)))if(o=f[1]){if(9===T){if(!(l=t.getElementById(o)))return r;if(l.id===o)return r.push(l),r}else if(m&&(l=m.getElementById(o))&&x(t,l)&&l.id===o)return r.push(l),r}else{if(f[2])return L.apply(r,t.getElementsByTagName(e)),r;if((o=f[3])&&n.getElementsByClassName&&t.getElementsByClassName)return L.apply(r,t.getElementsByClassName(o)),r}if(n.qsa&&!S[e+" "]&&(!y||!y.test(e))){if(1!==T)m=t,v=e;else if("object"!==t.nodeName.toLowerCase()){(c=t.getAttribute("id"))?c=c.replace(te,ne):t.setAttribute("id",c=b),s=(h=a(e)).length;while(s--)h[s]="#"+c+" "+ve(h[s]);v=h.join(","),m=K.test(e)&&ge(t.parentNode)||t}if(v)try{return L.apply(r,m.querySelectorAll(v)),r}catch(e){}finally{c===b&&t.removeAttribute("id")}}}return u(e.replace(B,"$1"),t,r,i)}function ae(){var e=[];function t(n,i){return e.push(n+" ")>r.cacheLength&&delete t[e.shift()],t[n+" "]=i}return t}function se(e){return e[b]=!0,e}function ue(e){var t=d.createElement("fieldset");try{return!!e(t)}catch(e){return!1}finally{t.parentNode&&t.parentNode.removeChild(t),t=null}}function le(e,t){var n=e.split("|"),i=n.length;while(i--)r.attrHandle[n[i]]=t}function ce(e,t){var n=t&&e,r=n&&1===e.nodeType&&1===t.nodeType&&e.sourceIndex-t.sourceIndex;if(r)return r;if(n)while(n=n.nextSibling)if(n===t)return-1;return e?1:-1}function fe(e){return function(t){return"input"===t.nodeName.toLowerCase()&&t.type===e}}function pe(e){return function(t){var n=t.nodeName.toLowerCase();return("input"===n||"button"===n)&&t.type===e}}function de(e){return function(t){return"form"in t?t.parentNode&&!1===t.disabled?"label"in t?"label"in t.parentNode?t.parentNode.disabled===e:t.disabled===e:t.isDisabled===e||t.isDisabled!==!e&&ie(t)===e:t.disabled===e:"label"in t&&t.disabled===e}}function he(e){return se(function(t){return t=+t,se(function(n,r){var i,o=e([],n.length,t),a=o.length;while(a--)n[i=o[a]]&&(n[i]=!(r[i]=n[i]))})})}function ge(e){return e&&"undefined"!=typeof e.getElementsByTagName&&e}n=oe.support={},o=oe.isXML=function(e){var t=e&&(e.ownerDocument||e).documentElement;return!!t&&"HTML"!==t.nodeName},p=oe.setDocument=function(e){var t,i,a=e?e.ownerDocument||e:w;return a!==d&&9===a.nodeType&&a.documentElement?(d=a,h=d.documentElement,g=!o(d),w!==d&&(i=d.defaultView)&&i.top!==i&&(i.addEventListener?i.addEventListener("unload",re,!1):i.attachEvent&&i.attachEvent("onunload",re)),n.attributes=ue(function(e){return e.className="i",!e.getAttribute("className")}),n.getElementsByTagName=ue(function(e){return e.appendChild(d.createComment("")),!e.getElementsByTagName("*").length}),n.getElementsByClassName=Q.test(d.getElementsByClassName),n.getById=ue(function(e){return h.appendChild(e).id=b,!d.getElementsByName||!d.getElementsByName(b).length}),n.getById?(r.filter.ID=function(e){var t=e.replace(Z,ee);return function(e){return e.getAttribute("id")===t}},r.find.ID=function(e,t){if("undefined"!=typeof t.getElementById&&g){var n=t.getElementById(e);return n?[n]:[]}}):(r.filter.ID=function(e){var t=e.replace(Z,ee);return function(e){var n="undefined"!=typeof e.getAttributeNode&&e.getAttributeNode("id");return n&&n.value===t}},r.find.ID=function(e,t){if("undefined"!=typeof t.getElementById&&g){var n,r,i,o=t.getElementById(e);if(o){if((n=o.getAttributeNode("id"))&&n.value===e)return[o];i=t.getElementsByName(e),r=0;while(o=i[r++])if((n=o.getAttributeNode("id"))&&n.value===e)return[o]}return[]}}),r.find.TAG=n.getElementsByTagName?function(e,t){return"undefined"!=typeof t.getElementsByTagName?t.getElementsByTagName(e):n.qsa?t.querySelectorAll(e):void 0}:function(e,t){var n,r=[],i=0,o=t.getElementsByTagName(e);if("*"===e){while(n=o[i++])1===n.nodeType&&r.push(n);return r}return o},r.find.CLASS=n.getElementsByClassName&&function(e,t){if("undefined"!=typeof t.getElementsByClassName&&g)return t.getElementsByClassName(e)},v=[],y=[],(n.qsa=Q.test(d.querySelectorAll))&&(ue(function(e){h.appendChild(e).innerHTML="<a id='"+b+"'></a><select id='"+b+"-\r\\' msallowcapture=''><option selected=''></option></select>",e.querySelectorAll("[msallowcapture^='']").length&&y.push("[*^$]="+M+"*(?:''|\"\")"),e.querySelectorAll("[selected]").length||y.push("\\["+M+"*(?:value|"+P+")"),e.querySelectorAll("[id~="+b+"-]").length||y.push("~="),e.querySelectorAll(":checked").length||y.push(":checked"),e.querySelectorAll("a#"+b+"+*").length||y.push(".#.+[+~]")}),ue(function(e){e.innerHTML="<a href='' disabled='disabled'></a><select disabled='disabled'><option/></select>";var t=d.createElement("input");t.setAttribute("type","hidden"),e.appendChild(t).setAttribute("name","D"),e.querySelectorAll("[name=d]").length&&y.push("name"+M+"*[*^$|!~]?="),2!==e.querySelectorAll(":enabled").length&&y.push(":enabled",":disabled"),h.appendChild(e).disabled=!0,2!==e.querySelectorAll(":disabled").length&&y.push(":enabled",":disabled"),e.querySelectorAll("*,:x"),y.push(",.*:")})),(n.matchesSelector=Q.test(m=h.matches||h.webkitMatchesSelector||h.mozMatchesSelector||h.oMatchesSelector||h.msMatchesSelector))&&ue(function(e){n.disconnectedMatch=m.call(e,"*"),m.call(e,"[s!='']:x"),v.push("!=",W)}),y=y.length&&new RegExp(y.join("|")),v=v.length&&new RegExp(v.join("|")),t=Q.test(h.compareDocumentPosition),x=t||Q.test(h.contains)?function(e,t){var n=9===e.nodeType?e.documentElement:e,r=t&&t.parentNode;return e===r||!(!r||1!==r.nodeType||!(n.contains?n.contains(r):e.compareDocumentPosition&&16&e.compareDocumentPosition(r)))}:function(e,t){if(t)while(t=t.parentNode)if(t===e)return!0;return!1},D=t?function(e,t){if(e===t)return f=!0,0;var r=!e.compareDocumentPosition-!t.compareDocumentPosition;return r||(1&(r=(e.ownerDocument||e)===(t.ownerDocument||t)?e.compareDocumentPosition(t):1)||!n.sortDetached&&t.compareDocumentPosition(e)===r?e===d||e.ownerDocument===w&&x(w,e)?-1:t===d||t.ownerDocument===w&&x(w,t)?1:c?O(c,e)-O(c,t):0:4&r?-1:1)}:function(e,t){if(e===t)return f=!0,0;var n,r=0,i=e.parentNode,o=t.parentNode,a=[e],s=[t];if(!i||!o)return e===d?-1:t===d?1:i?-1:o?1:c?O(c,e)-O(c,t):0;if(i===o)return ce(e,t);n=e;while(n=n.parentNode)a.unshift(n);n=t;while(n=n.parentNode)s.unshift(n);while(a[r]===s[r])r++;return r?ce(a[r],s[r]):a[r]===w?-1:s[r]===w?1:0},d):d},oe.matches=function(e,t){return oe(e,null,null,t)},oe.matchesSelector=function(e,t){if((e.ownerDocument||e)!==d&&p(e),t=t.replace(z,"='$1']"),n.matchesSelector&&g&&!S[t+" "]&&(!v||!v.test(t))&&(!y||!y.test(t)))try{var r=m.call(e,t);if(r||n.disconnectedMatch||e.document&&11!==e.document.nodeType)return r}catch(e){}return oe(t,d,null,[e]).length>0},oe.contains=function(e,t){return(e.ownerDocument||e)!==d&&p(e),x(e,t)},oe.attr=function(e,t){(e.ownerDocument||e)!==d&&p(e);var i=r.attrHandle[t.toLowerCase()],o=i&&N.call(r.attrHandle,t.toLowerCase())?i(e,t,!g):void 0;return void 0!==o?o:n.attributes||!g?e.getAttribute(t):(o=e.getAttributeNode(t))&&o.specified?o.value:null},oe.escape=function(e){return(e+"").replace(te,ne)},oe.error=function(e){throw new Error("Syntax error, unrecognized expression: "+e)},oe.uniqueSort=function(e){var t,r=[],i=0,o=0;if(f=!n.detectDuplicates,c=!n.sortStable&&e.slice(0),e.sort(D),f){while(t=e[o++])t===e[o]&&(i=r.push(o));while(i--)e.splice(r[i],1)}return c=null,e},i=oe.getText=function(e){var t,n="",r=0,o=e.nodeType;if(o){if(1===o||9===o||11===o){if("string"==typeof e.textContent)return e.textContent;for(e=e.firstChild;e;e=e.nextSibling)n+=i(e)}else if(3===o||4===o)return e.nodeValue}else while(t=e[r++])n+=i(t);return n},(r=oe.selectors={cacheLength:50,createPseudo:se,match:V,attrHandle:{},find:{},relative:{">":{dir:"parentNode",first:!0}," ":{dir:"parentNode"},"+":{dir:"previousSibling",first:!0},"~":{dir:"previousSibling"}},preFilter:{ATTR:function(e){return e[1]=e[1].replace(Z,ee),e[3]=(e[3]||e[4]||e[5]||"").replace(Z,ee),"~="===e[2]&&(e[3]=" "+e[3]+" "),e.slice(0,4)},CHILD:function(e){return e[1]=e[1].toLowerCase(),"nth"===e[1].slice(0,3)?(e[3]||oe.error(e[0]),e[4]=+(e[4]?e[5]+(e[6]||1):2*("even"===e[3]||"odd"===e[3])),e[5]=+(e[7]+e[8]||"odd"===e[3])):e[3]&&oe.error(e[0]),e},PSEUDO:function(e){var t,n=!e[6]&&e[2];return V.CHILD.test(e[0])?null:(e[3]?e[2]=e[4]||e[5]||"":n&&X.test(n)&&(t=a(n,!0))&&(t=n.indexOf(")",n.length-t)-n.length)&&(e[0]=e[0].slice(0,t),e[2]=n.slice(0,t)),e.slice(0,3))}},filter:{TAG:function(e){var t=e.replace(Z,ee).toLowerCase();return"*"===e?function(){return!0}:function(e){return e.nodeName&&e.nodeName.toLowerCase()===t}},CLASS:function(e){var t=E[e+" "];return t||(t=new RegExp("(^|"+M+")"+e+"("+M+"|$)"))&&E(e,function(e){return t.test("string"==typeof e.className&&e.className||"undefined"!=typeof e.getAttribute&&e.getAttribute("class")||"")})},ATTR:function(e,t,n){return function(r){var i=oe.attr(r,e);return null==i?"!="===t:!t||(i+="","="===t?i===n:"!="===t?i!==n:"^="===t?n&&0===i.indexOf(n):"*="===t?n&&i.indexOf(n)>-1:"$="===t?n&&i.slice(-n.length)===n:"~="===t?(" "+i.replace($," ")+" ").indexOf(n)>-1:"|="===t&&(i===n||i.slice(0,n.length+1)===n+"-"))}},CHILD:function(e,t,n,r,i){var o="nth"!==e.slice(0,3),a="last"!==e.slice(-4),s="of-type"===t;return 1===r&&0===i?function(e){return!!e.parentNode}:function(t,n,u){var l,c,f,p,d,h,g=o!==a?"nextSibling":"previousSibling",y=t.parentNode,v=s&&t.nodeName.toLowerCase(),m=!u&&!s,x=!1;if(y){if(o){while(g){p=t;while(p=p[g])if(s?p.nodeName.toLowerCase()===v:1===p.nodeType)return!1;h=g="only"===e&&!h&&"nextSibling"}return!0}if(h=[a?y.firstChild:y.lastChild],a&&m){x=(d=(l=(c=(f=(p=y)[b]||(p[b]={}))[p.uniqueID]||(f[p.uniqueID]={}))[e]||[])[0]===T&&l[1])&&l[2],p=d&&y.childNodes[d];while(p=++d&&p&&p[g]||(x=d=0)||h.pop())if(1===p.nodeType&&++x&&p===t){c[e]=[T,d,x];break}}else if(m&&(x=d=(l=(c=(f=(p=t)[b]||(p[b]={}))[p.uniqueID]||(f[p.uniqueID]={}))[e]||[])[0]===T&&l[1]),!1===x)while(p=++d&&p&&p[g]||(x=d=0)||h.pop())if((s?p.nodeName.toLowerCase()===v:1===p.nodeType)&&++x&&(m&&((c=(f=p[b]||(p[b]={}))[p.uniqueID]||(f[p.uniqueID]={}))[e]=[T,x]),p===t))break;return(x-=i)===r||x%r==0&&x/r>=0}}},PSEUDO:function(e,t){var n,i=r.pseudos[e]||r.setFilters[e.toLowerCase()]||oe.error("unsupported pseudo: "+e);return i[b]?i(t):i.length>1?(n=[e,e,"",t],r.setFilters.hasOwnProperty(e.toLowerCase())?se(function(e,n){var r,o=i(e,t),a=o.length;while(a--)e[r=O(e,o[a])]=!(n[r]=o[a])}):function(e){return i(e,0,n)}):i}},pseudos:{not:se(function(e){var t=[],n=[],r=s(e.replace(B,"$1"));return r[b]?se(function(e,t,n,i){var o,a=r(e,null,i,[]),s=e.length;while(s--)(o=a[s])&&(e[s]=!(t[s]=o))}):function(e,i,o){return t[0]=e,r(t,null,o,n),t[0]=null,!n.pop()}}),has:se(function(e){return function(t){return oe(e,t).length>0}}),contains:se(function(e){return e=e.replace(Z,ee),function(t){return(t.textContent||t.innerText||i(t)).indexOf(e)>-1}}),lang:se(function(e){return U.test(e||"")||oe.error("unsupported lang: "+e),e=e.replace(Z,ee).toLowerCase(),function(t){var n;do{if(n=g?t.lang:t.getAttribute("xml:lang")||t.getAttribute("lang"))return(n=n.toLowerCase())===e||0===n.indexOf(e+"-")}while((t=t.parentNode)&&1===t.nodeType);return!1}}),target:function(t){var n=e.location&&e.location.hash;return n&&n.slice(1)===t.id},root:function(e){return e===h},focus:function(e){return e===d.activeElement&&(!d.hasFocus||d.hasFocus())&&!!(e.type||e.href||~e.tabIndex)},enabled:de(!1),disabled:de(!0),checked:function(e){var t=e.nodeName.toLowerCase();return"input"===t&&!!e.checked||"option"===t&&!!e.selected},selected:function(e){return e.parentNode&&e.parentNode.selectedIndex,!0===e.selected},empty:function(e){for(e=e.firstChild;e;e=e.nextSibling)if(e.nodeType<6)return!1;return!0},parent:function(e){return!r.pseudos.empty(e)},header:function(e){return Y.test(e.nodeName)},input:function(e){return G.test(e.nodeName)},button:function(e){var t=e.nodeName.toLowerCase();return"input"===t&&"button"===e.type||"button"===t},text:function(e){var t;return"input"===e.nodeName.toLowerCase()&&"text"===e.type&&(null==(t=e.getAttribute("type"))||"text"===t.toLowerCase())},first:he(function(){return[0]}),last:he(function(e,t){return[t-1]}),eq:he(function(e,t,n){return[n<0?n+t:n]}),even:he(function(e,t){for(var n=0;n<t;n+=2)e.push(n);return e}),odd:he(function(e,t){for(var n=1;n<t;n+=2)e.push(n);return e}),lt:he(function(e,t,n){for(var r=n<0?n+t:n;--r>=0;)e.push(r);return e}),gt:he(function(e,t,n){for(var r=n<0?n+t:n;++r<t;)e.push(r);return e})}}).pseudos.nth=r.pseudos.eq;for(t in{radio:!0,checkbox:!0,file:!0,password:!0,image:!0})r.pseudos[t]=fe(t);for(t in{submit:!0,reset:!0})r.pseudos[t]=pe(t);function ye(){}ye.prototype=r.filters=r.pseudos,r.setFilters=new ye,a=oe.tokenize=function(e,t){var n,i,o,a,s,u,l,c=k[e+" "];if(c)return t?0:c.slice(0);s=e,u=[],l=r.preFilter;while(s){n&&!(i=F.exec(s))||(i&&(s=s.slice(i[0].length)||s),u.push(o=[])),n=!1,(i=_.exec(s))&&(n=i.shift(),o.push({value:n,type:i[0].replace(B," ")}),s=s.slice(n.length));for(a in r.filter)!(i=V[a].exec(s))||l[a]&&!(i=l[a](i))||(n=i.shift(),o.push({value:n,type:a,matches:i}),s=s.slice(n.length));if(!n)break}return t?s.length:s?oe.error(e):k(e,u).slice(0)};function ve(e){for(var t=0,n=e.length,r="";t<n;t++)r+=e[t].value;return r}function me(e,t,n){var r=t.dir,i=t.next,o=i||r,a=n&&"parentNode"===o,s=C++;return t.first?function(t,n,i){while(t=t[r])if(1===t.nodeType||a)return e(t,n,i);return!1}:function(t,n,u){var l,c,f,p=[T,s];if(u){while(t=t[r])if((1===t.nodeType||a)&&e(t,n,u))return!0}else while(t=t[r])if(1===t.nodeType||a)if(f=t[b]||(t[b]={}),c=f[t.uniqueID]||(f[t.uniqueID]={}),i&&i===t.nodeName.toLowerCase())t=t[r]||t;else{if((l=c[o])&&l[0]===T&&l[1]===s)return p[2]=l[2];if(c[o]=p,p[2]=e(t,n,u))return!0}return!1}}function xe(e){return e.length>1?function(t,n,r){var i=e.length;while(i--)if(!e[i](t,n,r))return!1;return!0}:e[0]}function be(e,t,n){for(var r=0,i=t.length;r<i;r++)oe(e,t[r],n);return n}function we(e,t,n,r,i){for(var o,a=[],s=0,u=e.length,l=null!=t;s<u;s++)(o=e[s])&&(n&&!n(o,r,i)||(a.push(o),l&&t.push(s)));return a}function Te(e,t,n,r,i,o){return r&&!r[b]&&(r=Te(r)),i&&!i[b]&&(i=Te(i,o)),se(function(o,a,s,u){var l,c,f,p=[],d=[],h=a.length,g=o||be(t||"*",s.nodeType?[s]:s,[]),y=!e||!o&&t?g:we(g,p,e,s,u),v=n?i||(o?e:h||r)?[]:a:y;if(n&&n(y,v,s,u),r){l=we(v,d),r(l,[],s,u),c=l.length;while(c--)(f=l[c])&&(v[d[c]]=!(y[d[c]]=f))}if(o){if(i||e){if(i){l=[],c=v.length;while(c--)(f=v[c])&&l.push(y[c]=f);i(null,v=[],l,u)}c=v.length;while(c--)(f=v[c])&&(l=i?O(o,f):p[c])>-1&&(o[l]=!(a[l]=f))}}else v=we(v===a?v.splice(h,v.length):v),i?i(null,a,v,u):L.apply(a,v)})}function Ce(e){for(var t,n,i,o=e.length,a=r.relative[e[0].type],s=a||r.relative[" "],u=a?1:0,c=me(function(e){return e===t},s,!0),f=me(function(e){return O(t,e)>-1},s,!0),p=[function(e,n,r){var i=!a&&(r||n!==l)||((t=n).nodeType?c(e,n,r):f(e,n,r));return t=null,i}];u<o;u++)if(n=r.relative[e[u].type])p=[me(xe(p),n)];else{if((n=r.filter[e[u].type].apply(null,e[u].matches))[b]){for(i=++u;i<o;i++)if(r.relative[e[i].type])break;return Te(u>1&&xe(p),u>1&&ve(e.slice(0,u-1).concat({value:" "===e[u-2].type?"*":""})).replace(B,"$1"),n,u<i&&Ce(e.slice(u,i)),i<o&&Ce(e=e.slice(i)),i<o&&ve(e))}p.push(n)}return xe(p)}function Ee(e,t){var n=t.length>0,i=e.length>0,o=function(o,a,s,u,c){var f,h,y,v=0,m="0",x=o&&[],b=[],w=l,C=o||i&&r.find.TAG("*",c),E=T+=null==w?1:Math.random()||.1,k=C.length;for(c&&(l=a===d||a||c);m!==k&&null!=(f=C[m]);m++){if(i&&f){h=0,a||f.ownerDocument===d||(p(f),s=!g);while(y=e[h++])if(y(f,a||d,s)){u.push(f);break}c&&(T=E)}n&&((f=!y&&f)&&v--,o&&x.push(f))}if(v+=m,n&&m!==v){h=0;while(y=t[h++])y(x,b,a,s);if(o){if(v>0)while(m--)x[m]||b[m]||(b[m]=j.call(u));b=we(b)}L.apply(u,b),c&&!o&&b.length>0&&v+t.length>1&&oe.uniqueSort(u)}return c&&(T=E,l=w),x};return n?se(o):o}return s=oe.compile=function(e,t){var n,r=[],i=[],o=S[e+" "];if(!o){t||(t=a(e)),n=t.length;while(n--)(o=Ce(t[n]))[b]?r.push(o):i.push(o);(o=S(e,Ee(i,r))).selector=e}return o},u=oe.select=function(e,t,n,i){var o,u,l,c,f,p="function"==typeof e&&e,d=!i&&a(e=p.selector||e);if(n=n||[],1===d.length){if((u=d[0]=d[0].slice(0)).length>2&&"ID"===(l=u[0]).type&&9===t.nodeType&&g&&r.relative[u[1].type]){if(!(t=(r.find.ID(l.matches[0].replace(Z,ee),t)||[])[0]))return n;p&&(t=t.parentNode),e=e.slice(u.shift().value.length)}o=V.needsContext.test(e)?0:u.length;while(o--){if(l=u[o],r.relative[c=l.type])break;if((f=r.find[c])&&(i=f(l.matches[0].replace(Z,ee),K.test(u[0].type)&&ge(t.parentNode)||t))){if(u.splice(o,1),!(e=i.length&&ve(u)))return L.apply(n,i),n;break}}}return(p||s(e,d))(i,t,!g,n,!t||K.test(e)&&ge(t.parentNode)||t),n},n.sortStable=b.split("").sort(D).join("")===b,n.detectDuplicates=!!f,p(),n.sortDetached=ue(function(e){return 1&e.compareDocumentPosition(d.createElement("fieldset"))}),ue(function(e){return e.innerHTML="<a href='#'></a>","#"===e.firstChild.getAttribute("href")})||le("type|href|height|width",function(e,t,n){if(!n)return e.getAttribute(t,"type"===t.toLowerCase()?1:2)}),n.attributes&&ue(function(e){return e.innerHTML="<input/>",e.firstChild.setAttribute("value",""),""===e.firstChild.getAttribute("value")})||le("value",function(e,t,n){if(!n&&"input"===e.nodeName.toLowerCase())return e.defaultValue}),ue(function(e){return null==e.getAttribute("disabled")})||le(P,function(e,t,n){var r;if(!n)return!0===e[t]?t.toLowerCase():(r=e.getAttributeNode(t))&&r.specified?r.value:null}),oe}(e);w.find=E,w.expr=E.selectors,w.expr[":"]=w.expr.pseudos,w.uniqueSort=w.unique=E.uniqueSort,w.text=E.getText,w.isXMLDoc=E.isXML,w.contains=E.contains,w.escapeSelector=E.escape;var k=function(e,t,n){var r=[],i=void 0!==n;while((e=e[t])&&9!==e.nodeType)if(1===e.nodeType){if(i&&w(e).is(n))break;r.push(e)}return r},S=function(e,t){for(var n=[];e;e=e.nextSibling)1===e.nodeType&&e!==t&&n.push(e);return n},D=w.expr.match.needsContext;function N(e,t){return e.nodeName&&e.nodeName.toLowerCase()===t.toLowerCase()}var A=/^<([a-z][^\/\0>:\x20\t\r\n\f]*)[\x20\t\r\n\f]*\/?>(?:<\/\1>|)$/i;function j(e,t,n){return g(t)?w.grep(e,function(e,r){return!!t.call(e,r,e)!==n}):t.nodeType?w.grep(e,function(e){return e===t!==n}):"string"!=typeof t?w.grep(e,function(e){return u.call(t,e)>-1!==n}):w.filter(t,e,n)}w.filter=function(e,t,n){var r=t[0];return n&&(e=":not("+e+")"),1===t.length&&1===r.nodeType?w.find.matchesSelector(r,e)?[r]:[]:w.find.matches(e,w.grep(t,function(e){return 1===e.nodeType}))},w.fn.extend({find:function(e){var t,n,r=this.length,i=this;if("string"!=typeof e)return this.pushStack(w(e).filter(function(){for(t=0;t<r;t++)if(w.contains(i[t],this))return!0}));for(n=this.pushStack([]),t=0;t<r;t++)w.find(e,i[t],n);return r>1?w.uniqueSort(n):n},filter:function(e){return this.pushStack(j(this,e||[],!1))},not:function(e){return this.pushStack(j(this,e||[],!0))},is:function(e){return!!j(this,"string"==typeof e&&D.test(e)?w(e):e||[],!1).length}});var q,L=/^(?:\s*(<[\w\W]+>)[^>]*|#([\w-]+))$/;(w.fn.init=function(e,t,n){var i,o;if(!e)return this;if(n=n||q,"string"==typeof e){if(!(i="<"===e[0]&&">"===e[e.length-1]&&e.length>=3?[null,e,null]:L.exec(e))||!i[1]&&t)return!t||t.jquery?(t||n).find(e):this.constructor(t).find(e);if(i[1]){if(t=t instanceof w?t[0]:t,w.merge(this,w.parseHTML(i[1],t&&t.nodeType?t.ownerDocument||t:r,!0)),A.test(i[1])&&w.isPlainObject(t))for(i in t)g(this[i])?this[i](t[i]):this.attr(i,t[i]);return this}return(o=r.getElementById(i[2]))&&(this[0]=o,this.length=1),this}return e.nodeType?(this[0]=e,this.length=1,this):g(e)?void 0!==n.ready?n.ready(e):e(w):w.makeArray(e,this)}).prototype=w.fn,q=w(r);var H=/^(?:parents|prev(?:Until|All))/,O={children:!0,contents:!0,next:!0,prev:!0};w.fn.extend({has:function(e){var t=w(e,this),n=t.length;return this.filter(function(){for(var e=0;e<n;e++)if(w.contains(this,t[e]))return!0})},closest:function(e,t){var n,r=0,i=this.length,o=[],a="string"!=typeof e&&w(e);if(!D.test(e))for(;r<i;r++)for(n=this[r];n&&n!==t;n=n.parentNode)if(n.nodeType<11&&(a?a.index(n)>-1:1===n.nodeType&&w.find.matchesSelector(n,e))){o.push(n);break}return this.pushStack(o.length>1?w.uniqueSort(o):o)},index:function(e){return e?"string"==typeof e?u.call(w(e),this[0]):u.call(this,e.jquery?e[0]:e):this[0]&&this[0].parentNode?this.first().prevAll().length:-1},add:function(e,t){return this.pushStack(w.uniqueSort(w.merge(this.get(),w(e,t))))},addBack:function(e){return this.add(null==e?this.prevObject:this.prevObject.filter(e))}});function P(e,t){while((e=e[t])&&1!==e.nodeType);return e}w.each({parent:function(e){var t=e.parentNode;return t&&11!==t.nodeType?t:null},parents:function(e){return k(e,"parentNode")},parentsUntil:function(e,t,n){return k(e,"parentNode",n)},next:function(e){return P(e,"nextSibling")},prev:function(e){return P(e,"previousSibling")},nextAll:function(e){return k(e,"nextSibling")},prevAll:function(e){return k(e,"previousSibling")},nextUntil:function(e,t,n){return k(e,"nextSibling",n)},prevUntil:function(e,t,n){return k(e,"previousSibling",n)},siblings:function(e){return S((e.parentNode||{}).firstChild,e)},children:function(e){return S(e.firstChild)},contents:function(e){return N(e,"iframe")?e.contentDocument:(N(e,"template")&&(e=e.content||e),w.merge([],e.childNodes))}},function(e,t){w.fn[e]=function(n,r){var i=w.map(this,t,n);return"Until"!==e.slice(-5)&&(r=n),r&&"string"==typeof r&&(i=w.filter(r,i)),this.length>1&&(O[e]||w.uniqueSort(i),H.test(e)&&i.reverse()),this.pushStack(i)}});var M=/[^\x20\t\r\n\f]+/g;function R(e){var t={};return w.each(e.match(M)||[],function(e,n){t[n]=!0}),t}w.Callbacks=function(e){e="string"==typeof e?R(e):w.extend({},e);var t,n,r,i,o=[],a=[],s=-1,u=function(){for(i=i||e.once,r=t=!0;a.length;s=-1){n=a.shift();while(++s<o.length)!1===o[s].apply(n[0],n[1])&&e.stopOnFalse&&(s=o.length,n=!1)}e.memory||(n=!1),t=!1,i&&(o=n?[]:"")},l={add:function(){return o&&(n&&!t&&(s=o.length-1,a.push(n)),function t(n){w.each(n,function(n,r){g(r)?e.unique&&l.has(r)||o.push(r):r&&r.length&&"string"!==x(r)&&t(r)})}(arguments),n&&!t&&u()),this},remove:function(){return w.each(arguments,function(e,t){var n;while((n=w.inArray(t,o,n))>-1)o.splice(n,1),n<=s&&s--}),this},has:function(e){return e?w.inArray(e,o)>-1:o.length>0},empty:function(){return o&&(o=[]),this},disable:function(){return i=a=[],o=n="",this},disabled:function(){return!o},lock:function(){return i=a=[],n||t||(o=n=""),this},locked:function(){return!!i},fireWith:function(e,n){return i||(n=[e,(n=n||[]).slice?n.slice():n],a.push(n),t||u()),this},fire:function(){return l.fireWith(this,arguments),this},fired:function(){return!!r}};return l};function I(e){return e}function W(e){throw e}function $(e,t,n,r){var i;try{e&&g(i=e.promise)?i.call(e).done(t).fail(n):e&&g(i=e.then)?i.call(e,t,n):t.apply(void 0,[e].slice(r))}catch(e){n.apply(void 0,[e])}}w.extend({Deferred:function(t){var n=[["notify","progress",w.Callbacks("memory"),w.Callbacks("memory"),2],["resolve","done",w.Callbacks("once memory"),w.Callbacks("once memory"),0,"resolved"],["reject","fail",w.Callbacks("once memory"),w.Callbacks("once memory"),1,"rejected"]],r="pending",i={state:function(){return r},always:function(){return o.done(arguments).fail(arguments),this},"catch":function(e){return i.then(null,e)},pipe:function(){var e=arguments;return w.Deferred(function(t){w.each(n,function(n,r){var i=g(e[r[4]])&&e[r[4]];o[r[1]](function(){var e=i&&i.apply(this,arguments);e&&g(e.promise)?e.promise().progress(t.notify).done(t.resolve).fail(t.reject):t[r[0]+"With"](this,i?[e]:arguments)})}),e=null}).promise()},then:function(t,r,i){var o=0;function a(t,n,r,i){return function(){var s=this,u=arguments,l=function(){var e,l;if(!(t<o)){if((e=r.apply(s,u))===n.promise())throw new TypeError("Thenable self-resolution");l=e&&("object"==typeof e||"function"==typeof e)&&e.then,g(l)?i?l.call(e,a(o,n,I,i),a(o,n,W,i)):(o++,l.call(e,a(o,n,I,i),a(o,n,W,i),a(o,n,I,n.notifyWith))):(r!==I&&(s=void 0,u=[e]),(i||n.resolveWith)(s,u))}},c=i?l:function(){try{l()}catch(e){w.Deferred.exceptionHook&&w.Deferred.exceptionHook(e,c.stackTrace),t+1>=o&&(r!==W&&(s=void 0,u=[e]),n.rejectWith(s,u))}};t?c():(w.Deferred.getStackHook&&(c.stackTrace=w.Deferred.getStackHook()),e.setTimeout(c))}}return w.Deferred(function(e){n[0][3].add(a(0,e,g(i)?i:I,e.notifyWith)),n[1][3].add(a(0,e,g(t)?t:I)),n[2][3].add(a(0,e,g(r)?r:W))}).promise()},promise:function(e){return null!=e?w.extend(e,i):i}},o={};return w.each(n,function(e,t){var a=t[2],s=t[5];i[t[1]]=a.add,s&&a.add(function(){r=s},n[3-e][2].disable,n[3-e][3].disable,n[0][2].lock,n[0][3].lock),a.add(t[3].fire),o[t[0]]=function(){return o[t[0]+"With"](this===o?void 0:this,arguments),this},o[t[0]+"With"]=a.fireWith}),i.promise(o),t&&t.call(o,o),o},when:function(e){var t=arguments.length,n=t,r=Array(n),i=o.call(arguments),a=w.Deferred(),s=function(e){return function(n){r[e]=this,i[e]=arguments.length>1?o.call(arguments):n,--t||a.resolveWith(r,i)}};if(t<=1&&($(e,a.done(s(n)).resolve,a.reject,!t),"pending"===a.state()||g(i[n]&&i[n].then)))return a.then();while(n--)$(i[n],s(n),a.reject);return a.promise()}});var B=/^(Eval|Internal|Range|Reference|Syntax|Type|URI)Error$/;w.Deferred.exceptionHook=function(t,n){e.console&&e.console.warn&&t&&B.test(t.name)&&e.console.warn("jQuery.Deferred exception: "+t.message,t.stack,n)},w.readyException=function(t){e.setTimeout(function(){throw t})};var F=w.Deferred();w.fn.ready=function(e){return F.then(e)["catch"](function(e){w.readyException(e)}),this},w.extend({isReady:!1,readyWait:1,ready:function(e){(!0===e?--w.readyWait:w.isReady)||(w.isReady=!0,!0!==e&&--w.readyWait>0||F.resolveWith(r,[w]))}}),w.ready.then=F.then;function _(){r.removeEventListener("DOMContentLoaded",_),e.removeEventListener("load",_),w.ready()}"complete"===r.readyState||"loading"!==r.readyState&&!r.documentElement.doScroll?e.setTimeout(w.ready):(r.addEventListener("DOMContentLoaded",_),e.addEventListener("load",_));var z=function(e,t,n,r,i,o,a){var s=0,u=e.length,l=null==n;if("object"===x(n)){i=!0;for(s in n)z(e,t,s,n[s],!0,o,a)}else if(void 0!==r&&(i=!0,g(r)||(a=!0),l&&(a?(t.call(e,r),t=null):(l=t,t=function(e,t,n){return l.call(w(e),n)})),t))for(;s<u;s++)t(e[s],n,a?r:r.call(e[s],s,t(e[s],n)));return i?e:l?t.call(e):u?t(e[0],n):o},X=/^-ms-/,U=/-([a-z])/g;function V(e,t){return t.toUpperCase()}function G(e){return e.replace(X,"ms-").replace(U,V)}var Y=function(e){return 1===e.nodeType||9===e.nodeType||!+e.nodeType};function Q(){this.expando=w.expando+Q.uid++}Q.uid=1,Q.prototype={cache:function(e){var t=e[this.expando];return t||(t={},Y(e)&&(e.nodeType?e[this.expando]=t:Object.defineProperty(e,this.expando,{value:t,configurable:!0}))),t},set:function(e,t,n){var r,i=this.cache(e);if("string"==typeof t)i[G(t)]=n;else for(r in t)i[G(r)]=t[r];return i},get:function(e,t){return void 0===t?this.cache(e):e[this.expando]&&e[this.expando][G(t)]},access:function(e,t,n){return void 0===t||t&&"string"==typeof t&&void 0===n?this.get(e,t):(this.set(e,t,n),void 0!==n?n:t)},remove:function(e,t){var n,r=e[this.expando];if(void 0!==r){if(void 0!==t){n=(t=Array.isArray(t)?t.map(G):(t=G(t))in r?[t]:t.match(M)||[]).length;while(n--)delete r[t[n]]}(void 0===t||w.isEmptyObject(r))&&(e.nodeType?e[this.expando]=void 0:delete e[this.expando])}},hasData:function(e){var t=e[this.expando];return void 0!==t&&!w.isEmptyObject(t)}};var J=new Q,K=new Q,Z=/^(?:\{[\w\W]*\}|\[[\w\W]*\])$/,ee=/[A-Z]/g;function te(e){return"true"===e||"false"!==e&&("null"===e?null:e===+e+""?+e:Z.test(e)?JSON.parse(e):e)}function ne(e,t,n){var r;if(void 0===n&&1===e.nodeType)if(r="data-"+t.replace(ee,"-{{ jquery }}").toLowerCase(),"string"==typeof(n=e.getAttribute(r))){try{n=te(n)}catch(e){}K.set(e,t,n)}else n=void 0;return n}w.extend({hasData:function(e){return K.hasData(e)||J.hasData(e)},data:function(e,t,n){return K.access(e,t,n)},removeData:function(e,t){K.remove(e,t)},_data:function(e,t,n){return J.access(e,t,n)},_removeData:function(e,t){J.remove(e,t)}}),w.fn.extend({data:function(e,t){var n,r,i,o=this[0],a=o&&o.attributes;if(void 0===e){if(this.length&&(i=K.get(o),1===o.nodeType&&!J.get(o,"hasDataAttrs"))){n=a.length;while(n--)a[n]&&0===(r=a[n].name).indexOf("data-")&&(r=G(r.slice(5)),ne(o,r,i[r]));J.set(o,"hasDataAttrs",!0)}return i}return"object"==typeof e?this.each(function(){K.set(this,e)}):z(this,function(t){var n;if(o&&void 0===t){if(void 0!==(n=K.get(o,e)))return n;if(void 0!==(n=ne(o,e)))return n}else this.each(function(){K.set(this,e,t)})},null,t,arguments.length>1,null,!0)},removeData:function(e){return this.each(function(){K.remove(this,e)})}}),w.extend({queue:function(e,t,n){var r;if(e)return t=(t||"fx")+"queue",r=J.get(e,t),n&&(!r||Array.isArray(n)?r=J.access(e,t,w.makeArray(n)):r.push(n)),r||[]},dequeue:function(e,t){t=t||"fx";var n=w.queue(e,t),r=n.length,i=n.shift(),o=w._queueHooks(e,t),a=function(){w.dequeue(e,t)};"inprogress"===i&&(i=n.shift(),r--),i&&("fx"===t&&n.unshift("inprogress"),delete o.stop,i.call(e,a,o)),!r&&o&&o.empty.fire()},_queueHooks:function(e,t){var n=t+"queueHooks";return J.get(e,n)||J.access(e,n,{empty:w.Callbacks("once memory").add(function(){J.remove(e,[t+"queue",n])})})}}),w.fn.extend({queue:function(e,t){var n=2;return"string"!=typeof e&&(t=e,e="fx",n--),arguments.length<n?w.queue(this[0],e):void 0===t?this:this.each(function(){var n=w.queue(this,e,t);w._queueHooks(this,e),"fx"===e&&"inprogress"!==n[0]&&w.dequeue(this,e)})},dequeue:function(e){return this.each(function(){w.dequeue(this,e)})},clearQueue:function(e){return this.queue(e||"fx",[])},promise:function(e,t){var n,r=1,i=w.Deferred(),o=this,a=this.length,s=function(){--r||i.resolveWith(o,[o])};"string"!=typeof e&&(t=e,e=void 0),e=e||"fx";while(a--)(n=J.get(o[a],e+"queueHooks"))&&n.empty&&(r++,n.empty.add(s));return s(),i.promise(t)}});var re=/[+-]?(?:\d*\.|)\d+(?:[eE][+-]?\d+|)/.source,ie=new RegExp("^(?:([+-])=|)("+re+")([a-z%]*)$","i"),oe=["Top","Right","Bottom","Left"],ae=function(e,t){return"none"===(e=t||e).style.display||""===e.style.display&&w.contains(e.ownerDocument,e)&&"none"===w.css(e,"display")},se=function(e,t,n,r){var i,o,a={};for(o in t)a[o]=e.style[o],e.style[o]=t[o];i=n.apply(e,r||[]);for(o in t)e.style[o]=a[o];return i};function ue(e,t,n,r){var i,o,a=20,s=r?function(){return r.cur()}:function(){return w.css(e,t,"")},u=s(),l=n&&n[3]||(w.cssNumber[t]?"":"px"),c=(w.cssNumber[t]||"px"!==l&&+u)&&ie.exec(w.css(e,t));if(c&&c[3]!==l){u/=2,l=l||c[3],c=+u||1;while(a--)w.style(e,t,c+l),(1-o)*(1-(o=s()/u||.5))<=0&&(a=0),c/=o;c*=2,w.style(e,t,c+l),n=n||[]}return n&&(c=+c||+u||0,i=n[1]?c+(n[1]+1)*n[2]:+n[2],r&&(r.unit=l,r.start=c,r.end=i)),i}var le={};function ce(e){var t,n=e.ownerDocument,r=e.nodeName,i=le[r];return i||(t=n.body.appendChild(n.createElement(r)),i=w.css(t,"display"),t.parentNode.removeChild(t),"none"===i&&(i="block"),le[r]=i,i)}function fe(e,t){for(var n,r,i=[],o=0,a=e.length;o<a;o++)(r=e[o]).style&&(n=r.style.display,t?("none"===n&&(i[o]=J.get(r,"display")||null,i[o]||(r.style.display="")),""===r.style.display&&ae(r)&&(i[o]=ce(r))):"none"!==n&&(i[o]="none",J.set(r,"display",n)));for(o=0;o<a;o++)null!=i[o]&&(e[o].style.display=i[o]);return e}w.fn.extend({show:function(){return fe(this,!0)},hide:function(){return fe(this)},toggle:function(e){return"boolean"==typeof e?e?this.show():this.hide():this.each(function(){ae(this)?w(this).show():w(this).hide()})}});var pe=/^(?:checkbox|radio)$/i,de=/<([a-z][^\/\0>\x20\t\r\n\f]+)/i,he=/^$|^module$|\/(?:java|ecma)script/i,ge={option:[1,"<select multiple='multiple'>","</select>"],thead:[1,"<table>","</table>"],col:[2,"<table><colgroup>","</colgroup></table>"],tr:[2,"<table><tbody>","</tbody></table>"],td:[3,"<table><tbody><tr>","</tr></tbody></table>"],_default:[0,"",""]};ge.optgroup=ge.option,ge.tbody=ge.tfoot=ge.colgroup=ge.caption=ge.thead,ge.th=ge.td;function ye(e,t){var n;return n="undefined"!=typeof e.getElementsByTagName?e.getElementsByTagName(t||"*"):"undefined"!=typeof e.querySelectorAll?e.querySelectorAll(t||"*"):[],void 0===t||t&&N(e,t)?w.merge([e],n):n}function ve(e,t){for(var n=0,r=e.length;n<r;n++)J.set(e[n],"globalEval",!t||J.get(t[n],"globalEval"))}var me=/<|&#?\w+;/;function xe(e,t,n,r,i){for(var o,a,s,u,l,c,f=t.createDocumentFragment(),p=[],d=0,h=e.length;d<h;d++)if((o=e[d])||0===o)if("object"===x(o))w.merge(p,o.nodeType?[o]:o);else if(me.test(o)){a=a||f.appendChild(t.createElement("div")),s=(de.exec(o)||["",""])[1].toLowerCase(),u=ge[s]||ge._default,a.innerHTML=u[1]+w.htmlPrefilter(o)+u[2],c=u[0];while(c--)a=a.lastChild;w.merge(p,a.childNodes),(a=f.firstChild).textContent=""}else p.push(t.createTextNode(o));f.textContent="",d=0;while(o=p[d++])if(r&&w.inArray(o,r)>-1)i&&i.push(o);else if(l=w.contains(o.ownerDocument,o),a=ye(f.appendChild(o),"script"),l&&ve(a),n){c=0;while(o=a[c++])he.test(o.type||"")&&n.push(o)}return f}!function(){var e=r.createDocumentFragment().appendChild(r.createElement("div")),t=r.createElement("input");t.setAttribute("type","radio"),t.setAttribute("checked","checked"),t.setAttribute("name","t"),e.appendChild(t),h.checkClone=e.cloneNode(!0).cloneNode(!0).lastChild.checked,e.innerHTML="<textarea>x</textarea>",h.noCloneChecked=!!e.cloneNode(!0).lastChild.defaultValue}();var be=r.documentElement,we=/^key/,Te=/^(?:mouse|pointer|contextmenu|drag|drop)|click/,Ce=/^([^.]*)(?:\.(.+)|)/;function Ee(){return!0}function ke(){return!1}function Se(){try{return r.activeElement}catch(e){}}function De(e,t,n,r,i,o){var a,s;if("object"==typeof t){"string"!=typeof n&&(r=r||n,n=void 0);for(s in t)De(e,s,n,r,t[s],o);return e}if(null==r&&null==i?(i=n,r=n=void 0):null==i&&("string"==typeof n?(i=r,r=void 0):(i=r,r=n,n=void 0)),!1===i)i=ke;else if(!i)return e;return 1===o&&(a=i,(i=function(e){return w().off(e),a.apply(this,arguments)}).guid=a.guid||(a.guid=w.guid++)),e.each(function(){w.event.add(this,t,i,r,n)})}w.event={global:{},add:function(e,t,n,r,i){var o,a,s,u,l,c,f,p,d,h,g,y=J.get(e);if(y){n.handler&&(n=(o=n).handler,i=o.selector),i&&w.find.matchesSelector(be,i),n.guid||(n.guid=w.guid++),(u=y.events)||(u=y.events={}),(a=y.handle)||(a=y.handle=function(t){return"undefined"!=typeof w&&w.event.triggered!==t.type?w.event.dispatch.apply(e,arguments):void 0}),l=(t=(t||"").match(M)||[""]).length;while(l--)d=g=(s=Ce.exec(t[l])||[])[1],h=(s[2]||"").split(".").sort(),d&&(f=w.event.special[d]||{},d=(i?f.delegateType:f.bindType)||d,f=w.event.special[d]||{},c=w.extend({type:d,origType:g,data:r,handler:n,guid:n.guid,selector:i,needsContext:i&&w.expr.match.needsContext.test(i),namespace:h.join(".")},o),(p=u[d])||((p=u[d]=[]).delegateCount=0,f.setup&&!1!==f.setup.call(e,r,h,a)||e.addEventListener&&e.addEventListener(d,a)),f.add&&(f.add.call(e,c),c.handler.guid||(c.handler.guid=n.guid)),i?p.splice(p.delegateCount++,0,c):p.push(c),w.event.global[d]=!0)}},remove:function(e,t,n,r,i){var o,a,s,u,l,c,f,p,d,h,g,y=J.hasData(e)&&J.get(e);if(y&&(u=y.events)){l=(t=(t||"").match(M)||[""]).length;while(l--)if(s=Ce.exec(t[l])||[],d=g=s[1],h=(s[2]||"").split(".").sort(),d){f=w.event.special[d]||{},p=u[d=(r?f.delegateType:f.bindType)||d]||[],s=s[2]&&new RegExp("(^|\\.)"+h.join("\\.(?:.*\\.|)")+"(\\.|$)"),a=o=p.length;while(o--)c=p[o],!i&&g!==c.origType||n&&n.guid!==c.guid||s&&!s.test(c.namespace)||r&&r!==c.selector&&("**"!==r||!c.selector)||(p.splice(o,1),c.selector&&p.delegateCount--,f.remove&&f.remove.call(e,c));a&&!p.length&&(f.teardown&&!1!==f.teardown.call(e,h,y.handle)||w.removeEvent(e,d,y.handle),delete u[d])}else for(d in u)w.event.remove(e,d+t[l],n,r,!0);w.isEmptyObject(u)&&J.remove(e,"handle events")}},dispatch:function(e){var t=w.event.fix(e),n,r,i,o,a,s,u=new Array(arguments.length),l=(J.get(this,"events")||{})[t.type]||[],c=w.event.special[t.type]||{};for(u[0]=t,n=1;n<arguments.length;n++)u[n]=arguments[n];if(t.delegateTarget=this,!c.preDispatch||!1!==c.preDispatch.call(this,t)){s=w.event.handlers.call(this,t,l),n=0;while((o=s[n++])&&!t.isPropagationStopped()){t.currentTarget=o.elem,r=0;while((a=o.handlers[r++])&&!t.isImmediatePropagationStopped())t.rnamespace&&!t.rnamespace.test(a.namespace)||(t.handleObj=a,t.data=a.data,void 0!==(i=((w.event.special[a.origType]||{}).handle||a.handler).apply(o.elem,u))&&!1===(t.result=i)&&(t.preventDefault(),t.stopPropagation()))}return c.postDispatch&&c.postDispatch.call(this,t),t.result}},handlers:function(e,t){var n,r,i,o,a,s=[],u=t.delegateCount,l=e.target;if(u&&l.nodeType&&!("click"===e.type&&e.button>=1))for(;l!==this;l=l.parentNode||this)if(1===l.nodeType&&("click"!==e.type||!0!==l.disabled)){for(o=[],a={},n=0;n<u;n++)void 0===a[i=(r=t[n]).selector+" "]&&(a[i]=r.needsContext?w(i,this).index(l)>-1:w.find(i,this,null,[l]).length),a[i]&&o.push(r);o.length&&s.push({elem:l,handlers:o})}return l=this,u<t.length&&s.push({elem:l,handlers:t.slice(u)}),s},addProp:function(e,t){Object.defineProperty(w.Event.prototype,e,{enumerable:!0,configurable:!0,get:g(t)?function(){if(this.originalEvent)return t(this.originalEvent)}:function(){if(this.originalEvent)return this.originalEvent[e]},set:function(t){Object.defineProperty(this,e,{enumerable:!0,configurable:!0,writable:!0,value:t})}})},fix:function(e){return e[w.expando]?e:new w.Event(e)},special:{load:{noBubble:!0},focus:{trigger:function(){if(this!==Se()&&this.focus)return this.focus(),!1},delegateType:"focusin"},blur:{trigger:function(){if(this===Se()&&this.blur)return this.blur(),!1},delegateType:"focusout"},click:{trigger:function(){if("checkbox"===this.type&&this.click&&N(this,"input"))return this.click(),!1},_default:function(e){return N(e.target,"a")}},beforeunload:{postDispatch:function(e){void 0!==e.result&&e.originalEvent&&(e.originalEvent.returnValue=e.result)}}}},w.removeEvent=function(e,t,n){e.removeEventListener&&e.removeEventListener(t,n)},w.Event=function(e,t){if(!(this instanceof w.Event))return new w.Event(e,t);e&&e.type?(this.originalEvent=e,this.type=e.type,this.isDefaultPrevented=e.defaultPrevented||void 0===e.defaultPrevented&&!1===e.returnValue?Ee:ke,this.target=e.target&&3===e.target.nodeType?e.target.parentNode:e.target,this.currentTarget=e.currentTarget,this.relatedTarget=e.relatedTarget):this.type=e,t&&w.extend(this,t),this.timeStamp=e&&e.timeStamp||Date.now(),this[w.expando]=!0},w.Event.prototype={constructor:w.Event,isDefaultPrevented:ke,isPropagationStopped:ke,isImmediatePropagationStopped:ke,isSimulated:!1,preventDefault:function(){var e=this.originalEvent;this.isDefaultPrevented=Ee,e&&!this.isSimulated&&e.preventDefault()},stopPropagation:function(){var e=this.originalEvent;this.isPropagationStopped=Ee,e&&!this.isSimulated&&e.stopPropagation()},stopImmediatePropagation:function(){var e=this.originalEvent;this.isImmediatePropagationStopped=Ee,e&&!this.isSimulated&&e.stopImmediatePropagation(),this.stopPropagation()}},w.each({altKey:!0,bubbles:!0,cancelable:!0,changedTouches:!0,ctrlKey:!0,detail:!0,eventPhase:!0,metaKey:!0,pageX:!0,pageY:!0,shiftKey:!0,view:!0,"char":!0,charCode:!0,key:!0,keyCode:!0,button:!0,buttons:!0,clientX:!0,clientY:!0,offsetX:!0,offsetY:!0,pointerId:!0,pointerType:!0,screenX:!0,screenY:!0,targetTouches:!0,toElement:!0,touches:!0,which:function(e){var t=e.button;return null==e.which&&we.test(e.type)?null!=e.charCode?e.charCode:e.keyCode:!e.which&&void 0!==t&&Te.test(e.type)?1&t?1:2&t?3:4&t?2:0:e.which}},w.event.addProp),w.each({mouseenter:"mouseover",mouseleave:"mouseout",pointerenter:"pointerover",pointerleave:"pointerout"},function(e,t){w.event.special[e]={delegateType:t,bindType:t,handle:function(e){var n,r=this,i=e.relatedTarget,o=e.handleObj;return i&&(i===r||w.contains(r,i))||(e.type=o.origType,n=o.handler.apply(this,arguments),e.type=t),n}}}),w.fn.extend({on:function(e,t,n,r){return De(this,e,t,n,r)},one:function(e,t,n,r){return De(this,e,t,n,r,1)},off:function(e,t,n){var r,i;if(e&&e.preventDefault&&e.handleObj)return r=e.handleObj,w(e.delegateTarget).off(r.namespace?r.origType+"."+r.namespace:r.origType,r.selector,r.handler),this;if("object"==typeof e){for(i in e)this.off(i,t,e[i]);return this}return!1!==t&&"function"!=typeof t||(n=t,t=void 0),!1===n&&(n=ke),this.each(function(){w.event.remove(this,e,n,t)})}});var Ne=/<(?!area|br|col|embed|hr|img|input|link|meta|param)(([a-z][^\/\0>\x20\t\r\n\f]*)[^>]*)\/>/gi,Ae=/<script|<style|<link/i,je=/checked\s*(?:[^=]|=\s*.checked.)/i,qe=/^\s*<!(?:\[CDATA\[|--)|(?:\]\]|--)>\s*$/g;function Le(e,t){return N(e,"table")&&N(11!==t.nodeType?t:t.firstChild,"tr")?w(e).children("tbody")[0]||e:e}function He(e){return e.type=(null!==e.getAttribute("type"))+"/"+e.type,e}function Oe(e){return"true/"===(e.type||"").slice(0,5)?e.type=e.type.slice(5):e.removeAttribute("type"),e}function Pe(e,t){var n,r,i,o,a,s,u,l;if(1===t.nodeType){if(J.hasData(e)&&(o=J.access(e),a=J.set(t,o),l=o.events)){delete a.handle,a.events={};for(i in l)for(n=0,r=l[i].length;n<r;n++)w.event.add(t,i,l[i][n])}K.hasData(e)&&(s=K.access(e),u=w.extend({},s),K.set(t,u))}}function Me(e,t){var n=t.nodeName.toLowerCase();"input"===n&&pe.test(e.type)?t.checked=e.checked:"input"!==n&&"textarea"!==n||(t.defaultValue=e.defaultValue)}function Re(e,t,n,r){t=a.apply([],t);var i,o,s,u,l,c,f=0,p=e.length,d=p-1,y=t[0],v=g(y);if(v||p>1&&"string"==typeof y&&!h.checkClone&&je.test(y))return e.each(function(i){var o=e.eq(i);v&&(t[0]=y.call(this,i,o.html())),Re(o,t,n,r)});if(p&&(i=xe(t,e[0].ownerDocument,!1,e,r),o=i.firstChild,1===i.childNodes.length&&(i=o),o||r)){for(u=(s=w.map(ye(i,"script"),He)).length;f<p;f++)l=i,f!==d&&(l=w.clone(l,!0,!0),u&&w.merge(s,ye(l,"script"))),n.call(e[f],l,f);if(u)for(c=s[s.length-1].ownerDocument,w.map(s,Oe),f=0;f<u;f++)l=s[f],he.test(l.type||"")&&!J.access(l,"globalEval")&&w.contains(c,l)&&(l.src&&"module"!==(l.type||"").toLowerCase()?w._evalUrl&&w._evalUrl(l.src):m(l.textContent.replace(qe,""),c,l))}return e}function Ie(e,t,n){for(var r,i=t?w.filter(t,e):e,o=0;null!=(r=i[o]);o++)n||1!==r.nodeType||w.cleanData(ye(r)),r.parentNode&&(n&&w.contains(r.ownerDocument,r)&&ve(ye(r,"script")),r.parentNode.removeChild(r));return e}w.extend({htmlPrefilter:function(e){return e.replace(Ne,"<$1></$2>")},clone:function(e,t,n){var r,i,o,a,s=e.cloneNode(!0),u=w.contains(e.ownerDocument,e);if(!(h.noCloneChecked||1!==e.nodeType&&11!==e.nodeType||w.isXMLDoc(e)))for(a=ye(s),r=0,i=(o=ye(e)).length;r<i;r++)Me(o[r],a[r]);if(t)if(n)for(o=o||ye(e),a=a||ye(s),r=0,i=o.length;r<i;r++)Pe(o[r],a[r]);else Pe(e,s);return(a=ye(s,"script")).length>0&&ve(a,!u&&ye(e,"script")),s},cleanData:function(e){for(var t,n,r,i=w.event.special,o=0;void 0!==(n=e[o]);o++)if(Y(n)){if(t=n[J.expando]){if(t.events)for(r in t.events)i[r]?w.event.remove(n,r):w.removeEvent(n,r,t.handle);n[J.expando]=void 0}n[K.expando]&&(n[K.expando]=void 0)}}}),w.fn.extend({detach:function(e){return Ie(this,e,!0)},remove:function(e){return Ie(this,e)},text:function(e){return z(this,function(e){return void 0===e?w.text(this):this.empty().each(function(){1!==this.nodeType&&11!==this.nodeType&&9!==this.nodeType||(this.textContent=e)})},null,e,arguments.length)},append:function(){return Re(this,arguments,function(e){1!==this.nodeType&&11!==this.nodeType&&9!==this.nodeType||Le(this,e).appendChild(e)})},prepend:function(){return Re(this,arguments,function(e){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var t=Le(this,e);t.insertBefore(e,t.firstChild)}})},before:function(){return Re(this,arguments,function(e){this.parentNode&&this.parentNode.insertBefore(e,this)})},after:function(){return Re(this,arguments,function(e){this.parentNode&&this.parentNode.insertBefore(e,this.nextSibling)})},empty:function(){for(var e,t=0;null!=(e=this[t]);t++)1===e.nodeType&&(w.cleanData(ye(e,!1)),e.textContent="");return this},clone:function(e,t){return e=null!=e&&e,t=null==t?e:t,this.map(function(){return w.clone(this,e,t)})},html:function(e){return z(this,function(e){var t=this[0]||{},n=0,r=this.length;if(void 0===e&&1===t.nodeType)return t.innerHTML;if("string"==typeof e&&!Ae.test(e)&&!ge[(de.exec(e)||["",""])[1].toLowerCase()]){e=w.htmlPrefilter(e);try{for(;n<r;n++)1===(t=this[n]||{}).nodeType&&(w.cleanData(ye(t,!1)),t.innerHTML=e);t=0}catch(e){}}t&&this.empty().append(e)},null,e,arguments.length)},replaceWith:function(){var e=[];return Re(this,arguments,function(t){var n=this.parentNode;w.inArray(this,e)<0&&(w.cleanData(ye(this)),n&&n.replaceChild(t,this))},e)}}),w.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(e,t){w.fn[e]=function(e){for(var n,r=[],i=w(e),o=i.length-1,a=0;a<=o;a++)n=a===o?this:this.clone(!0),w(i[a])[t](n),s.apply(r,n.get());return this.pushStack(r)}});var We=new RegExp("^("+re+")(?!px)[a-z%]+$","i"),$e=function(t){var n=t.ownerDocument.defaultView;return n&&n.opener||(n=e),n.getComputedStyle(t)},Be=new RegExp(oe.join("|"),"i");!function(){function t(){if(c){l.style.cssText="position:absolute;left:-11111px;width:60px;margin-top:1px;padding:0;border:0",c.style.cssText="position:relative;display:block;box-sizing:border-box;overflow:scroll;margin:auto;border:1px;padding:1px;width:60%;top:1%",be.appendChild(l).appendChild(c);var t=e.getComputedStyle(c);i="1%"!==t.top,u=12===n(t.marginLeft),c.style.right="60%",s=36===n(t.right),o=36===n(t.width),c.style.position="absolute",a=36===c.offsetWidth||"absolute",be.removeChild(l),c=null}}function n(e){return Math.round(parseFloat(e))}var i,o,a,s,u,l=r.createElement("div"),c=r.createElement("div");c.style&&(c.style.backgroundClip="content-box",c.cloneNode(!0).style.backgroundClip="",h.clearCloneStyle="content-box"===c.style.backgroundClip,w.extend(h,{boxSizingReliable:function(){return t(),o},pixelBoxStyles:function(){return t(),s},pixelPosition:function(){return t(),i},reliableMarginLeft:function(){return t(),u},scrollboxSize:function(){return t(),a}}))}();function Fe(e,t,n){var r,i,o,a,s=e.style;return(n=n||$e(e))&&(""!==(a=n.getPropertyValue(t)||n[t])||w.contains(e.ownerDocument,e)||(a=w.style(e,t)),!h.pixelBoxStyles()&&We.test(a)&&Be.test(t)&&(r=s.width,i=s.minWidth,o=s.maxWidth,s.minWidth=s.maxWidth=s.width=a,a=n.width,s.width=r,s.minWidth=i,s.maxWidth=o)),void 0!==a?a+"":a}function _e(e,t){return{get:function(){if(!e())return(this.get=t).apply(this,arguments);delete this.get}}}var ze=/^(none|table(?!-c[ea]).+)/,Xe=/^--/,Ue={position:"absolute",visibility:"hidden",display:"block"},Ve={letterSpacing:"0",fontWeight:"400"},Ge=["Webkit","Moz","ms"],Ye=r.createElement("div").style;function Qe(e){if(e in Ye)return e;var t=e[0].toUpperCase()+e.slice(1),n=Ge.length;while(n--)if((e=Ge[n]+t)in Ye)return e}function Je(e){var t=w.cssProps[e];return t||(t=w.cssProps[e]=Qe(e)||e),t}function Ke(e,t,n){var r=ie.exec(t);return r?Math.max(0,r[2]-(n||0))+(r[3]||"px"):t}function Ze(e,t,n,r,i,o){var a="width"===t?1:0,s=0,u=0;if(n===(r?"border":"content"))return 0;for(;a<4;a+=2)"margin"===n&&(u+=w.css(e,n+oe[a],!0,i)),r?("content"===n&&(u-=w.css(e,"padding"+oe[a],!0,i)),"margin"!==n&&(u-=w.css(e,"border"+oe[a]+"Width",!0,i))):(u+=w.css(e,"padding"+oe[a],!0,i),"padding"!==n?u+=w.css(e,"border"+oe[a]+"Width",!0,i):s+=w.css(e,"border"+oe[a]+"Width",!0,i));return!r&&o>=0&&(u+=Math.max(0,Math.ceil(e["offset"+t[0].toUpperCase()+t.slice(1)]-o-u-s-.5))),u}function et(e,t,n){var r=$e(e),i=Fe(e,t,r),o="border-box"===w.css(e,"boxSizing",!1,r),a=o;if(We.test(i)){if(!n)return i;i="auto"}return a=a&&(h.boxSizingReliable()||i===e.style[t]),("auto"===i||!parseFloat(i)&&"inline"===w.css(e,"display",!1,r))&&(i=e["offset"+t[0].toUpperCase()+t.slice(1)],a=!0),(i=parseFloat(i)||0)+Ze(e,t,n||(o?"border":"content"),a,r,i)+"px"}w.extend({cssHooks:{opacity:{get:function(e,t){if(t){var n=Fe(e,"opacity");return""===n?"1":n}}}},cssNumber:{animationIterationCount:!0,columnCount:!0,fillOpacity:!0,flexGrow:!0,flexShrink:!0,fontWeight:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,widows:!0,zIndex:!0,zoom:!0},cssProps:{},style:function(e,t,n,r){if(e&&3!==e.nodeType&&8!==e.nodeType&&e.style){var i,o,a,s=G(t),u=Xe.test(t),l=e.style;if(u||(t=Je(s)),a=w.cssHooks[t]||w.cssHooks[s],void 0===n)return a&&"get"in a&&void 0!==(i=a.get(e,!1,r))?i:l[t];"string"==(o=typeof n)&&(i=ie.exec(n))&&i[1]&&(n=ue(e,t,i),o="number"),null!=n&&n===n&&("number"===o&&(n+=i&&i[3]||(w.cssNumber[s]?"":"px")),h.clearCloneStyle||""!==n||0!==t.indexOf("background")||(l[t]="inherit"),a&&"set"in a&&void 0===(n=a.set(e,n,r))||(u?l.setProperty(t,n):l[t]=n))}},css:function(e,t,n,r){var i,o,a,s=G(t);return Xe.test(t)||(t=Je(s)),(a=w.cssHooks[t]||w.cssHooks[s])&&"get"in a&&(i=a.get(e,!0,n)),void 0===i&&(i=Fe(e,t,r)),"normal"===i&&t in Ve&&(i=Ve[t]),""===n||n?(o=parseFloat(i),!0===n||isFinite(o)?o||0:i):i}}),w.each(["height","width"],function(e,t){w.cssHooks[t]={get:function(e,n,r){if(n)return!ze.test(w.css(e,"display"))||e.getClientRects().length&&e.getBoundingClientRect().width?et(e,t,r):se(e,Ue,function(){return et(e,t,r)})},set:function(e,n,r){var i,o=$e(e),a="border-box"===w.css(e,"boxSizing",!1,o),s=r&&Ze(e,t,r,a,o);return a&&h.scrollboxSize()===o.position&&(s-=Math.ceil(e["offset"+t[0].toUpperCase()+t.slice(1)]-parseFloat(o[t])-Ze(e,t,"border",!1,o)-.5)),s&&(i=ie.exec(n))&&"px"!==(i[3]||"px")&&(e.style[t]=n,n=w.css(e,t)),Ke(e,n,s)}}}),w.cssHooks.marginLeft=_e(h.reliableMarginLeft,function(e,t){if(t)return(parseFloat(Fe(e,"marginLeft"))||e.getBoundingClientRect().left-se(e,{marginLeft:0},function(){return e.getBoundingClientRect().left}))+"px"}),w.each({margin:"",padding:"",border:"Width"},function(e,t){w.cssHooks[e+t]={expand:function(n){for(var r=0,i={},o="string"==typeof n?n.split(" "):[n];r<4;r++)i[e+oe[r]+t]=o[r]||o[r-2]||o[0];return i}},"margin"!==e&&(w.cssHooks[e+t].set=Ke)}),w.fn.extend({css:function(e,t){return z(this,function(e,t,n){var r,i,o={},a=0;if(Array.isArray(t)){for(r=$e(e),i=t.length;a<i;a++)o[t[a]]=w.css(e,t[a],!1,r);return o}return void 0!==n?w.style(e,t,n):w.css(e,t)},e,t,arguments.length>1)}});function tt(e,t,n,r,i){return new tt.prototype.init(e,t,n,r,i)}w.Tween=tt,tt.prototype={constructor:tt,init:function(e,t,n,r,i,o){this.elem=e,this.prop=n,this.easing=i||w.easing._default,this.options=t,this.start=this.now=this.cur(),this.end=r,this.unit=o||(w.cssNumber[n]?"":"px")},cur:function(){var e=tt.propHooks[this.prop];return e&&e.get?e.get(this):tt.propHooks._default.get(this)},run:function(e){var t,n=tt.propHooks[this.prop];return this.options.duration?this.pos=t=w.easing[this.easing](e,this.options.duration*e,0,1,this.options.duration):this.pos=t=e,this.now=(this.end-this.start)*t+this.start,this.options.step&&this.options.step.call(this.elem,this.now,this),n&&n.set?n.set(this):tt.propHooks._default.set(this),this}},tt.prototype.init.prototype=tt.prototype,tt.propHooks={_default:{get:function(e){var t;return 1!==e.elem.nodeType||null!=e.elem[e.prop]&&null==e.elem.style[e.prop]?e.elem[e.prop]:(t=w.css(e.elem,e.prop,""))&&"auto"!==t?t:0},set:function(e){w.fx.step[e.prop]?w.fx.step[e.prop](e):1!==e.elem.nodeType||null==e.elem.style[w.cssProps[e.prop]]&&!w.cssHooks[e.prop]?e.elem[e.prop]=e.now:w.style(e.elem,e.prop,e.now+e.unit)}}},tt.propHooks.scrollTop=tt.propHooks.scrollLeft={set:function(e){e.elem.nodeType&&e.elem.parentNode&&(e.elem[e.prop]=e.now)}},w.easing={linear:function(e){return e},swing:function(e){return.5-Math.cos(e*Math.PI)/2},_default:"swing"},w.fx=tt.prototype.init,w.fx.step={};var nt,rt,it=/^(?:toggle|show|hide)$/,ot=/queueHooks$/;function at(){rt&&(!1===r.hidden&&e.requestAnimationFrame?e.requestAnimationFrame(at):e.setTimeout(at,w.fx.interval),w.fx.tick())}function st(){return e.setTimeout(function(){nt=void 0}),nt=Date.now()}function ut(e,t){var n,r=0,i={height:e};for(t=t?1:0;r<4;r+=2-t)i["margin"+(n=oe[r])]=i["padding"+n]=e;return t&&(i.opacity=i.width=e),i}function lt(e,t,n){for(var r,i=(pt.tweeners[t]||[]).concat(pt.tweeners["*"]),o=0,a=i.length;o<a;o++)if(r=i[o].call(n,t,e))return r}function ct(e,t,n){var r,i,o,a,s,u,l,c,f="width"in t||"height"in t,p=this,d={},h=e.style,g=e.nodeType&&ae(e),y=J.get(e,"fxshow");n.queue||(null==(a=w._queueHooks(e,"fx")).unqueued&&(a.unqueued=0,s=a.empty.fire,a.empty.fire=function(){a.unqueued||s()}),a.unqueued++,p.always(function(){p.always(function(){a.unqueued--,w.queue(e,"fx").length||a.empty.fire()})}));for(r in t)if(i=t[r],it.test(i)){if(delete t[r],o=o||"toggle"===i,i===(g?"hide":"show")){if("show"!==i||!y||void 0===y[r])continue;g=!0}d[r]=y&&y[r]||w.style(e,r)}if((u=!w.isEmptyObject(t))||!w.isEmptyObject(d)){f&&1===e.nodeType&&(n.overflow=[h.overflow,h.overflowX,h.overflowY],null==(l=y&&y.display)&&(l=J.get(e,"display")),"none"===(c=w.css(e,"display"))&&(l?c=l:(fe([e],!0),l=e.style.display||l,c=w.css(e,"display"),fe([e]))),("inline"===c||"inline-block"===c&&null!=l)&&"none"===w.css(e,"float")&&(u||(p.done(function(){h.display=l}),null==l&&(c=h.display,l="none"===c?"":c)),h.display="inline-block")),n.overflow&&(h.overflow="hidden",p.always(function(){h.overflow=n.overflow[0],h.overflowX=n.overflow[1],h.overflowY=n.overflow[2]})),u=!1;for(r in d)u||(y?"hidden"in y&&(g=y.hidden):y=J.access(e,"fxshow",{display:l}),o&&(y.hidden=!g),g&&fe([e],!0),p.done(function(){g||fe([e]),J.remove(e,"fxshow");for(r in d)w.style(e,r,d[r])})),u=lt(g?y[r]:0,r,p),r in y||(y[r]=u.start,g&&(u.end=u.start,u.start=0))}}function ft(e,t){var n,r,i,o,a;for(n in e)if(r=G(n),i=t[r],o=e[n],Array.isArray(o)&&(i=o[1],o=e[n]=o[0]),n!==r&&(e[r]=o,delete e[n]),(a=w.cssHooks[r])&&"expand"in a){o=a.expand(o),delete e[r];for(n in o)n in e||(e[n]=o[n],t[n]=i)}else t[r]=i}function pt(e,t,n){var r,i,o=0,a=pt.prefilters.length,s=w.Deferred().always(function(){delete u.elem}),u=function(){if(i)return!1;for(var t=nt||st(),n=Math.max(0,l.startTime+l.duration-t),r=1-(n/l.duration||0),o=0,a=l.tweens.length;o<a;o++)l.tweens[o].run(r);return s.notifyWith(e,[l,r,n]),r<1&&a?n:(a||s.notifyWith(e,[l,1,0]),s.resolveWith(e,[l]),!1)},l=s.promise({elem:e,props:w.extend({},t),opts:w.extend(!0,{specialEasing:{},easing:w.easing._default},n),originalProperties:t,originalOptions:n,startTime:nt||st(),duration:n.duration,tweens:[],createTween:function(t,n){var r=w.Tween(e,l.opts,t,n,l.opts.specialEasing[t]||l.opts.easing);return l.tweens.push(r),r},stop:function(t){var n=0,r=t?l.tweens.length:0;if(i)return this;for(i=!0;n<r;n++)l.tweens[n].run(1);return t?(s.notifyWith(e,[l,1,0]),s.resolveWith(e,[l,t])):s.rejectWith(e,[l,t]),this}}),c=l.props;for(ft(c,l.opts.specialEasing);o<a;o++)if(r=pt.prefilters[o].call(l,e,c,l.opts))return g(r.stop)&&(w._queueHooks(l.elem,l.opts.queue).stop=r.stop.bind(r)),r;return w.map(c,lt,l),g(l.opts.start)&&l.opts.start.call(e,l),l.progress(l.opts.progress).done(l.opts.done,l.opts.complete).fail(l.opts.fail).always(l.opts.always),w.fx.timer(w.extend(u,{elem:e,anim:l,queue:l.opts.queue})),l}w.Animation=w.extend(pt,{tweeners:{"*":[function(e,t){var n=this.createTween(e,t);return ue(n.elem,e,ie.exec(t),n),n}]},tweener:function(e,t){g(e)?(t=e,e=["*"]):e=e.match(M);for(var n,r=0,i=e.length;r<i;r++)n=e[r],pt.tweeners[n]=pt.tweeners[n]||[],pt.tweeners[n].unshift(t)},prefilters:[ct],prefilter:function(e,t){t?pt.prefilters.unshift(e):pt.prefilters.push(e)}}),w.speed=function(e,t,n){var r=e&&"object"==typeof e?w.extend({},e):{complete:n||!n&&t||g(e)&&e,duration:e,easing:n&&t||t&&!g(t)&&t};return w.fx.off?r.duration=0:"number"!=typeof r.duration&&(r.duration in w.fx.speeds?r.duration=w.fx.speeds[r.duration]:r.duration=w.fx.speeds._default),null!=r.queue&&!0!==r.queue||(r.queue="fx"),r.old=r.complete,r.complete=function(){g(r.old)&&r.old.call(this),r.queue&&w.dequeue(this,r.queue)},r},w.fn.extend({fadeTo:function(e,t,n,r){return this.filter(ae).css("opacity",0).show().end().animate({opacity:t},e,n,r)},animate:function(e,t,n,r){var i=w.isEmptyObject(e),o=w.speed(t,n,r),a=function(){var t=pt(this,w.extend({},e),o);(i||J.get(this,"finish"))&&t.stop(!0)};return a.finish=a,i||!1===o.queue?this.each(a):this.queue(o.queue,a)},stop:function(e,t,n){var r=function(e){var t=e.stop;delete e.stop,t(n)};return"string"!=typeof e&&(n=t,t=e,e=void 0),t&&!1!==e&&this.queue(e||"fx",[]),this.each(function(){var t=!0,i=null!=e&&e+"queueHooks",o=w.timers,a=J.get(this);if(i)a[i]&&a[i].stop&&r(a[i]);else for(i in a)a[i]&&a[i].stop&&ot.test(i)&&r(a[i]);for(i=o.length;i--;)o[i].elem!==this||null!=e&&o[i].queue!==e||(o[i].anim.stop(n),t=!1,o.splice(i,1));!t&&n||w.dequeue(this,e)})},finish:function(e){return!1!==e&&(e=e||"fx"),this.each(function(){var t,n=J.get(this),r=n[e+"queue"],i=n[e+"queueHooks"],o=w.timers,a=r?r.length:0;for(n.finish=!0,w.queue(this,e,[]),i&&i.stop&&i.stop.call(this,!0),t=o.length;t--;)o[t].elem===this&&o[t].queue===e&&(o[t].anim.stop(!0),o.splice(t,1));for(t=0;t<a;t++)r[t]&&r[t].finish&&r[t].finish.call(this);delete n.finish})}}),w.each(["toggle","show","hide"],function(e,t){var n=w.fn[t];w.fn[t]=function(e,r,i){return null==e||"boolean"==typeof e?n.apply(this,arguments):this.animate(ut(t,!0),e,r,i)}}),w.each({slideDown:ut("show"),slideUp:ut("hide"),slideToggle:ut("toggle"),fadeIn:{opacity:"show"},fadeOut:{opacity:"hide"},fadeToggle:{opacity:"toggle"}},function(e,t){w.fn[e]=function(e,n,r){return this.animate(t,e,n,r)}}),w.timers=[],w.fx.tick=function(){var e,t=0,n=w.timers;for(nt=Date.now();t<n.length;t++)(e=n[t])()||n[t]!==e||n.splice(t--,1);n.length||w.fx.stop(),nt=void 0},w.fx.timer=function(e){w.timers.push(e),w.fx.start()},w.fx.interval=13,w.fx.start=function(){rt||(rt=!0,at())},w.fx.stop=function(){rt=null},w.fx.speeds={slow:600,fast:200,_default:400},w.fn.delay=function(t,n){return t=w.fx?w.fx.speeds[t]||t:t,n=n||"fx",this.queue(n,function(n,r){var i=e.setTimeout(n,t);r.stop=function(){e.clearTimeout(i)}})},function(){var e=r.createElement("input"),t=r.createElement("select").appendChild(r.createElement("option"));e.type="checkbox",h.checkOn=""!==e.value,h.optSelected=t.selected,(e=r.createElement("input")).value="t",e.type="radio",h.radioValue="t"===e.value}();var dt,ht=w.expr.attrHandle;w.fn.extend({attr:function(e,t){return z(this,w.attr,e,t,arguments.length>1)},removeAttr:function(e){return this.each(function(){w.removeAttr(this,e)})}}),w.extend({attr:function(e,t,n){var r,i,o=e.nodeType;if(3!==o&&8!==o&&2!==o)return"undefined"==typeof e.getAttribute?w.prop(e,t,n):(1===o&&w.isXMLDoc(e)||(i=w.attrHooks[t.toLowerCase()]||(w.expr.match.bool.test(t)?dt:void 0)),void 0!==n?null===n?void w.removeAttr(e,t):i&&"set"in i&&void 0!==(r=i.set(e,n,t))?r:(e.setAttribute(t,n+""),n):i&&"get"in i&&null!==(r=i.get(e,t))?r:null==(r=w.find.attr(e,t))?void 0:r)},attrHooks:{type:{set:function(e,t){if(!h.radioValue&&"radio"===t&&N(e,"input")){var n=e.value;return e.setAttribute("type",t),n&&(e.value=n),t}}}},removeAttr:function(e,t){var n,r=0,i=t&&t.match(M);if(i&&1===e.nodeType)while(n=i[r++])e.removeAttribute(n)}}),dt={set:function(e,t,n){return!1===t?w.removeAttr(e,n):e.setAttribute(n,n),n}},w.each(w.expr.match.bool.source.match(/\w+/g),function(e,t){var n=ht[t]||w.find.attr;ht[t]=function(e,t,r){var i,o,a=t.toLowerCase();return r||(o=ht[a],ht[a]=i,i=null!=n(e,t,r)?a:null,ht[a]=o),i}});var gt=/^(?:input|select|textarea|button)$/i,yt=/^(?:a|area)$/i;w.fn.extend({prop:function(e,t){return z(this,w.prop,e,t,arguments.length>1)},removeProp:function(e){return this.each(function(){delete this[w.propFix[e]||e]})}}),w.extend({prop:function(e,t,n){var r,i,o=e.nodeType;if(3!==o&&8!==o&&2!==o)return 1===o&&w.isXMLDoc(e)||(t=w.propFix[t]||t,i=w.propHooks[t]),void 0!==n?i&&"set"in i&&void 0!==(r=i.set(e,n,t))?r:e[t]=n:i&&"get"in i&&null!==(r=i.get(e,t))?r:e[t]},propHooks:{tabIndex:{get:function(e){var t=w.find.attr(e,"tabindex");return t?parseInt(t,10):gt.test(e.nodeName)||yt.test(e.nodeName)&&e.href?0:-1}}},propFix:{"for":"htmlFor","class":"className"}}),h.optSelected||(w.propHooks.selected={get:function(e){var t=e.parentNode;return t&&t.parentNode&&t.parentNode.selectedIndex,null},set:function(e){var t=e.parentNode;t&&(t.selectedIndex,t.parentNode&&t.parentNode.selectedIndex)}}),w.each(["tabIndex","readOnly","maxLength","cellSpacing","cellPadding","rowSpan","colSpan","useMap","frameBorder","contentEditable"],function(){w.propFix[this.toLowerCase()]=this});function vt(e){return(e.match(M)||[]).join(" ")}function mt(e){return e.getAttribute&&e.getAttribute("class")||""}function xt(e){return Array.isArray(e)?e:"string"==typeof e?e.match(M)||[]:[]}w.fn.extend({addClass:function(e){var t,n,r,i,o,a,s,u=0;if(g(e))return this.each(function(t){w(this).addClass(e.call(this,t,mt(this)))});if((t=xt(e)).length)while(n=this[u++])if(i=mt(n),r=1===n.nodeType&&" "+vt(i)+" "){a=0;while(o=t[a++])r.indexOf(" "+o+" ")<0&&(r+=o+" ");i!==(s=vt(r))&&n.setAttribute("class",s)}return this},removeClass:function(e){var t,n,r,i,o,a,s,u=0;if(g(e))return this.each(function(t){w(this).removeClass(e.call(this,t,mt(this)))});if(!arguments.length)return this.attr("class","");if((t=xt(e)).length)while(n=this[u++])if(i=mt(n),r=1===n.nodeType&&" "+vt(i)+" "){a=0;while(o=t[a++])while(r.indexOf(" "+o+" ")>-1)r=r.replace(" "+o+" "," ");i!==(s=vt(r))&&n.setAttribute("class",s)}return this},toggleClass:function(e,t){var n=typeof e,r="string"===n||Array.isArray(e);return"boolean"==typeof t&&r?t?this.addClass(e):this.removeClass(e):g(e)?this.each(function(n){w(this).toggleClass(e.call(this,n,mt(this),t),t)}):this.each(function(){var t,i,o,a;if(r){i=0,o=w(this),a=xt(e);while(t=a[i++])o.hasClass(t)?o.removeClass(t):o.addClass(t)}else void 0!==e&&"boolean"!==n||((t=mt(this))&&J.set(this,"__className__",t),this.setAttribute&&this.setAttribute("class",t||!1===e?"":J.get(this,"__className__")||""))})},hasClass:function(e){var t,n,r=0;t=" "+e+" ";while(n=this[r++])if(1===n.nodeType&&(" "+vt(mt(n))+" ").indexOf(t)>-1)return!0;return!1}});var bt=/\r/g;w.fn.extend({val:function(e){var t,n,r,i=this[0];{if(arguments.length)return r=g(e),this.each(function(n){var i;1===this.nodeType&&(null==(i=r?e.call(this,n,w(this).val()):e)?i="":"number"==typeof i?i+="":Array.isArray(i)&&(i=w.map(i,function(e){return null==e?"":e+""})),(t=w.valHooks[this.type]||w.valHooks[this.nodeName.toLowerCase()])&&"set"in t&&void 0!==t.set(this,i,"value")||(this.value=i))});if(i)return(t=w.valHooks[i.type]||w.valHooks[i.nodeName.toLowerCase()])&&"get"in t&&void 0!==(n=t.get(i,"value"))?n:"string"==typeof(n=i.value)?n.replace(bt,""):null==n?"":n}}}),w.extend({valHooks:{option:{get:function(e){var t=w.find.attr(e,"value");return null!=t?t:vt(w.text(e))}},select:{get:function(e){var t,n,r,i=e.options,o=e.selectedIndex,a="select-one"===e.type,s=a?null:[],u=a?o+1:i.length;for(r=o<0?u:a?o:0;r<u;r++)if(((n=i[r]).selected||r===o)&&!n.disabled&&(!n.parentNode.disabled||!N(n.parentNode,"optgroup"))){if(t=w(n).val(),a)return t;s.push(t)}return s},set:function(e,t){var n,r,i=e.options,o=w.makeArray(t),a=i.length;while(a--)((r=i[a]).selected=w.inArray(w.valHooks.option.get(r),o)>-1)&&(n=!0);return n||(e.selectedIndex=-1),o}}}}),w.each(["radio","checkbox"],function(){w.valHooks[this]={set:function(e,t){if(Array.isArray(t))return e.checked=w.inArray(w(e).val(),t)>-1}},h.checkOn||(w.valHooks[this].get=function(e){return null===e.getAttribute("value")?"on":e.value})}),h.focusin="onfocusin"in e;var wt=/^(?:focusinfocus|focusoutblur)$/,Tt=function(e){e.stopPropagation()};w.extend(w.event,{trigger:function(t,n,i,o){var a,s,u,l,c,p,d,h,v=[i||r],m=f.call(t,"type")?t.type:t,x=f.call(t,"namespace")?t.namespace.split("."):[];if(s=h=u=i=i||r,3!==i.nodeType&&8!==i.nodeType&&!wt.test(m+w.event.triggered)&&(m.indexOf(".")>-1&&(m=(x=m.split(".")).shift(),x.sort()),c=m.indexOf(":")<0&&"on"+m,t=t[w.expando]?t:new w.Event(m,"object"==typeof t&&t),t.isTrigger=o?2:3,t.namespace=x.join("."),t.rnamespace=t.namespace?new RegExp("(^|\\.)"+x.join("\\.(?:.*\\.|)")+"(\\.|$)"):null,t.result=void 0,t.target||(t.target=i),n=null==n?[t]:w.makeArray(n,[t]),d=w.event.special[m]||{},o||!d.trigger||!1!==d.trigger.apply(i,n))){if(!o&&!d.noBubble&&!y(i)){for(l=d.delegateType||m,wt.test(l+m)||(s=s.parentNode);s;s=s.parentNode)v.push(s),u=s;u===(i.ownerDocument||r)&&v.push(u.defaultView||u.parentWindow||e)}a=0;while((s=v[a++])&&!t.isPropagationStopped())h=s,t.type=a>1?l:d.bindType||m,(p=(J.get(s,"events")||{})[t.type]&&J.get(s,"handle"))&&p.apply(s,n),(p=c&&s[c])&&p.apply&&Y(s)&&(t.result=p.apply(s,n),!1===t.result&&t.preventDefault());return t.type=m,o||t.isDefaultPrevented()||d._default&&!1!==d._default.apply(v.pop(),n)||!Y(i)||c&&g(i[m])&&!y(i)&&((u=i[c])&&(i[c]=null),w.event.triggered=m,t.isPropagationStopped()&&h.addEventListener(m,Tt),i[m](),t.isPropagationStopped()&&h.removeEventListener(m,Tt),w.event.triggered=void 0,u&&(i[c]=u)),t.result}},simulate:function(e,t,n){var r=w.extend(new w.Event,n,{type:e,isSimulated:!0});w.event.trigger(r,null,t)}}),w.fn.extend({trigger:function(e,t){return this.each(function(){w.event.trigger(e,t,this)})},triggerHandler:function(e,t){var n=this[0];if(n)return w.event.trigger(e,t,n,!0)}}),h.focusin||w.each({focus:"focusin",blur:"focusout"},function(e,t){var n=function(e){w.event.simulate(t,e.target,w.event.fix(e))};w.event.special[t]={setup:function(){var r=this.ownerDocument||this,i=J.access(r,t);i||r.addEventListener(e,n,!0),J.access(r,t,(i||0)+1)},teardown:function(){var r=this.ownerDocument||this,i=J.access(r,t)-1;i?J.access(r,t,i):(r.removeEventListener(e,n,!0),J.remove(r,t))}}});var Ct=e.location,Et=Date.now(),kt=/\?/;w.parseXML=function(t){var n;if(!t||"string"!=typeof t)return null;try{n=(new e.DOMParser).parseFromString(t,"text/xml")}catch(e){n=void 0}return n&&!n.getElementsByTagName("parsererror").length||w.error("Invalid XML: "+t),n};var St=/\[\]$/,Dt=/\r?\n/g,Nt=/^(?:submit|button|image|reset|file)$/i,At=/^(?:input|select|textarea|keygen)/i;function jt(e,t,n,r){var i;if(Array.isArray(t))w.each(t,function(t,i){n||St.test(e)?r(e,i):jt(e+"["+("object"==typeof i&&null!=i?t:"")+"]",i,n,r)});else if(n||"object"!==x(t))r(e,t);else for(i in t)jt(e+"["+i+"]",t[i],n,r)}w.param=function(e,t){var n,r=[],i=function(e,t){var n=g(t)?t():t;r[r.length]=encodeURIComponent(e)+"="+encodeURIComponent(null==n?"":n)};if(Array.isArray(e)||e.jquery&&!w.isPlainObject(e))w.each(e,function(){i(this.name,this.value)});else for(n in e)jt(n,e[n],t,i);return r.join("&")},w.fn.extend({serialize:function(){return w.param(this.serializeArray())},serializeArray:function(){return this.map(function(){var e=w.prop(this,"elements");return e?w.makeArray(e):this}).filter(function(){var e=this.type;return this.name&&!w(this).is(":disabled")&&At.test(this.nodeName)&&!Nt.test(e)&&(this.checked||!pe.test(e))}).map(function(e,t){var n=w(this).val();return null==n?null:Array.isArray(n)?w.map(n,function(e){return{name:t.name,value:e.replace(Dt,"\r\n")}}):{name:t.name,value:n.replace(Dt,"\r\n")}}).get()}});var qt=/%20/g,Lt=/#.*$/,Ht=/([?&])_=[^&]*/,Ot=/^(.*?):[ \t]*([^\r\n]*)$/gm,Pt=/^(?:about|app|app-storage|.+-extension|file|res|widget):$/,Mt=/^(?:GET|HEAD)$/,Rt=/^\/\//,It={},Wt={},$t="*/".concat("*"),Bt=r.createElement("a");Bt.href=Ct.href;function Ft(e){return function(t,n){"string"!=typeof t&&(n=t,t="*");var r,i=0,o=t.toLowerCase().match(M)||[];if(g(n))while(r=o[i++])"+"===r[0]?(r=r.slice(1)||"*",(e[r]=e[r]||[]).unshift(n)):(e[r]=e[r]||[]).push(n)}}function _t(e,t,n,r){var i={},o=e===Wt;function a(s){var u;return i[s]=!0,w.each(e[s]||[],function(e,s){var l=s(t,n,r);return"string"!=typeof l||o||i[l]?o?!(u=l):void 0:(t.dataTypes.unshift(l),a(l),!1)}),u}return a(t.dataTypes[0])||!i["*"]&&a("*")}function zt(e,t){var n,r,i=w.ajaxSettings.flatOptions||{};for(n in t)void 0!==t[n]&&((i[n]?e:r||(r={}))[n]=t[n]);return r&&w.extend(!0,e,r),e}function Xt(e,t,n){var r,i,o,a,s=e.contents,u=e.dataTypes;while("*"===u[0])u.shift(),void 0===r&&(r=e.mimeType||t.getResponseHeader("Content-Type"));if(r)for(i in s)if(s[i]&&s[i].test(r)){u.unshift(i);break}if(u[0]in n)o=u[0];else{for(i in n){if(!u[0]||e.converters[i+" "+u[0]]){o=i;break}a||(a=i)}o=o||a}if(o)return o!==u[0]&&u.unshift(o),n[o]}function Ut(e,t,n,r){var i,o,a,s,u,l={},c=e.dataTypes.slice();if(c[1])for(a in e.converters)l[a.toLowerCase()]=e.converters[a];o=c.shift();while(o)if(e.responseFields[o]&&(n[e.responseFields[o]]=t),!u&&r&&e.dataFilter&&(t=e.dataFilter(t,e.dataType)),u=o,o=c.shift())if("*"===o)o=u;else if("*"!==u&&u!==o){if(!(a=l[u+" "+o]||l["* "+o]))for(i in l)if((s=i.split(" "))[1]===o&&(a=l[u+" "+s[0]]||l["* "+s[0]])){!0===a?a=l[i]:!0!==l[i]&&(o=s[0],c.unshift(s[1]));break}if(!0!==a)if(a&&e["throws"])t=a(t);else try{t=a(t)}catch(e){return{state:"parsererror",error:a?e:"No conversion from "+u+" to "+o}}}return{state:"success",data:t}}w.extend({active:0,lastModified:{},etag:{},ajaxSettings:{url:Ct.href,type:"GET",isLocal:Pt.test(Ct.protocol),global:!0,processData:!0,async:!0,contentType:"application/x-www-form-urlencoded; charset=UTF-8",accepts:{"*":$t,text:"text/plain",html:"text/html",xml:"application/xml, text/xml",json:"application/json, text/javascript"},contents:{xml:/\bxml\b/,html:/\bhtml/,json:/\bjson\b/},responseFields:{xml:"responseXML",text:"responseText",json:"responseJSON"},converters:{"* text":String,"text html":!0,"text json":JSON.parse,"text xml":w.parseXML},flatOptions:{url:!0,context:!0}},ajaxSetup:function(e,t){return t?zt(zt(e,w.ajaxSettings),t):zt(w.ajaxSettings,e)},ajaxPrefilter:Ft(It),ajaxTransport:Ft(Wt),ajax:function(t,n){"object"==typeof t&&(n=t,t=void 0),n=n||{};var i,o,a,s,u,l,c,f,p,d,h=w.ajaxSetup({},n),g=h.context||h,y=h.context&&(g.nodeType||g.jquery)?w(g):w.event,v=w.Deferred(),m=w.Callbacks("once memory"),x=h.statusCode||{},b={},T={},C="canceled",E={readyState:0,getResponseHeader:function(e){var t;if(c){if(!s){s={};while(t=Ot.exec(a))s[t[1].toLowerCase()]=t[2]}t=s[e.toLowerCase()]}return null==t?null:t},getAllResponseHeaders:function(){return c?a:null},setRequestHeader:function(e,t){return null==c&&(e=T[e.toLowerCase()]=T[e.toLowerCase()]||e,b[e]=t),this},overrideMimeType:function(e){return null==c&&(h.mimeType=e),this},statusCode:function(e){var t;if(e)if(c)E.always(e[E.status]);else for(t in e)x[t]=[x[t],e[t]];return this},abort:function(e){var t=e||C;return i&&i.abort(t),k(0,t),this}};if(v.promise(E),h.url=((t||h.url||Ct.href)+"").replace(Rt,Ct.protocol+"//"),h.type=n.method||n.type||h.method||h.type,h.dataTypes=(h.dataType||"*").toLowerCase().match(M)||[""],null==h.crossDomain){l=r.createElement("a");try{l.href=h.url,l.href=l.href,h.crossDomain=Bt.protocol+"//"+Bt.host!=l.protocol+"//"+l.host}catch(e){h.crossDomain=!0}}if(h.data&&h.processData&&"string"!=typeof h.data&&(h.data=w.param(h.data,h.traditional)),_t(It,h,n,E),c)return E;(f=w.event&&h.global)&&0==w.active++&&w.event.trigger("ajaxStart"),h.type=h.type.toUpperCase(),h.hasContent=!Mt.test(h.type),o=h.url.replace(Lt,""),h.hasContent?h.data&&h.processData&&0===(h.contentType||"").indexOf("application/x-www-form-urlencoded")&&(h.data=h.data.replace(qt,"+")):(d=h.url.slice(o.length),h.data&&(h.processData||"string"==typeof h.data)&&(o+=(kt.test(o)?"&":"?")+h.data,delete h.data),!1===h.cache&&(o=o.replace(Ht,"$1"),d=(kt.test(o)?"&":"?")+"_="+Et+++d),h.url=o+d),h.ifModified&&(w.lastModified[o]&&E.setRequestHeader("If-Modified-Since",w.lastModified[o]),w.etag[o]&&E.setRequestHeader("If-None-Match",w.etag[o])),(h.data&&h.hasContent&&!1!==h.contentType||n.contentType)&&E.setRequestHeader("Content-Type",h.contentType),E.setRequestHeader("Accept",h.dataTypes[0]&&h.accepts[h.dataTypes[0]]?h.accepts[h.dataTypes[0]]+("*"!==h.dataTypes[0]?", "+$t+"; q=0.01":""):h.accepts["*"]);for(p in h.headers)E.setRequestHeader(p,h.headers[p]);if(h.beforeSend&&(!1===h.beforeSend.call(g,E,h)||c))return E.abort();if(C="abort",m.add(h.complete),E.done(h.success),E.fail(h.error),i=_t(Wt,h,n,E)){if(E.readyState=1,f&&y.trigger("ajaxSend",[E,h]),c)return E;h.async&&h.timeout>0&&(u=e.setTimeout(function(){E.abort("timeout")},h.timeout));try{c=!1,i.send(b,k)}catch(e){if(c)throw e;k(-1,e)}}else k(-1,"No Transport");function k(t,n,r,s){var l,p,d,b,T,C=n;c||(c=!0,u&&e.clearTimeout(u),i=void 0,a=s||"",E.readyState=t>0?4:0,l=t>=200&&t<300||304===t,r&&(b=Xt(h,E,r)),b=Ut(h,b,E,l),l?(h.ifModified&&((T=E.getResponseHeader("Last-Modified"))&&(w.lastModified[o]=T),(T=E.getResponseHeader("etag"))&&(w.etag[o]=T)),204===t||"HEAD"===h.type?C="nocontent":304===t?C="notmodified":(C=b.state,p=b.data,l=!(d=b.error))):(d=C,!t&&C||(C="error",t<0&&(t=0))),E.status=t,E.statusText=(n||C)+"",l?v.resolveWith(g,[p,C,E]):v.rejectWith(g,[E,C,d]),E.statusCode(x),x=void 0,f&&y.trigger(l?"ajaxSuccess":"ajaxError",[E,h,l?p:d]),m.fireWith(g,[E,C]),f&&(y.trigger("ajaxComplete",[E,h]),--w.active||w.event.trigger("ajaxStop")))}return E},getJSON:function(e,t,n){return w.get(e,t,n,"json")},getScript:function(e,t){return w.get(e,void 0,t,"script")}}),w.each(["get","post"],function(e,t){w[t]=function(e,n,r,i){return g(n)&&(i=i||r,r=n,n=void 0),w.ajax(w.extend({url:e,type:t,dataType:i,data:n,success:r},w.isPlainObject(e)&&e))}}),w._evalUrl=function(e){return w.ajax({url:e,type:"GET",dataType:"script",cache:!0,async:!1,global:!1,"throws":!0})},w.fn.extend({wrapAll:function(e){var t;return this[0]&&(g(e)&&(e=e.call(this[0])),t=w(e,this[0].ownerDocument).eq(0).clone(!0),this[0].parentNode&&t.insertBefore(this[0]),t.map(function(){var e=this;while(e.firstElementChild)e=e.firstElementChild;return e}).append(this)),this},wrapInner:function(e){return g(e)?this.each(function(t){w(this).wrapInner(e.call(this,t))}):this.each(function(){var t=w(this),n=t.contents();n.length?n.wrapAll(e):t.append(e)})},wrap:function(e){var t=g(e);return this.each(function(n){w(this).wrapAll(t?e.call(this,n):e)})},unwrap:function(e){return this.parent(e).not("body").each(function(){w(this).replaceWith(this.childNodes)}),this}}),w.expr.pseudos.hidden=function(e){return!w.expr.pseudos.visible(e)},w.expr.pseudos.visible=function(e){return!!(e.offsetWidth||e.offsetHeight||e.getClientRects().length)},w.ajaxSettings.xhr=function(){try{return new e.XMLHttpRequest}catch(e){}};var Vt={0:200,1223:204},Gt=w.ajaxSettings.xhr();h.cors=!!Gt&&"withCredentials"in Gt,h.ajax=Gt=!!Gt,w.ajaxTransport(function(t){var n,r;if(h.cors||Gt&&!t.crossDomain)return{send:function(i,o){var a,s=t.xhr();if(s.open(t.type,t.url,t.async,t.username,t.password),t.xhrFields)for(a in t.xhrFields)s[a]=t.xhrFields[a];t.mimeType&&s.overrideMimeType&&s.overrideMimeType(t.mimeType),t.crossDomain||i["X-Requested-With"]||(i["X-Requested-With"]="XMLHttpRequest");for(a in i)s.setRequestHeader(a,i[a]);n=function(e){return function(){n&&(n=r=s.onload=s.onerror=s.onabort=s.ontimeout=s.onreadystatechange=null,"abort"===e?s.abort():"error"===e?"number"!=typeof s.status?o(0,"error"):o(s.status,s.statusText):o(Vt[s.status]||s.status,s.statusText,"text"!==(s.responseType||"text")||"string"!=typeof s.responseText?{binary:s.response}:{text:s.responseText},s.getAllResponseHeaders()))}},s.onload=n(),r=s.onerror=s.ontimeout=n("error"),void 0!==s.onabort?s.onabort=r:s.onreadystatechange=function(){4===s.readyState&&e.setTimeout(function(){n&&r()})},n=n("abort");try{s.send(t.hasContent&&t.data||null)}catch(e){if(n)throw e}},abort:function(){n&&n()}}}),w.ajaxPrefilter(function(e){e.crossDomain&&(e.contents.script=!1)}),w.ajaxSetup({accepts:{script:"text/javascript, application/javascript, application/ecmascript, application/x-ecmascript"},contents:{script:/\b(?:java|ecma)script\b/},converters:{"text script":function(e){return w.globalEval(e),e}}}),w.ajaxPrefilter("script",function(e){void 0===e.cache&&(e.cache=!1),e.crossDomain&&(e.type="GET")}),w.ajaxTransport("script",function(e){if(e.crossDomain){var t,n;return{send:function(i,o){t=w("<script>").prop({charset:e.scriptCharset,src:e.url}).on("load error",n=function(e){t.remove(),n=null,e&&o("error"===e.type?404:200,e.type)}),r.head.appendChild(t[0])},abort:function(){n&&n()}}}});var Yt=[],Qt=/(=)\?(?=&|$)|\?\?/;w.ajaxSetup({jsonp:"callback",jsonpCallback:function(){var e=Yt.pop()||w.expando+"_"+Et++;return this[e]=!0,e}}),w.ajaxPrefilter("json jsonp",function(t,n,r){var i,o,a,s=!1!==t.jsonp&&(Qt.test(t.url)?"url":"string"==typeof t.data&&0===(t.contentType||"").indexOf("application/x-www-form-urlencoded")&&Qt.test(t.data)&&"data");if(s||"jsonp"===t.dataTypes[0])return i=t.jsonpCallback=g(t.jsonpCallback)?t.jsonpCallback():t.jsonpCallback,s?t[s]=t[s].replace(Qt,"$1"+i):!1!==t.jsonp&&(t.url+=(kt.test(t.url)?"&":"?")+t.jsonp+"="+i),t.converters["script json"]=function(){return a||w.error(i+" was not called"),a[0]},t.dataTypes[0]="json",o=e[i],e[i]=function(){a=arguments},r.always(function(){void 0===o?w(e).removeProp(i):e[i]=o,t[i]&&(t.jsonpCallback=n.jsonpCallback,Yt.push(i)),a&&g(o)&&o(a[0]),a=o=void 0}),"script"}),h.createHTMLDocument=function(){var e=r.implementation.createHTMLDocument("").body;return e.innerHTML="<form></form><form></form>",2===e.childNodes.length}(),w.parseHTML=function(e,t,n){if("string"!=typeof e)return[];"boolean"==typeof t&&(n=t,t=!1);var i,o,a;return t||(h.createHTMLDocument?((i=(t=r.implementation.createHTMLDocument("")).createElement("base")).href=r.location.href,t.head.appendChild(i)):t=r),o=A.exec(e),a=!n&&[],o?[t.createElement(o[1])]:(o=xe([e],t,a),a&&a.length&&w(a).remove(),w.merge([],o.childNodes))},w.fn.load=function(e,t,n){var r,i,o,a=this,s=e.indexOf(" ");return s>-1&&(r=vt(e.slice(s)),e=e.slice(0,s)),g(t)?(n=t,t=void 0):t&&"object"==typeof t&&(i="POST"),a.length>0&&w.ajax({url:e,type:i||"GET",dataType:"html",data:t}).done(function(e){o=arguments,a.html(r?w("<div>").append(w.parseHTML(e)).find(r):e)}).always(n&&function(e,t){a.each(function(){n.apply(this,o||[e.responseText,t,e])})}),this},w.each(["ajaxStart","ajaxStop","ajaxComplete","ajaxError","ajaxSuccess","ajaxSend"],function(e,t){w.fn[t]=function(e){return this.on(t,e)}}),w.expr.pseudos.animated=function(e){return w.grep(w.timers,function(t){return e===t.elem}).length},w.offset={setOffset:function(e,t,n){var r,i,o,a,s,u,l,c=w.css(e,"position"),f=w(e),p={};"static"===c&&(e.style.position="relative"),s=f.offset(),o=w.css(e,"top"),u=w.css(e,"left"),(l=("absolute"===c||"fixed"===c)&&(o+u).indexOf("auto")>-1)?(a=(r=f.position()).top,i=r.left):(a=parseFloat(o)||0,i=parseFloat(u)||0),g(t)&&(t=t.call(e,n,w.extend({},s))),null!=t.top&&(p.top=t.top-s.top+a),null!=t.left&&(p.left=t.left-s.left+i),"using"in t?t.using.call(e,p):f.css(p)}},w.fn.extend({offset:function(e){if(arguments.length)return void 0===e?this:this.each(function(t){w.offset.setOffset(this,e,t)});var t,n,r=this[0];if(r)return r.getClientRects().length?(t=r.getBoundingClientRect(),n=r.ownerDocument.defaultView,{top:t.top+n.pageYOffset,left:t.left+n.pageXOffset}):{top:0,left:0}},position:function(){if(this[0]){var e,t,n,r=this[0],i={top:0,left:0};if("fixed"===w.css(r,"position"))t=r.getBoundingClientRect();else{t=this.offset(),n=r.ownerDocument,e=r.offsetParent||n.documentElement;while(e&&(e===n.body||e===n.documentElement)&&"static"===w.css(e,"position"))e=e.parentNode;e&&e!==r&&1===e.nodeType&&((i=w(e).offset()).top+=w.css(e,"borderTopWidth",!0),i.left+=w.css(e,"borderLeftWidth",!0))}return{top:t.top-i.top-w.css(r,"marginTop",!0),left:t.left-i.left-w.css(r,"marginLeft",!0)}}},offsetParent:function(){return this.map(function(){var e=this.offsetParent;while(e&&"static"===w.css(e,"position"))e=e.offsetParent;return e||be})}}),w.each({scrollLeft:"pageXOffset",scrollTop:"pageYOffset"},function(e,t){var n="pageYOffset"===t;w.fn[e]=function(r){return z(this,function(e,r,i){var o;if(y(e)?o=e:9===e.nodeType&&(o=e.defaultView),void 0===i)return o?o[t]:e[r];o?o.scrollTo(n?o.pageXOffset:i,n?i:o.pageYOffset):e[r]=i},e,r,arguments.length)}}),w.each(["top","left"],function(e,t){w.cssHooks[t]=_e(h.pixelPosition,function(e,n){if(n)return n=Fe(e,t),We.test(n)?w(e).position()[t]+"px":n})}),w.each({Height:"height",Width:"width"},function(e,t){w.each({padding:"inner"+e,content:t,"":"outer"+e},function(n,r){w.fn[r]=function(i,o){var a=arguments.length&&(n||"boolean"!=typeof i),s=n||(!0===i||!0===o?"margin":"border");return z(this,function(t,n,i){var o;return y(t)?0===r.indexOf("outer")?t["inner"+e]:t.document.documentElement["client"+e]:9===t.nodeType?(o=t.documentElement,Math.max(t.body["scroll"+e],o["scroll"+e],t.body["offset"+e],o["offset"+e],o["client"+e])):void 0===i?w.css(t,n,s):w.style(t,n,i,s)},t,a?i:void 0,a)}})}),w.each("blur focus focusin focusout resize scroll click dblclick mousedown mouseup mousemove mouseover mouseout mouseenter mouseleave change select submit keydown keypress keyup contextmenu".split(" "),function(e,t){w.fn[t]=function(e,n){return arguments.length>0?this.on(t,null,e,n):this.trigger(t)}}),w.fn.extend({hover:function(e,t){return this.mouseenter(e).mouseleave(t||e)}}),w.fn.extend({bind:function(e,t,n){return this.on(e,null,t,n)},unbind:function(e,t){return this.off(e,null,t)},delegate:function(e,t,n,r){return this.on(t,e,n,r)},undelegate:function(e,t,n){return 1===arguments.length?this.off(e,"**"):this.off(t,e||"**",n)}}),w.proxy=function(e,t){var n,r,i;if("string"==typeof t&&(n=e[t],t=e,e=n),g(e))return r=o.call(arguments,2),i=function(){return e.apply(t||this,r.concat(o.call(arguments)))},i.guid=e.guid=e.guid||w.guid++,i},w.holdReady=function(e){e?w.readyWait++:w.ready(!0)},w.isArray=Array.isArray,w.parseJSON=JSON.parse,w.nodeName=N,w.isFunction=g,w.isWindow=y,w.camelCase=G,w.type=x,w.now=Date.now,w.isNumeric=function(e){var t=w.type(e);return("number"===t||"string"===t)&&!isNaN(e-parseFloat(e))},"function"==typeof define&&define.amd&&define("jquery",[],function(){return w});var Jt=e.jQuery,Kt=e.$;return w.noConflict=function(t){return e.$===w&&(e.$=Kt),t&&e.jQuery===w&&(e.jQuery=Jt),w},t||(e.jQuery=e.$=w),w});
</script>
  <script type="text/javascript">/**
 * StyleFix 1.0.3 & PrefixFree 1.0.7
 * @author Lea Verou
 * MIT license
 */(function(){function t(e,t){return[].slice.call((t||document).querySelectorAll(e))}if(!window.addEventListener)return;var e=window.StyleFix={link:function(t){try{if(t.rel!=="stylesheet"||t.hasAttribute("data-noprefix"))return}catch(n){return}var r=t.href||t.getAttribute("data-href"),i=r.replace(/[^\/]+$/,""),s=t.parentNode,o=new XMLHttpRequest,u;o.onreadystatechange=function(){o.readyState===4&&u()};u=function(){var n=o.responseText;if(n&&t.parentNode&&(!o.status||o.status<400||o.status>600)){n=e.fix(n,!0,t);if(i){n=n.replace(/url\(\s*?((?:"|')?)(.+?)\1\s*?\)/gi,function(e,t,n){return/^([a-z]{3,10}:|\/|#)/i.test(n)?e:'url("'+i+n+'")'});var r=i.replace(/([\\\^\$*+[\]?{}.=!:(|)])/g,"\\$1");n=n.replace(RegExp("\\b(behavior:\\s*?url\\('?\"?)"+r,"gi"),"$1")}var u=document.createElement("style");u.textContent=n;u.media=t.media;u.disabled=t.disabled;u.setAttribute("data-href",t.getAttribute("href"));s.insertBefore(u,t);s.removeChild(t);u.media=t.media}};try{o.open("GET",r);o.send(null)}catch(n){if(typeof XDomainRequest!="undefined"){o=new XDomainRequest;o.onerror=o.onprogress=function(){};o.onload=u;o.open("GET",r);o.send(null)}}t.setAttribute("data-inprogress","")},styleElement:function(t){if(t.hasAttribute("data-noprefix"))return;var n=t.disabled;t.textContent=e.fix(t.textContent,!0,t);t.disabled=n},styleAttribute:function(t){var n=t.getAttribute("style");n=e.fix(n,!1,t);t.setAttribute("style",n)},process:function(){t('link[rel="stylesheet"]:not([data-inprogress])').forEach(StyleFix.link);t("style").forEach(StyleFix.styleElement);t("[style]").forEach(StyleFix.styleAttribute)},register:function(t,n){(e.fixers=e.fixers||[]).splice(n===undefined?e.fixers.length:n,0,t)},fix:function(t,n,r){for(var i=0;i<e.fixers.length;i++)t=e.fixers[i](t,n,r)||t;return t},camelCase:function(e){return e.replace(/-([a-z])/g,function(e,t){return t.toUpperCase()}).replace("-","")},deCamelCase:function(e){return e.replace(/[A-Z]/g,function(e){return"-"+e.toLowerCase()})}};(function(){setTimeout(function(){t('link[rel="stylesheet"]').forEach(StyleFix.link)},10);document.addEventListener("DOMContentLoaded",StyleFix.process,!1)})()})();(function(e){function t(e,t,r,i,s){e=n[e];if(e.length){var o=RegExp(t+"("+e.join("|")+")"+r,"gi");s=s.replace(o,i)}return s}if(!window.StyleFix||!window.getComputedStyle)return;var n=window.PrefixFree={prefixCSS:function(e,r,i){var s=n.prefix;n.functions.indexOf("linear-gradient")>-1&&(e=e.replace(/(\s|:|,)(repeating-)?linear-gradient\(\s*(-?\d*\.?\d*)deg/ig,function(e,t,n,r){return t+(n||"")+"linear-gradient("+(90-r)+"deg"}));e=t("functions","(\\s|:|,)","\\s*\\(","$1"+s+"$2(",e);e=t("keywords","(\\s|:)","(\\s|;|\\}|$)","$1"+s+"$2$3",e);e=t("properties","(^|\\{|\\s|;)","\\s*:","$1"+s+"$2:",e);if(n.properties.length){var o=RegExp("\\b("+n.properties.join("|")+")(?!:)","gi");e=t("valueProperties","\\b",":(.+?);",function(e){return e.replace(o,s+"$1")},e)}if(r){e=t("selectors","","\\b",n.prefixSelector,e);e=t("atrules","@","\\b","@"+s+"$1",e)}e=e.replace(RegExp("-"+s,"g"),"-");e=e.replace(/-\*-(?=[a-z]+)/gi,n.prefix);return e},property:function(e){return(n.properties.indexOf(e)?n.prefix:"")+e},value:function(e,r){e=t("functions","(^|\\s|,)","\\s*\\(","$1"+n.prefix+"$2(",e);e=t("keywords","(^|\\s)","(\\s|$)","$1"+n.prefix+"$2$3",e);return e},prefixSelector:function(e){return e.replace(/^:{1,2}/,function(e){return e+n.prefix})},prefixProperty:function(e,t){var r=n.prefix+e;return t?StyleFix.camelCase(r):r}};(function(){var e={},t=[],r={},i=getComputedStyle(document.documentElement,null),s=document.createElement("div").style,o=function(n){if(n.charAt(0)==="-"){t.push(n);var r=n.split("-"),i=r[1];e[i]=++e[i]||1;while(r.length>3){r.pop();var s=r.join("-");u(s)&&t.indexOf(s)===-1&&t.push(s)}}},u=function(e){return StyleFix.camelCase(e)in s};if(i.length>0)for(var a=0;a<i.length;a++)o(i[a]);else for(var f in i)o(StyleFix.deCamelCase(f));var l={uses:0};for(var c in e){var h=e[c];l.uses<h&&(l={prefix:c,uses:h})}n.prefix="-"+l.prefix+"-";n.Prefix=StyleFix.camelCase(n.prefix);n.properties=[];for(var a=0;a<t.length;a++){var f=t[a];if(f.indexOf(n.prefix)===0){var p=f.slice(n.prefix.length);u(p)||n.properties.push(p)}}n.Prefix=="Ms"&&!("transform"in s)&&!("MsTransform"in s)&&"msTransform"in s&&n.properties.push("transform","transform-origin");n.properties.sort()})();(function(){function i(e,t){r[t]="";r[t]=e;return!!r[t]}var e={"linear-gradient":{property:"backgroundImage",params:"red, teal"},calc:{property:"width",params:"1px + 5%"},element:{property:"backgroundImage",params:"#foo"},"cross-fade":{property:"backgroundImage",params:"url(a.png), url(b.png), 50%"}};e["repeating-linear-gradient"]=e["repeating-radial-gradient"]=e["radial-gradient"]=e["linear-gradient"];var t={initial:"color","zoom-in":"cursor","zoom-out":"cursor",box:"display",flexbox:"display","inline-flexbox":"display",flex:"display","inline-flex":"display"};n.functions=[];n.keywords=[];var r=document.createElement("div").style;for(var s in e){var o=e[s],u=o.property,a=s+"("+o.params+")";!i(a,u)&&i(n.prefix+a,u)&&n.functions.push(s)}for(var f in t){var u=t[f];!i(f,u)&&i(n.prefix+f,u)&&n.keywords.push(f)}})();(function(){function s(e){i.textContent=e+"{}";return!!i.sheet.cssRules.length}var t={":read-only":null,":read-write":null,":any-link":null,"::selection":null},r={keyframes:"name",viewport:null,document:'regexp(".")'};n.selectors=[];n.atrules=[];var i=e.appendChild(document.createElement("style"));for(var o in t){var u=o+(t[o]?"("+t[o]+")":"");!s(u)&&s(n.prefixSelector(u))&&n.selectors.push(o)}for(var a in r){var u=a+" "+(r[a]||"");!s("@"+u)&&s("@"+n.prefix+u)&&n.atrules.push(a)}e.removeChild(i)})();n.valueProperties=["transition","transition-property"];e.className+=" "+n.prefix;StyleFix.register(n.prefixCSS)})(document.documentElement);
 </script>
  <script type="text/javascript">/**
 * Minified by jsDelivr using Terser v5.19.2.
 * Original file: /npm/conic-gradient@1.0.0/conic-gradient.js
 *
 * Do NOT use SRI with dynamically generated files! More information: https://www.jsdelivr.com/using-sri-with-dynamic-files
 */
!function(){var t=Math.PI,s=2*t,o=t/180,i=document.createElement("div");document.head.appendChild(i);var e=self.ConicGradient=function(t){e.all.push(this),t=t||{},this.canvas=document.createElement("canvas"),this.context=this.canvas.getContext("2d"),this.repeating=!!t.repeating,this.size=t.size||Math.max(innerWidth,innerHeight),this.canvas.width=this.canvas.height=this.size;var s=t.stops;this.stops=(s||"").split(/\s*,(?![^(]*\))\s*/),this.from=0;for(var o=0;o<this.stops.length;o++)if(this.stops[o]){var i=this.stops[o]=new e.ColorStop(this,this.stops[o]);i.next&&(this.stops.splice(o+1,0,i.next),o++)}else this.stops.splice(o,1),o--;if(0==this.stops[0].color.indexOf("from")&&(this.from=360*this.stops[0].pos,this.stops.shift()),void 0===this.stops[0].pos)this.stops[0].pos=0;else if(this.stops[0].pos>0){var r=this.stops[0].clone();r.pos=0,this.stops.unshift(r)}if(void 0===this.stops[this.stops.length-1].pos)this.stops[this.stops.length-1].pos=1;else if(!this.repeating&&this.stops[this.stops.length-1].pos<1){var n=this.stops[this.stops.length-1].clone();n.pos=1,this.stops.push(n)}if(this.stops.forEach((function(t,s){if(void 0===t.pos){for(var o=s+1;this[o];o++)if(void 0!==this[o].pos){t.pos=this[s-1].pos+(this[o].pos-this[s-1].pos)/(o-s+1);break}}else s>0&&(t.pos=Math.max(t.pos,this[s-1].pos))}),this.stops),this.repeating){var h=(s=this.stops.slice())[s.length-1].pos-s[0].pos;for(o=0;this.stops[this.stops.length-1].pos<1&&o<1e4;o++)for(var a=0;a<s.length;a++){var p=s[a].clone();p.pos+=(o+1)*h,this.stops.push(p)}}this.paint()};e.all=[],e.prototype={toString:function(){return"url('"+this.dataURL+"')"},get dataURL(){return"data:image/svg+xml,"+encodeURIComponent(this.svg)},get blobURL(){return URL.createObjectURL(new Blob([this.svg],{type:"image/svg+xml"}))},get svg(){return'<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="none"><svg viewBox="0 0 100 100" preserveAspectRatio="xMidYMid slice"><image width="100" height="100%" xlink:href="'+this.png+'" /></svg></svg>'},get png(){return this.canvas.toDataURL()},get r(){return Math.sqrt(2)*this.size/2},paint:function(){var t,s,i,e=this.context,r=this.r,n=this.size/2,h=0,a=this.stops[h];e.translate(this.size/2,this.size/2),e.rotate(-90*o),e.rotate(this.from*o),e.translate(-this.size/2,-this.size/2);for(var p=0;p<360;){if(p/360+1e-5>=a.pos){do{t=a,h++,a=this.stops[h]}while(a&&a!=t&&a.pos===t.pos);if(!a)break;var l=t.color+""==a.color+""&&t!=a;s=t.color.map((function(t,s){return a.color[s]-t}))}i=(p/360-t.pos)/(a.pos-t.pos);var c=l?a.color:s.map((function(s,o){var e=s*i+t.color[o];return o<3?255&e:e}));if(e.fillStyle="rgba("+c.join(",")+")",e.beginPath(),e.moveTo(n,n),l)var g=360*(a.pos-t.pos);else g=.5;var d=p*o,f=(d=Math.min(360*o,d))+g*o;f=Math.min(360*o,f+.02),e.arc(n,n,r,d,f),e.closePath(),e.fill(),p+=g}}},e.ColorStop=function(t,o){if(this.gradient=t,o){var i=o.match(/^(.+?)(?:\s+([\d.]+)(%|deg|turn|grad|rad)?)?(?:\s+([\d.]+)(%|deg|turn|grad|rad)?)?\s*$/);if(this.color=e.ColorStop.colorToRGBA(i[1]),i[2]){var r=i[3];"%"==r||"0"===i[2]&&!r?this.pos=i[2]/100:"turn"==r?this.pos=+i[2]:"deg"==r?this.pos=i[2]/360:"grad"==r?this.pos=i[2]/400:"rad"==r&&(this.pos=i[2]/s)}i[4]&&(this.next=new e.ColorStop(t,i[1]+" "+i[4]+i[5]))}},e.ColorStop.prototype={clone:function(){var t=new e.ColorStop(this.gradient);return t.color=this.color,t.pos=this.pos,t},toString:function(){return"rgba("+this.color.join(", ")+") "+100*this.pos+"%"}},e.ColorStop.colorToRGBA=function(t){if(!Array.isArray(t)&&-1==t.indexOf("from")){i.style.color=t;var s=getComputedStyle(i).color.match(/rgba?\(([\d.]+), ([\d.]+), ([\d.]+)(?:, ([\d.]+))?\)/);return s&&(s.shift(),(s=s.map((function(t){return+t})))[3]=isNaN(s[3])?1:s[3]),s||[0,0,0,0]}return t}}(),self.StyleFix&&function(){var t=document.createElement("p");t.style.backgroundImage="conic-gradient(white, black)",t.style.backgroundImage=PrefixFree.prefix+"conic-gradient(white, black)",t.style.backgroundImage||StyleFix.register((function(t,s){return t.indexOf("conic-gradient")>-1&&(t=t.replace(/(?:repeating-)?conic-gradient\(\s*((?:\([^()]+\)|[^;()}])+?)\)/g,(function(t,s){return new ConicGradient({stops:s,repeating:t.indexOf("repeating-")>-1})}))),t}))}();
//# sourceMappingURL=/sm/2b199b591534779693c5cc22501b3760aa452ae279ec270886fd041a7ec68d42.map</script>
  <script src="https://unpkg.com/tabbable/dist/index.umd.js"></script>
  <script src="https://unpkg.com/focus-trap/dist/focus-trap.umd.js"></script>
  <script type="text/javascript">
    $(window).on('load', function () {
  const readerFocusTrap = focusTrap.createFocusTrap('#reader');
  const settingsFocusTrap = focusTrap.createFocusTrap('#settings');
  const accessibilityFocusTrap = focusTrap.createFocusTrap('#accessibility');
  let topColHeight = 'auto';
  let sideColWidth = 'auto';
  const csvData = {"rows":[[{"span":4,"isColumn":true,"id":107,"title":"Granularity of extraction"}],[{"span":1,"isColumn":true,"id":11,"title":"Sentences"},{"span":1,"isColumn":true,"id":23,"title":"Entities"},{"span":1,"isColumn":true,"id":97,"title":"Binary for each document"},{"span":1,"isColumn":true,"id":101,"title":"Other"}],[{"span":32,"isColumn":false,"id":103,"title":"Entities mined"},{"span":1,"isColumn":false,"id":1,"title":"P"}],[{"span":1,"isColumn":false,"id":2,"title":"IC"}],[{"span":1,"isColumn":false,"id":3,"title":"O"}],[{"span":1,"isColumn":false,"id":4,"title":"Sections (Aim; Method etc.)"}],[{"span":1,"isColumn":false,"id":17,"title":"Country"}],[{"span":1,"isColumn":false,"id":18,"title":"Exposure"}],[{"span":1,"isColumn":false,"id":28,"title":"O (measurement instrument)"}],[{"span":1,"isColumn":false,"id":33,"title":"Age"}],[{"span":1,"isColumn":false,"id":34,"title":"Gender"}],[{"span":1,"isColumn":false,"id":35,"title":"P (Condition or disease)"}],[{"span":1,"isColumn":false,"id":38,"title":"IC (per arm)"}],[{"span":1,"isColumn":false,"id":39,"title":"IC (dose; duration and others)"}],[{"span":1,"isColumn":false,"id":40,"title":"O (time point)"}],[{"span":1,"isColumn":false,"id":41,"title":"O (primary or secondary outcome)"}],[{"span":1,"isColumn":false,"id":42,"title":"N (total)"}],[{"span":1,"isColumn":false,"id":43,"title":"Eligibility criteria"}],[{"span":1,"isColumn":false,"id":44,"title":"Enrolment dates"}],[{"span":1,"isColumn":false,"id":45,"title":"Funding org"}],[{"span":1,"isColumn":false,"id":46,"title":"Grant number"}],[{"span":1,"isColumn":false,"id":47,"title":"Early stopping"}],[{"span":1,"isColumn":false,"id":48,"title":"Trial registration"}],[{"span":1,"isColumn":false,"id":49,"title":"Other"}],[{"span":1,"isColumn":false,"id":60,"title":"N (per arm)"}],[{"span":1,"isColumn":false,"id":63,"title":"Randomisation"}],[{"span":1,"isColumn":false,"id":64,"title":"Blinding"}],[{"span":1,"isColumn":false,"id":65,"title":"Design"}],[{"span":1,"isColumn":false,"id":66,"title":"Race"}],[{"span":1,"isColumn":false,"id":75,"title":"IC (Drug name)"}],[{"span":1,"isColumn":false,"id":78,"title":"Diagnostic tests"}],[{"span":1,"isColumn":false,"id":84,"title":"Setting"}],[{"span":1,"isColumn":false,"id":89,"title":"Withdrawals or exclusions"}],[{"span":1,"isColumn":false,"id":98,"title":"Exclusion criteria"}]],"totalColBreadth":4,"totalColDepth":2,"totalRowBreadth":32,"totalRowDepth":1};
  const filters = [{"id":203,"label":"LSR Version","checked":false,"children":[]}];
  const autoOpenFilter = false;
  const externalURLedAttributes = [];
  const metaProperties = [];
  const aboutContent = "";
  const aboutPopup = false;
  const studySubmissionContent = "";
  const segmentAttributes = [{"attribute":{"AttributeId":2031,"AttributeName":"Base Review"},"color":"#26D84F","isLight":true},{"attribute":{"AttributeId":2032,"AttributeName":"Update 1"},"color":"#1F2CD1","isLight":false},{"attribute":{"AttributeId":2033,"AttributeName":"Update 2"},"color":"#E72727","isLight":true}];
  const referenceData = [{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":16,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":668,"Title":"Sentence retrieval for abstracts of randomized controlled trials","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2009","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"BACKGROUND: The practice of evidence-based medicine (EBM) requires clinicians to integrate their expertise with the latest scientific research. But this is becoming increasingly difficult with the growing numbers of published articles. There is a clear need for better tools to improve clinician's ability to search the primary literature. Randomized clinical trials (RCTs) are the most reliable source of evidence documenting the efficacy of treatment options. This paper describes the retrieval of key sentences from abstracts of RCTs as a step towards helping users find relevant facts about the experimental design of clinical studies. METHOD: Using Conditional Random Fields (CRFs), a popular and successful method for natural language processing problems, sentences referring to Intervention, Participants and Outcome Measures are automatically categorized. This is done by extending a previous approach for labeling sentences in an abstract for general categories associated with scientific argumentation or rhetorical roles: Aim, Method, Results and Conclusion. Methods are tested on several corpora of RCT abstracts. First structured abstracts with headings specifically indicating Intervention, Participant and Outcome Measures are used. Also a manually annotated corpus of structured and unstructured abstracts is prepared for testing a classifier that identifies sentences belonging to each category. RESULTS: Using CRFs, sentences can be labeled for the four rhetorical roles with F-scores from 0.93-0.98. This outperforms the use of Support Vector Machines. Furthermore, sentences can be automatically labeled for Intervention, Participant and Outcome Measures, in unstructured and structured abstracts where the section headings do not specifically indicate these three topics. F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences. CONCLUSION: Results indicate that some of the methodological elements of RCTs are identifiable at the sentence level in both structured and unstructured abstract reports. This is promising in that sentences labeled automatically could potentially form concise summaries, assist in information retrieval and finer-grained extraction.","Comments":"","TypeName":"","Authors":"Chung, G. Y.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":18,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":21,"ItemAttributeFullTextDetails":[]},{"AttributeId":22,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":24,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1789,"Title":"Evaluation of a rule-based method for epidemiological document classification towards the automation of systematic reviews","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2017","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"INTRODUCTION: Most data extraction efforts in epidemiology are focused on obtaining targeted information from clinical trials. In contrast, limited research has been conducted on the identification of information from observational studies, a major source for human evidence in many fields, including environmental health. The recognition of key epidemiological information (e.g., exposures) through text mining techniques can assist in the automation of systematic reviews and other evidence summaries. METHOD: We designed and applied a knowledge-driven, rule-based approach to identify targeted information (study design, participant population, exposure, outcome, confounding factors, and the country where the study was conducted) from abstracts of epidemiological studies included in several systematic reviews of environmental health exposures. The rules were based on common syntactical patterns observed in text and are thus not specific to any systematic review. To validate the general applicability of our approach, we compared the data extracted using our approach versus hand curation for 35 epidemiological study abstracts manually selected for inclusion in two systematic reviews. RESULTS: The returned F-score, precision, and recall ranged from 70% to 98%, 81% to 100%, and 54% to 97%, respectively. The highest precision was observed for exposure, outcome and population (100%) while recall was best for exposure and study design with 97% and 89%, respectively. The lowest recall was observed for the population (54%), which also had the lowest F-score (70%). CONCLUSION: The generated performance of our text-mining approach demonstrated encouraging results for the identification of targeted information from observational epidemiological study abstracts related to environmental exposures. We have demonstrated that rules based on generic syntactic patterns in one corpus can be applied to other observational study design by simple interchanging the dictionaries aiming to identify certain characteristics (i.e., outcomes, exposures). At the document level, the recognised information can assist in the selection and categorization of studies included in a systematic review.","Comments":"","TypeName":"","Authors":"Karystianis, G.;  and Thayer, K.;  and Wolfe, M.;  and Tsafnat, G.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":27,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":563,"Title":"Combination of conditional random field with a rule based method in the extraction of PICO elements","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2018","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"BACKGROUND: Extracting primary care information in terms of Patient/Problem, Intervention, Comparison and Outcome, known as PICO elements, is difficult as the volume of medical information expands and the health semantics is complex to capture it from unstructured information. The combination of the machine learning methods (MLMs) with rule based methods (RBMs) could facilitate and improve the PICO extraction. This paper studies the PICO elements extraction methods. The goal is to combine the MLMs with the RBMs to extract PICO elements in medical papers to facilitate answering clinical questions formulated with the PICO framework. METHODS: First, we analyze the aspects of the MLM model that influence the quality of the PICO elements extraction. Secondly, we combine the MLM approach with the RBMs in order to improve the PICO elements retrieval process. To conduct our experiments, we use a corpus of 1000 abstracts. RESULTS: We obtain an F-score of 80% for P element, 64% for the I element and 92% for the O element. Given the nature of the used training corpus where P and I elements represent respectively only 6.5 and 5.8% of total sentences, the results are competitive with previously published ones. CONCLUSIONS: Our study of the PICO element extraction shows that the task is very challenging. The MLMs tend to have an acceptable precision rate but they have a low recall rate when the corpus is not representative. The RBMs backed up the MLMs to increase the recall rate and consequently the combination of the two methods gave better results.","Comments":"","TypeName":"","Authors":"Chabou, S.;  and Iglewski, M.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":28,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":29,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":30,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":32,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":3905,"Title":"An Ontology-Enabled Natural Language Processing Pipeline for Provenance Metadata Extraction from Biomedical Text (Short Paper)","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2016","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Extraction of structured information from biomedical literature is a complex and challenging problem due to the complexity of biomedical domain and lack of appropriate natural language processing (NLP) techniques. High quality domain ontologies model both data and metadata information at a fine level of granularity, which can be effectively used to accurately extract structured information from biomedical text. Extraction of provenance metadata, which describes the history or source of information, from published articles is an important task to support scientific reproducibility. Reproducibility of results reported by previous research studies is a foundational component of scientific advancement. This is highlighted by the recent initiative by the US National Institutes of Health called 'Principles of Rigor and Reproducibility'. In this paper, we describe an effective approach to extract provenance metadata from published biomedical research literature using an ontology-enabled NLP platform as part of the Provenance for Clinical and Healthcare Research (ProvCaRe). The ProvCaRe-NLP tool extends the clinical Text Analysis and Knowledge Extraction System (cTAKES) platform using both provenance and biomedical domain ontologies. We demonstrate the effectiveness of ProvCaRe-NLP tool using a corpus of 20 peer-reviewed publications. The results of our evaluation demonstrate that the ProvCaRe-NLP tool has significantly higher recall in extracting provenance metadata as compared to existing NLP pipelines such as MetaMap.","Comments":"","TypeName":"","Authors":"Valdez, J.;  and Rueschman, M.;  and Kim, M.;  and Redline, S.;  and Sahoo, S. S.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":37,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":32,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":3102,"Title":"Towards Evidence-based Precision Medicine: Extracting Population Information from Biomedical Text using Binary Classifiers and Syntactic Patterns","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2016","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Precision Medicine is an emerging approach for prevention and treatment of disease that considers individual variability in genes, environment, and lifestyle for each person. The dissemination of individualized evidence by automatically identifying population information in literature is a key for evidence-based precision medicine at the point-of-care. We propose a hybrid approach using natural language processing techniques to automatically extract the population information from biomedical literature. Our approach first implements a binary classifier to classify sentences with or without population information. A rule-based system based on syntactic-tree regular expressions is then applied to sentences containing population information to extract the population named entities. The proposed two-stage approach achieved an F-score of 0.81 using a MaxEnt classifier and the rule- based system, and an F-score of 0.87 using a Nai've-Bayes classifier and the rule-based system, and performed relatively well compared to many existing systems. The system and evaluation dataset is being released as open source.","Comments":"","TypeName":"","Authors":"Raja, K.;  and Dasot, N.;  and Goyal, P.;  and Jonnalagadda, S. R.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":40,"ItemAttributeFullTextDetails":[]},{"AttributeId":41,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":44,"ItemAttributeFullTextDetails":[]},{"AttributeId":45,"ItemAttributeFullTextDetails":[]},{"AttributeId":46,"ItemAttributeFullTextDetails":[]},{"AttributeId":47,"ItemAttributeFullTextDetails":[]},{"AttributeId":48,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":51,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":834,"Title":"Automated information extraction of key trial design elements from clinical trial publications","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2008","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Clinical trials are one of the most valuable sources of scientific evidence for improving the practice of medicine. The Trial Bank project aims to improve structured access to trial findings by including formalized trial information into a knowledge base. Manually extracting trial information from published articles is costly, but automated information extraction techniques can assist. The current study highlights a single architecture to extract a wide array of information elements from full-text publications of randomized clinical trials (RCTs). This architecture combines a text classifier with a weak regular expression matcher. We tested this two-stage architecture on 88 RCT reports from 5 leading medical journals, extracting 23 elements of key trial information such as eligibility rules, sample size, intervention, and outcome names. Results prove this to be a promising avenue to help critical appraisers, systematic reviewers, and curators quickly identify key information elements in published RCT articles.","Comments":"","TypeName":"","Authors":"de Bruijn, B.;  and Carini, S.;  and Kiritchenko, S.;  and Martin, J.;  and Sim, I.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":52,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":32,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":9471,"Title":"A Novel Framework to Expedite Systematic Reviews by Automatically   Building Information Extraction Training Corpora","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2016","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/1606.06424","OldItemId":"","Abstract":"A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data.A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews.","Comments":"","TypeName":"","Authors":"['Tanmay Basu', 'Shraman Kumar', 'Abhishek Kalyan', 'Priyanka Jayaswal', 'Pawan Goyal', 'Stephen Pettifer', 'Siddhartha R. Jonnalagadda']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":9964,"Title":"Data Mining in Clinical Trial Text: Transformers for Classification and   Question Answering Tasks","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2001.11268","OldItemId":"","Abstract":"This research on data extraction methods applies recent advances in natural language processing to evidence synthesis based on medical texts. Texts of interest include abstracts of clinical trials in English and in multilingual contexts. The main focus is on information characterized via the Population, Intervention, Comparator, and Outcome (PICO) framework, but data extraction is not limited to these fields. Recent neural network architectures based on transformers show capacities for transfer learning and increased performance on downstream natural language processing tasks such as universal reading comprehension, brought forward by this architecture's use of contextualized word embeddings and self-attention mechanisms. This paper contributes to solving problems related to ambiguity in PICO sentence prediction tasks, as well as highlighting how annotations for training named entity recognition systems are used to train a high-performing, but nevertheless flexible architecture for question answering in systematic review automation. Additionally, it demonstrates how the problem of insufficient amounts of training annotations for PICO entity extraction is tackled by augmentation. All models in this paper were created with the aim to support systematic review (semi)automation. They achieve high F1 scores, and demonstrate the feasibility of applying transformer-based classification methods to support data mining in the biomedical literature.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs https://arxiv.org/abs/2001.11268; Years 2020; Authors Lena Schmidt; Julie Weeds; Julian P. T. Higgins; Deduplication_Notes ; X Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 1; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 1] CHECK DUPLICATE [Source dblp; Title Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; Abstract NaN; Keywords NaN; DOIs 10.5220/0008945700830094; DOI_original 10.5220/0008945700830094; URLs https://doi.org/10.5220/0008945700830094; Years 2020; Authors Lena Schmidt; Julie Weeds; Julian P. T. Higgins; Deduplication_Notes ; X Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 1; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 1]","Comments":"","TypeName":"","Authors":"['Lena Schmidt', 'Julie Weeds', 'Julian P. T. Higgins']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":32,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":4020,"Title":"Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2016","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method - supervised distant supervision (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.","Comments":"","TypeName":"","Authors":"Wallace, B. C.;  and Kuiper, J.;  and Sharma, A.;  and Zhu, M. B.;  and Marshall, I. J.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":55,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":4564,"Title":"Classification of PICO elements by text features systematically extracted from PubMed abstracts","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2011","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122608","OldItemId":"","Abstract":"We propose and evaluate a systematic approach to detect and classify Patient/Problem, Intervention, Comparison and Outcome (PICO) from the medical literature. The training and test corpora were generated systematically and automatically from structured PubMed abstracts. 23,472 sentences by exact pattern match of head words of P-I-O categories. Afterward, the terms with top frequencies were used as the features of Nave Bayesian classifier. This approach achieves F-measure values of 0.91 for Patient/Problem, 0.75 for Intervention and 0.88 for Outcome, comparable to previous studied based on mixed textural, paragraphical, and semantic features. In conclusion, we show that by stricter pattern matching criteria of training set, detection and classification of PICO elements can be reproducible with minimal expert intervention. The results of this work are higher than previous studies.","Comments":"","TypeName":"","Authors":"K. Huang;  and C. C. Liu;  and S. Yang;  and F. Xiao;  and J. Wong;  and C. Liao;  and I. Chiang","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":58,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":456,"Title":"Improving reference prioritisation with PICO recognition","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"BACKGROUND: Machine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition. METHODS: A publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts. RESULTS: Inclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase. CONCLUSIONS: Words within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search.","Comments":"","TypeName":"","Authors":"Brockmeier, A. J.;  and Ju, M.;  and Przybyla, P.;  and Ananiadou, S.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":52,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":32,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":473,"Title":"Extractive text summarization system to aid data extraction from full text in systematic review development","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2016","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"OBJECTIVES: Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. In this study, we developed a text summarization system aimed at enhancing productivity and reducing errors in the traditional data extraction process. METHODS: We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications. The summaries at the sentence and fragment levels were evaluated in finding common clinical SR data elements such as sample size, group size, and PICO values. We compared the computer-generated summaries with human written summaries (title and abstract) in terms of the presence of necessary information for the data extraction as presented in the Cochrane review's study characteristics tables. RESULTS: At the sentence level, the computer-generated summaries covered more information than humans do for systematic reviews (recall 91.2% vs. 83.8%, p<0.001). They also had a better density of relevant sentences (precision 59% vs. 39%, p<0.001). At the fragment level, the ensemble approach combining rule-based, concept mapping, and dictionary-based methods performed better than individual methods alone, achieving an 84.7% F-measure. CONCLUSION: Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system.","Comments":"","TypeName":"","Authors":"Bui, D. D. A.;  and Del Fiol, G.;  and Hurdle, J. F.;  and Jonnalagadda, S.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":444,"Title":"Developing a fully automated evidence synthesis tool for identifying, assessing and collating the evidence","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Evidence synthesis is a key element of evidence-based medicine. However, it is currently hampered by being labour intensive meaning that many trials are not incorporated into robust evidence syntheses and that many are out of date. To overcome this, a variety of techniques are being explored, including using automation technology. Here, we describe a fully automated evidence synthesis system for intervention studies, one that identifies all the relevant evidence, assesses the evidence for reliability and collates it to estimate the relative effectiveness of an intervention. Techniques used include machine learning, natural language processing and rule-based systems. Results are visualised using modern visualisation techniques. We believe this to be the first, publicly available, automated evidence synthesis system: an evidence mapping tool that synthesises evidence on the fly.","Comments":"","TypeName":"","Authors":"Brassey, J.;  and Price, C.;  and Edwards, J.;  and Zlabinger, M.;  and Bampoulidis, A.;  and Hanbury, A.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":24,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":3546,"Title":"A Neural Candidate-Selector Architecture for Automatic Structured Clinical Text Annotation","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2017","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"We consider the task of automatically annotating free texts describing clinical trials with concepts from a controlled, structured medical vocabulary. Specifically we aim to build a model to infer distinct sets of (ontological) concepts describing complementary clinically salient aspects of the underlying trials: the populations enrolled, the interventions administered and the outcomes measured, i.e., the PICO elements. This important practical problem poses a few key challenges. One issue is that the output space is vast, because the vocabulary comprises many unique concepts. Compounding this problem, annotated data in this domain is expensive to collect and hence sparse. Furthermore, the outputs (sets of concepts for each PICO element) are correlated: specific populations (e.g., diabetics) will render certain intervention concepts likely (insulin therapy) while effectively precluding others (radiation therapy). Such correlations should be exploited. We propose a novel neural model that addresses these challenges. We introduce a Candidate-Selector architecture in which the model considers setes of candidate concepts for PICO elements, and assesses their plausibility conditioned on the input text to be annotated. This relies on a 'candidate set' generator, which may be learned or relies on heuristics. A conditional discriminative neural model then jointly selects candidate concepts, given the input text. We compare the predictive performance of our approach to strong baselines, and show that it outperforms them. Finally, we perform a qualitative evaluation of the generated annotations by asking domain experts to assess their quality.","Comments":"","TypeName":"","Authors":"Singh, G.;  and Marshall, I. J.;  and Thomas, J.;  and Shawe-Taylor, J.;  and Wallace, B. C.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":62,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":30,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":27,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":876,"Title":"Finding medication doses in the liteature","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2018","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Medication doses, one of the determining factors in medication safety and effectiveness, are present in the literature, but only in free-text form. We set out to determine if the systems developed for extracting drug prescription information from clinical text would yield comparable results on scientific literature and if sequence-to-sequence learning with neural networks could improve over the current state-of-the-art. We developed a collection of 694 PubMed Central documents annotated with drug dose information using the i2b2 schema. We found that less than half of the drug doses are present in the MEDLINE/PubMed abstracts, and full-text is needed to identify the other half. We identified the differences in the scope and formatting of drug dose information in the literature and clinical text, which require developing new dose extraction approaches. Finally, we achieved 83.9% recall, 87.2% precision and 85.5% F<sub>1</sub> score in extracting complete drug prescription information from the literature.","Comments":"","TypeName":"","Authors":"Demner-Fushman, D.;  and Mork, J. G.;  and Rogers, W. J.;  and Shooshan, S. E.;  and Rodriguez, L.;  and Aronson, A. R.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":63,"ItemAttributeFullTextDetails":[]},{"AttributeId":64,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":66,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":67,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":52,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":24,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":2392,"Title":"Automating Biomedical Evidence Synthesis: RobotReviewer","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2017","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"We present RobotReviewer, an open-source web-based system that uses machine learning and NLP to semi-automate biomedical evidence synthesis, to aid the practice of Evidence-Based Medicine. RobotReviewer processes full-text journal articles (PDFs) describing randomized controlled trials (RCTs). It appraises the reliability of RCTs and extracts text describing key trial characteristics (e.g., descriptions of the population) using novel NLP methods. RobotReviewer then automatically generates a report synthesising this information. Our goal is for RobotReviewer to automatically extract and synthesise the full-range of structured data needed to inform evidence-based practice.","Comments":"","TypeName":"","Authors":"Marshall, I. J.;  and Kuiper, J.;  and Banner, E.;  and Wallace, B. C.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":62,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1771,"Title":"Pretraining to Recognize PICO Elements from Randomized Controlled Trial Literature","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"PICO (Population/problem, Intervention, Comparison, and Outcome) is widely adopted for formulating clinical questions to retrieve evidence from the literature. It plays a crucial role in Evidence-Based Medicine (EBM). This paper contributes a scalable deep learning method to extract PICO statements from RCT articles. It was trained on a small set of richly annotated PubMed abstracts using an LSTM-CRF model. By initializing our model with pretrained parameters from a large related corpus, we improved the model performance significantly with a minimal feature set. Our method has advantages in minimizing the need for laborious feature handcrafting and in avoiding the need for large shared annotated data by reusing related corpora in pretraining with a deep neural network.","Comments":"","TypeName":"","Authors":"Kang, T.;  and Zou, S.;  and Weng, C.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":18,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":69,"ItemAttributeFullTextDetails":[]},{"AttributeId":22,"ItemAttributeFullTextDetails":[]},{"AttributeId":37,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":70,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1788,"Title":"Mining characteristics of epidemiological studies from Medline: a case study in obesity","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2014","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"BACKGROUND: The health sciences literature incorporates a relatively large subset of epidemiological studies that focus on population-level findings, including various determinants, outcomes and correlations. Extracting structured information about those characteristics would be useful for more complete understanding of diseases and for meta-analyses and systematic reviews. RESULTS: We present an information extraction approach that enables users to identify key characteristics of epidemiological studies from MEDLINE abstracts. It extracts six types of epidemiological characteristic: design of the study, population that has been studied, exposure, outcome, covariates and effect size. We have developed a generic rule-based approach that has been designed according to semantic patterns observed in text, and tested it in the domain of obesity. Identified exposure, outcome and covariate concepts are clustered into health-related groups of interest. On a manually annotated test corpus of 60 epidemiological abstracts, the system achieved precision, recall and F-score between 79-100%, 80-100% and 82-96% respectively. We report the results of applying the method to a large scale epidemiological corpus related to obesity. CONCLUSIONS: The experiments suggest that the proposed approach could identify key epidemiological characteristics associated with a complex clinical problem from related abstracts. When integrated over the literature, the extracted data can be used to provide a more complete picture of epidemiological efforts, and thus support understanding via meta-analysis and systematic reviews.","Comments":"","TypeName":"","Authors":"Karystianis, G.;  and Buchan, I.;  and Nenadic, G.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":71,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":32,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":9483,"Title":"A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical   Knowledge Summarization: Combining Concept Extraction, Vector Similarity and   Query Expansion for High Precision","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2016","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/1609.01597","OldItemId":"","Abstract":"Novel information retrieval methods to identify citations relevant to a clinical topic can overcome the knowledge gap existing between the primary literature (MEDLINE) and online clinical knowledge resources such as UpToDate. Searching the MEDLINE database directly or with query expansion methods returns a large number of citations that are not relevant to the query. The current study presents a citation retrieval system that retrieves citations for evidence-based clinical knowledge summarization. This approach combines query expansion, concept-based screening algorithm, and concept-based vector similarity. We also propose an information extraction framework for automated concept (Population, Intervention, Comparison, and Disease) extraction. We evaluated our proposed system on all topics (as queries) available from UpToDate for two diseases, heart failure (HF) and atrial fibrillation (AFib). The system achieved an overall F-score of 41.2% on HF topics and 42.4% on AFib topics on a gold standard of citations available in UpToDate. This is significantly high when compared to a query-expansion based baseline (F-score of 1.3% on HF and 2.2% on AFib) and a system that uses query expansion with disease hyponyms and journal names, concept-based screening, and term-based vector similarity system (F-score of 37.5% on HF and 39.5% on AFib). Evaluating the system with top K relevant citations, where K is the number of citations in the gold standard achieved a much higher overall F-score of 69.9% on HF topics and 75.1% on AFib topics. In addition, the system retrieved up to 18 new relevant citations per topic when tested on ten HF and six AFib clinical topics.","Comments":"","TypeName":"","Authors":"['Kalpana Raja', 'Andrew J Sauer', 'Ravi P Garg', 'Melanie R Klerer', 'Siddhartha R Jonnalagadda']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":18,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":69,"ItemAttributeFullTextDetails":[]},{"AttributeId":22,"ItemAttributeFullTextDetails":[]},{"AttributeId":37,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":72,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":4236,"Title":"Mining Biomedical Literature for Terms related to Epidemiologic Exposures","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2010","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Epidemiologic studies contribute greatly to evidence-based medicine by identifying risk factors for diseases and determining optimal treatments for clinical practice. However, there is very limited effort on automatic extraction of knowledge from epidemiologic articles, such as exposures, outcomes, and their relations. In this initial study, we developed a system that consists of a natural language processing (NLP) engine and a rule-based classifier, to automatically extract exposure-related terms from titles of epidemiologic articles. The evaluation using 450 titles annotated by an epidemiologist showed the highest F-measure of 0.646 (Precision 0.610 and Recall 0.688) using in-exact matching, which indicated the feasibility of automated methods on mining epidemiologic literature. Further analysis of terms related to epidemiologic exposures suggested that although UMLS would have reasonable coverage, more appropriate semantic classifications of epidemiologic exposures would be required.","Comments":"","TypeName":"","Authors":"Xu, H.;  and Lu, Y.;  and Jiang, M.;  and Liu, M.;  and Denny, J. C.;  and Dai, Q.;  and Peterson, N. B.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":30,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":73,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":369,"Title":"Automatic endpoint detection to support the systematic review process","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2015","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Preparing a systematic review can take hundreds of hours to complete, but the process of reconciling different results from multiple studies is the bedrock of evidence-based medicine. We introduce a two-step approach to automatically extract three facets - two entities (the agent and object) and the way in which the entities are compared (the endpoint) - from direct comparative sentences in full-text articles. The system does not require a user to predefine entities in advance and thus can be used in domains where entity recognition is difficult or unavailable. As with a systematic review, the tabular summary produced using the automatically extracted facets shows how experimental results differ between studies. Experiments were conducted using a collection of more than 2million sentences from three journals Diabetes, Carcinogenesis and Endocrinology and two machine learning algorithms, support vector machines (SVM) and a general linear model (GLM). F1 and accuracy measures for the SVM and GLM differed by only 0.01 across all three comparison facets in a randomly selected set of test sentences. The system achieved the best performance of 92% for objects, whereas the accuracy for both agent and endpoints was 73%. F1 scores were higher for objects (0.77) than for endpoints (0.51) or agents (0.47). A situated evaluation of Metformin, a drug to treat diabetes, showed system accuracy of 95%, 83% and 79% for the object, endpoint and agent respectively. The situated evaluation had higher F1 scores of 0.88, 0.64 and 0.62 for object, endpoint, and agent respectively. On average, only 5.31% of the sentences in a full-text article are direct comparisons, but the tabular summaries suggest that these sentences provide a rich source of currently underutilized information that can be used to accelerate the systematic review process and identify gaps where future research should be focused.","Comments":"","TypeName":"","Authors":"Blake, C.;  and Lucic, A.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":40,"ItemAttributeFullTextDetails":[]},{"AttributeId":41,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":44,"ItemAttributeFullTextDetails":[]},{"AttributeId":45,"ItemAttributeFullTextDetails":[]},{"AttributeId":46,"ItemAttributeFullTextDetails":[]},{"AttributeId":47,"ItemAttributeFullTextDetails":[]},{"AttributeId":48,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":72,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1894,"Title":"ExaCT: automatic extraction of clinical trial characteristics from journal publications","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2010","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"BACKGROUND: Clinical trials are one of the most important sources of evidence for guiding evidence-based practice and the design of new trials. However, most of this information is available only in free text - e.g., in journal publications - which is labour intensive to process for systematic reviews, meta-analyses, and other evidence synthesis studies. This paper presents an automatic information extraction system, called ExaCT, that assists users with locating and extracting key trial characteristics (e.g., eligibility criteria, sample size, drug dosage, primary outcomes) from full-text journal articles reporting on randomized controlled trials (RCTs). METHODS: ExaCT consists of two parts: an information extraction (IE) engine that searches the article for text fragments that best describe the trial characteristics, and a web browser-based user interface that allows human reviewers to assess and modify the suggested selections. The IE engine uses a statistical text classifier to locate those sentences that have the highest probability of describing a trial characteristic. Then, the IE engine's second stage applies simple rules to these sentences to extract text fragments containing the target answer. The same approach is used for all 21 trial characteristics selected for this study. RESULTS: We evaluated ExaCT using 50 previously unseen articles describing RCTs. The text classifier (first stage) was able to recover 88% of relevant sentences among its top five candidates (top5 recall) with the topmost candidate being relevant in 80% of cases (top1 precision). Precision and recall of the extraction rules (second stage) were 93% and 91%, respectively. Together, the two stages of the extraction engine were able to provide (partially) correct solutions in 992 out of 1050 test tasks (94%), with a majority of these (696) representing fully correct and complete answers. CONCLUSIONS: Our experiments confirmed the applicability and efficacy of ExaCT. Furthermore, they demonstrated that combining a statistical method with 'weak' extraction rules can identify a variety of study characteristics. The system is flexible and can be extended to handle other characteristics and document types (e.g., study protocols).","Comments":"","TypeName":"","Authors":"Kiritchenko, S.;  and de Bruijn, B.;  and Carini, S.;  and Martin, J.;  and Sim, I.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":74,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":27,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":9695,"Title":"Advancing PICO Element Detection in Biomedical Text via Deep Neural   Networks","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2018","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/1810.12780","OldItemId":"","Abstract":"In evidence-based medicine (EBM), defining a clinical question in terms of the specific patient problem aids the physicians to efficiently identify appropriate resources and search for the best available evidence for medical treatment. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components typically reported in clinical trials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we propose a novel deep learning model for recognizing PICO elements in biomedical abstracts. Based on the previous state-of-the-art bidirectional long-short term memory (biLSTM) plus conditional random field (CRF) architecture, we add another layer of biLSTM upon the sentence representation vectors so that the contextual information from surrounding sentences can be gathered to help infer the interpretation of the current one. In addition, we propose two methods to further generalize and improve the model: adversarial training and unsupervised pre-training over large corpora. We tested our proposed approach over two benchmark datasets. One is the PubMed-PICO dataset, where our best results outperform the previous best by 5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score, respectively. And for the other dataset named NICTA-PIBOSO, the improvements for P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively. Overall, our proposed deep learning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Advancing PICO Element Detection in Medical Text via Deep Neural Networks.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/1810.12780; Years 2018; Authors Di Jin; Peter Szolovits; Deduplication_Notes ; X Advancing PICO Element Detection in Medical Text via Deep Neural Networks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","Comments":"","TypeName":"","Authors":"['Di Jin', 'Peter Szolovits']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":27,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":10346,"Title":"PICO Element Detection in Medical Text via Long Short-Term Memory Neural Networks.","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2018","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.18653/v1/w18-2308","OldItemId":"","Abstract":"not available","Comments":"","TypeName":"","Authors":"Di Jin;  and Peter Szolovits","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1376,"Title":"Evaluating automated entity extraction with respect to drug and non-drug treatment strategies","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1016/j.jbi.2019.103177","OldItemId":"","Abstract":"OBJECTIVES: Treatment used in a randomized clinical trial is a critical data element both for physicians at the point of care and reviewers who are evaluating different interventions. Much of existing work on treatment extraction from the biomedical literature has focused on the extraction of pharmacological interventions. However, non-pharmacological interventions (e.g., exercise, diet, etc.) that are frequently used to address chronic conditions are less well studied. The goal of this study is to compare knowledge-based and machine learning strategies for the extraction of both drug and non-drug treatments. METHODS: We collected 800 randomized clinical trial abstracts each for breast cancer and diabetes from PubMed. The treatments in the result/conclusion sentences of the abstracts were manually annotated and marked as drug/non-drug treatments. We then designed three methods to identify the treatments and evaluated the systems with respect to drug/non-drug treatments. The first method is solely based on knowledge base (here we used MetaMap). The second method is based on a machine learning model trained mainly on contextual features (ML_only). The third method is a combination approach that integrates the previous two approaches. RESULTS/DISCUSSION: Results show that MetaMap, when used with high precision semantic types, has better performance for drug compared to non-drug treatments (F1=0.77 vs. 0.64). The ML_only approach has smaller performance difference between drug and non-drug treatments compared with the KB-based approach (F1=0.02 vs. 0.05, 0.07, and 0.13). The combination approach achieves significantly better performance than all MetaMap approaches alone for total treatments (F1=0.76 vs. 0.72, p<0.001). The performance gain mainly comes from the non-drug treatments (0.03-0.08 improvement in F1), while the drug treatments do not benefit much from the combination approach (0-0.03 improvement in F1). CONCLUSION: These results suggest that a knowledge-based approach should be employed for medical conditions that are primarily treated with drugs whereas conditions that are treated with either a combination of drug and non-drug interventions or primarily non-drug interventions should use automated tools that combine machine learning and a knowledge-based approach to achieve optimal performance.","Comments":"","TypeName":"","Authors":"Guo, J.;  and Blake, C.;  and Guan, Y.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":66,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":76,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":4462,"Title":"Automatic extracting of patient-related attributes: disease, age, gender and race","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2012","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"In the Evidence-based Medicine (EBM), PICO format is designed to easily and correctly search for the best available evidence. As the main element of PICO, the Patient/Problem (P) represents the attributes of patient in the clinical question and studies. In order to better understand the clinical problems, patient attribute identification is crucial and indispensable. Due to the richness of the human nature language, many issues like various term representations, grammar structures and abbreviations present challenges for automatically extracting the patient-related attributes from the unstructured data. In this paper, we employed the nature language processing (NLP) technologies to deeply analyze the linguistic characteristics of the attributes. Based on the NLP analysis results, we built the rule sets for different attributes and applied the rule-based approach to extract the patient-related attributes.","Comments":"","TypeName":"","Authors":"Zhu, H.;  and Ni, Y.;  and Cai, P.;  and Qiu, Z.;  and Cao, F.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":9742,"Title":"Extracting PICO elements from RCT abstracts using 1-2gram analysis and   multitask classification","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/1901.08351","OldItemId":"","Abstract":"The core of evidence-based medicine is to read and analyze numerous papers in the medical literature on a specific clinical problem and summarize the authoritative answers to that problem. Currently, to formulate a clear and focused clinical problem, the popular PICO framework is usually adopted, in which each clinical problem is considered to consist of four parts: patient/problem (P), intervention (I), comparison (C) and outcome (O). In this study, we compared several classification models that are commonly used in traditional machine learning. Next, we developed a multitask classification model based on a soft-margin SVM with a specialized feature engineering method that combines 1-2gram analysis with TF-IDF analysis. Finally, we trained and tested several generic models on an open-source data set from BioNLP 2018. The results show that the proposed multitask SVM classification model based on 1-2gram TF-IDF features exhibits the best performance among the tested models.","Comments":"","TypeName":"","Authors":"['Xia Yuan', 'Liao xiaoli', 'Li Shilei', 'Shi Qinwen', 'Wu Jinfa', 'Li Ke']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":70,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1454,"Title":"Identifying scientific artefacts in biomedical literature: the Evidence Based Medicine use case","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2014","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1016/j.jbi.2014.02.006","OldItemId":"","Abstract":"Evidence Based Medicine (EBM) provides a framework that makes use of the current best evidence in the domain to support clinicians in the decision making process. In most cases, the underlying foundational knowledge is captured in scientific publications that detail specific clinical studies or randomised controlled trials. Over the course of the last two decades, research has been performed on modelling key aspects described within publications (e.g., aims, methods, results), to enable the successful realisation of the goals of EBM. A significant outcome of this research has been the PICO (Population/Problem-Intervention-Comparison-Outcome) structure, and its refined version PIBOSO (Population-Intervention-Background-Outcome-Study Design-Other), both of which provide a formalisation of these scientific artefacts. Subsequently, using these schemes, diverse automatic extraction techniques have been proposed to streamline the knowledge discovery and exploration process in EBM. In this paper, we present a Machine Learning approach that aims to classify sentences according to the PIBOSO scheme. We use a discriminative set of features that do not rely on any external resources to achieve results comparable to the state of the art. A corpus of 1000 structured and unstructured abstracts - i.e., the NICTA-PIBOSO corpus - is used for training and testing. Our best CRF classifier achieves a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, which represents an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to the best existing approaches.","Comments":"","TypeName":"","Authors":"Hassanzadeh, H.;  and Groza, T.;  and Hunter, J.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":77,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":1583,"Title":"PICO element detection in medical text without metadata: are first sentences enough?","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2013","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1016/j.jbi.2013.07.009","OldItemId":"","Abstract":"Efficient identification of patient, intervention, comparison, and outcome (PICO) components in medical articles is helpful in evidence-based medicine. The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence-level PICO element detection. We extracted 19,854 structured abstracts of randomized controlled trials with any P/I/O label from PubMed for naive Bayes classifiers training. Performances of classifiers trained by first sentences of each section (CF) and those trained by all sentences (CA) were compared using all sentences by ten-fold cross-validation. The results measured by recall, precision, and F-measures show that there are no significant differences in performance between CF and CA for detection of O-element (F-measure=0.731+/-0.009 vs. 0.738+/-0.010, p=0.123). However, CA perform better for I-elements, in terms of recall (0.752+/-0.012 vs. 0.620+/-0.007, p<0.001) and F-measures (0.728+/-0.006 vs. 0.662+/-0.007, p<0.001). For P-elements, CF have higher precision (0.714+/-0.009 vs. 0.665+/-0.010, p<0.001), but lower recall (0.766+/-0.013 vs. 0.811+/-0.012, p<0.001). CF are not always better than CA in sentence-level PICO element detection. Their performance varies in detecting different elements.","Comments":"","TypeName":"","Authors":"Huang, K. C.;  and Chiang, I. J.;  and Xiao, F.;  and Liao, C. C.;  and Liu, C. C.;  and Wong, J. M.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":73,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":6575,"Title":"Indonesian medical question classification with pattern matching","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2015","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440185","OldItemId":"","Abstract":"Indonesian medical question answering system requires the extraction of named entity recognition process. This research aims to propose and evaluate a systematic approach to classify Problem, Intervention, Comparison and Outcome (PICO) from the Indonesian medical sentences. We here declare that the extraction using the PICO frames for Indonesian medical sentences is the first. The advantage of PICO frame is to accelerate the classification process based on Problem Intervention, Comparison, and Outcome criteria. Our strategy here was to build a combining question term with multiple classifiers and repetition. The training and test data were generated automatically from Indonesia medical literature with 200 sentences by the exact pattern match of head words of P-I-C-O categories. This approach achieved F-measure values of 0.90 for Problem and Intervention; 0.89 for Problem, Intervention, and Comparison; 0.91 for Problem, Comparison and Outcome. It then can be concluded that by the pattern in matching criteria of the training set and the classification of PICO elements is reproducible with minimal expert intervention.","Comments":"","TypeName":"","Authors":"W. Suwarningsih;  and A. Purwarianti;  and I. Supriana","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":71,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":27,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":248,"Title":"Tool for filtering PubMed search results by sample size","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2018","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1093/jamia/ocx155","OldItemId":"","Abstract":"Objective: The most used search engine for scientific literature, PubMed, provides tools to filter results by several fields. When searching for reports on clinical trials, sample size can be among the most important factors to consider. However, PubMed does not currently provide any means of filtering search results by sample size. Such a filtering tool would be useful in a variety of situations, including meta-analyses or state-of-the-art analyses to support experimental therapies. In this work, a tool was developed to filter articles identified by PubMed based on their reported sample sizes. Materials and Methods: A search engine was designed to send queries to PubMed, retrieve results, and compute estimates of reported sample sizes using a combination of syntactical and machine learning methods. The sample size search tool is publicly available for download at http://ihealth.uemc.es. Its accuracy was assessed against a manually annotated database of 750 random clinical trials returned by PubMed. Results: Validation tests show that the sample size search tool is able to accurately (1) estimate sample size for 70% of abstracts and (2) classify 85% of abstracts into sample size quartiles. Conclusions: The proposed tool was validated as useful for advanced PubMed searches of clinical trials when the user is interested in identifying trials of a given sample size.","Comments":"","TypeName":"","Authors":"Baladron, C.;  and Santos-Lozano, A.;  and Aguiar, J. M.;  and Lucia, A.;  and Martin-Hernandez, J.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":66,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":62,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":27,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":2748,"Title":"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2018","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the 'PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.","Comments":"","TypeName":"","Authors":"Nye, B.;  and Jessy Li, J.;  and Patel, R.;  and Yang, Y.;  and Marshall, I. J.;  and Nenkova, A.;  and Wallace, B. C.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":73,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":6659,"Title":"PICO extraction by combining the robustness of machine-learning methods with the rule-based methods","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LM","DateEdited":"","EditedBy":"","Year":"2015","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367038","OldItemId":"","Abstract":"Machine-learning methods (MLMs) are robust methods in the extraction of the information; they have been also used in the extraction of PICO elements in order to answer clinical questions; MLMs are only used at coarse-grained level in PICO extraction, because of lack of training corpora for PICO at the fine-grained level. Coarse-grained level cannot explore the semantics within the sentence for use as a means of relevance between different answers. We propose a hybrid approach combining the robustness of MLMs and the fine grained level of RBMs to enhance PICO extraction process and facilitate the validity and the pertinence of the answers to clinic questions formulated with the PICO framework.","Comments":"","TypeName":"","Authors":"S. Chabou;  and M. Iglewski","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":16,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":9760,"Title":"Towards identifying intervention arms in randomized controlled trials: Extracting coordinating constructions","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2009","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"<Go to ISI>://WOS:000270870500005","OldItemId":"","Abstract":"Background: Large numbers of reports of randomized controlled trials (RCTs) are published each year, and it is becoming increasingly difficult for clinicians practicing evidence-based medicine to find answers to clinical questions. The automatic machine extraction of RCT experimental details, including design methodology and outcomes, could help clinicians and reviewers locate relevant studies more rapidly and easily. Aim: This paper investigates how the comparison of interventions is documented in the abstracts of published RCTs. The ultimate goal is to use automated text mining to locate each intervention arm of a trial. This preliminary work aims to identify coordinating constructions, which are prevalent in the expression of intervention comparisons. Methods and results: An analysis of the types of constructs that describe the allocation of intervention arms is conducted, revealing that the compared interventions are predominantly embedded in coordinating constructions. A method is developed for identifying the descriptions of the assignment of treatment arms in clinical trials, using a full sentence parser to locate coordinating constructions and a statistical classifier for labeling positive examples. Predicate-argument structures are used along with other linguistic features with a maximum entropy classifier. An F-score of 0.78 is obtained for labeling relevant coordinating constructions in an independent test set. Conclusions: The intervention arms of a randomized controlled trials can be identified by machine extraction incorporating syntactic features derived from full sentence parsing. (C) 2008 Elsevier Inc. All rights reserved.","Comments":"","TypeName":"","Authors":"Chung, G. Y. C.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":78,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":52,"ItemAttributeFullTextDetails":[]},{"AttributeId":79,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":10348,"Title":"A distantly supervised dataset for automated data extraction from diagnostic studies.","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.18653/v1/w19-5012","OldItemId":"","Abstract":"not available","Comments":"","TypeName":"","Authors":"Christopher R. Norman;  and Mariska M. G. Leeflang;  and Ren Spijker;  and Evangelos Kanoulas;  and Aurlie Nvol","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":30,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":24,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":2288,"Title":"Improving Endpoint Detection to Support Automated Systematic Reviews","ParentTitle":"","ShortTitle":"","DateCreated":"01/05/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2017","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"not available","OldItemId":"","Abstract":"Authors of biomedical articles use comparison sentences to communicate the findings of a study, and to compare the results of the current study with earlier studies. The Claim Framework defines a comparison claim as a sentence that includes at least two entities that are being compared, and an endpoint that captures the way in which the entities are compared. Although automated methods have been developed to identify comparison sentences from the text, identifying the role that a specific noun plays (i.e. entity or endpoint) is much more difficult. Automated methods have been successful at identifying the second entity, but classification models were unable to clearly differentiate between the first entity and the endpoint. We show empirically that establishing if head noun is an amount or measure provides a statistically significant improvement that increases the endpoint precision from 0.42 to 0.56 on longer and from 0.51 to 0.58 on shorter sentences and recall from 0.64 to 0.71 on longer and from 0.69 to 0.74 on shorter sentences. The differences were not statistically significant for the second compared entity.","Comments":"","TypeName":"","Authors":"Lucic, A.;  and Blake, C. L.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":55,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15918,"Title":"Automatic classification of sentences to support evidence based medicine","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2011","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5","OldItemId":"","Abstract":"Aim: Given a set of pre-defined medical categories used in Evidence Based Medicine, we aim to automatically annotate sentences in medical abstracts with these labels.; Method; ; We constructed a corpus of 1,000 medical abstracts annotated by hand with specified medical categories (e.g. Intervention, Outcome). We explored the use of various features based on lexical, semantic, structural, and sequential information in the data, using Conditional Random Fields (CRF) for classification.; Results; ; For the classification tasks over all labels, our systems achieved micro-averaged f-scores of 80.9% and 66.9% over datasets of structured and unstructured abstracts respectively, using sequential features. In labeling only the key sentences, our systems produced f-scores of 89.3% and 74.0% over structured and unstructured abstracts respectively, using the same sequential features. The results over an external dataset were lower (f-scores of 63.1% for all labels, and 83.8% for key sentences).; Conclusions; ; Of the features we used, the best for classifying any given sentence in an abstract were based on unigrams, section headings, and sequential information from preceding sentences. These features resulted in improved performance over a simple bag-of-words approach, and outperformed feature sets used in previous work.","Comments":"","TypeName":"","Authors":"S.N. Kim, D. Martinez, L. Cavedon, L. Yencken","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":72,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15919,"Title":"Combining Classifiers for robust PICO element detection","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2010","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-10-29","OldItemId":"","Abstract":"Background; ; Formulating a clinical information need in terms of the four atomic parts which are Population/Problem, Intervention, Comparison and Outcome (known as PICO elements) facilitates searching for a precise answer within a large medical citation database. However, using PICO defined items in the information retrieval process requires a search engine to be able to detect and index PICO elements in the collection in order for the system to retrieve relevant documents.; Methods; ; In this study, we tested multiple supervised classification algorithms and their combinations for detecting PICO elements within medical abstracts. Using the structural descriptors that are embedded in some medical abstracts, we have automatically gathered large training/testing data sets for each PICO element.; Results; ; Combining multiple classifiers using a weighted linear combination of their prediction scores achieves promising results with an f-measure score of 86.3% for P, 67% for I and 56.6% for O.; Conclusions; ; Our experiments on the identification of PICO elements showed that the task is very challenging. Nevertheless, the performance achieved by our identification method is competitive with previously published results and shows that this task can be achieved with a high accuracy for the P element but lower ones for I and O elements.","Comments":"","TypeName":"","Authors":"Boudin F, Nie JY, Bartlett JC, Grad R, Pluye P, Dawes M","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":80,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":76,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15920,"Title":"A statistical relational learning approach to identifying evidence based medicine categories","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2012","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.aclweb.org/anthology/D12-1053/","OldItemId":"","Abstract":"Evidence-based medicineis an approachwhereby clinical decisions are supported bythe best available findings gained from scien-tific research. This requires efficient accessto such evidence. To this end, abstracts inevidence-based medicine can be labeled usinga set of predefined medical categories, the so-calledPICOcriteria. This paper presents anapproach to automatically annotate sentencesin medical abstracts with these labels. Sinceboth structural and sequential information areimportant for this classification task, we usekLog, a new language for statistical relationallearning with kernels. Our results show a clearimprovement with respect to state-of-the-artsystems.","Comments":"","TypeName":"","Authors":"Verbeke M, Van Asch V, Morante R, Frasconi P, Daelemans W, De Raedt L. A","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":81,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":82,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15921,"Title":"Extracting clinical trial design information from MEDLINE abstracts","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2006","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://link.springer.com/article/10.1007/s00354-007-0017-5","OldItemId":"","Abstract":"Evidence-based medicine (EBM) requires medical practitioners to select appropriate treatments for individual patients based on the current best evidence, and the results of phase III clinical trials are the major source of such evidence. In this paper, we report results of experiment in extracting important information for EBM from the abstracts of phase III clinical trials, in an effort to investigate how far the existing natural language processing (NLP) techniques could support EBM using MEDLINE database.","Comments":"","TypeName":"","Authors":"Hara K, Matsumoto Y. Extracting clinical trial design information from MEDLINE abstracts","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":66,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":76,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15922,"Title":"Exploiting classification correlations for the extraction of evidence-based practice information","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2012","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540431/","OldItemId":"","Abstract":"Crucial study data in research articles, such as patient details, study design and results, need to be extracted and presented explicitly for the ease of applicability and validity judgment in evidence-based practice. To perform this extraction, we propose to use two soft classifications, one at the sentence level and the other at the word level, and exploit the correlations between them for better accuracy. Our evaluation results show that propagating the results from the first classification to second improves performance of the second and vice versa. Moreover, the two classifications may benefit each other and help improve performance through joint inference algorithms. Another key finding of our work is that irrelevant sentences in the training data need to be properly filtered out; otherwise they compromise system accuracy and make joint inference models less scalable and more expensive to train.","Comments":"","TypeName":"","Authors":"Zhao J, Bysani P, Kan MY","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":51,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15924,"Title":"A  method  of  extracting  thenumber  of  trial participants   from   abstracts   describing   randomized   controlled   trials","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2008","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://pubmed.ncbi.nlm.nih.gov/18852316/","OldItemId":"","Abstract":"We have developed a method for extracting the number of trial participants from abstracts describing randomized controlled trials (RCTs); the number of trial participants may be an indication of the reliability of the trial. The method depends on statistical natural language processing. The number of interest was determined by a binary supervised classification based on a support vector machine algorithm. The method was trialled on 223 abstracts in which the number of trial participants was identified manually to act as a gold standard. Automatic extraction resulted in 2 false-positive and 19 false-negative classifications. The algorithm was capable of extracting the number of trial participants with an accuracy of 97% and an F-measure of 0.84. The algorithm may improve the selection of relevant articles in regard to question-answering, and hence may assist in decision-making.","Comments":"","TypeName":"","Authors":"Hansen  MJ,  Rasmussen  NO,  Chung  G","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":80,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":83,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15925,"Title":"Extracting Subject Demographic Information from Abstracts of Randomized Clinical Trial Reports","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2007","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://pubmed.ncbi.nlm.nih.gov/17911777/","OldItemId":"","Abstract":"In order to make more informed healthcare decisions, consumers need information systems that deliver accurate and reliable information about their illnesses and potential treatments. Reports of randomized clinical trials (RCTs) provide reliable medical evidence about the efficacy of treatments. Current methods to access, search for, and retrieve RCTs are keyword-based, time-consuming, and suffer from poor precision. Personalized semantic search and medical evidence summarization aim to solve this problem. The performance of these approaches may improve if they have access to study subject descriptors (e.g. age, gender, and ethnicity), trial sizes, and diseases/symptoms studied.","Comments":"","TypeName":"","Authors":"Xu R, Garten Y, Supekar KS, Das AK, Altman RB, Garber AM","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":16,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15926,"Title":"Identifying treatments, groups, and outcomes in medical abstracts","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2009","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.216.2540&rep=rep1&type=pdf","OldItemId":"","Abstract":"Detecting and extracting treatments, treatmentgroups and outcomes is a key step in gener-ating summaries of medical research papers.We describe initial results in applying named-entity recognition methods to the task of ex-tracting such entities from BMJ abstracts. Re-sults are promising, showing that a conditionalrandom field approach using word and seman-tic features appears to be more useful for rec-ognizing treatments and outcomes than fea-tures based on word shape.","Comments":"","TypeName":"","Authors":"Summerscales RL, Argamon S, Hupert J, Schwartz A","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":55,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15927,"Title":"Automatic Summarization of Results from Clinical Trials","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2011","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://ieeexplore.ieee.org/document/6120468","OldItemId":"","Abstract":"A central concern in Evidence Based Medicine (EBM) is how to convey research results effectively to practitioners. One important idea is to summarize results by key summary statistics that describe the effectiveness (or lack thereof) of a given intervention, specifically the absolute risk reduction (ARR) and number needed to treat (NNT). Manual summarization is slow and expensive, thus, with the exponential growth of the biomedical research literature, automated solutions are needed. In this paper, we present a novel method for automatically creating EBM-oriented summaries from research abstracts of randomly-controlled trials (RCTs). The system extracts descriptions of the treatment groups and outcomes, as well as various associated quantities, and then calculates summary statistics. Results on a hand-annotated corpus of research abstracts show promising, and potentially useful, results.","Comments":"","TypeName":"","Authors":"Summerscales Rodney L, Argamon Shlomo, Bai Shangda, Hupert Jordan, Schwartz Alan","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":84,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":44,"ItemAttributeFullTextDetails":[]},{"AttributeId":45,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":72,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15928,"Title":"Extracting formulaic and free text clinical research articles metadata using conditional random fields","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2010","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.aclweb.org/anthology/W10-1114/","OldItemId":"","Abstract":"We explore the use of conditional randomfields (CRFs) to automatically extract impor-tant metadata from clinical research articles.These metadata fields include formulaic meta-data about the authors, extracted from the titlepage, as well as free text fields concerning thestudys critical parameters, such as longitudi-nal variables and medical intervention meth-ods, extracted from the body text of the arti-cle. Extracting such information can help bothreaders conduct deep semantic search of arti-cles and policy makers and sociologists trackmacro level trends in research. Preliminary re-sults show an acceptable level of performancefor formulaic metadata and a high precisionfor those found in the free text.","Comments":"","TypeName":"","Authors":"Lin S, Ng J-P, Pradhan S, Shah J, Pietrobon R, Kan M-Y","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":69,"ItemAttributeFullTextDetails":[]},{"AttributeId":22,"ItemAttributeFullTextDetails":[]},{"AttributeId":79,"ItemAttributeFullTextDetails":[]},{"AttributeId":37,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":85,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15930,"Title":"Knowledge  Extraction  for  Clinical  Question  Answering:   Preliminary   Results","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2005","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"http://users.umiacs.umd.edu/~jimmylin/publications/Demner-Fushman_Lin_AAAI2005_workshop.pdf","OldItemId":"","Abstract":"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing oftext in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalizemajor aspects of evidence-based medicine in the formof knowledge extractors that serve as the fundamentalbuilding blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specificknowledge can be effectively leveraged to extract PICOframe elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians decision-making process have the potential to improve the qual-ity of patient care in real-world settings.","Comments":"","TypeName":"","Authors":"D.  Demner-Fushman","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":83,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15931,"Title":"Interpreting comparative constructions in biomedical text","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2007","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.aclweb.org/anthology/W07-1018.pdf","OldItemId":"","Abstract":"We propose a methodology using underspecified semantic interpretation to process comparative constructions in MEDLINE citations, concentrating on two structures that are prevalent in the research literature reporting on clinical trials for drug therapies. The method exploits an existing semantic processor, SemRep, which constructs predications based on the Unified Medical Language System. Results of a preliminary evaluation were recall of 70%, precision of 96%, and F-score of 81%. We discuss the generalization of the methodology to other entities such as therapeutic and diagnostic procedures. The available structures in computable format are potentially useful for interpreting outcome statements in MEDLINE citations.","Comments":"","TypeName":"","Authors":"M. Fiszman, D. Demner-Fushman, F.M. Lang, P. Goetz, T.C. Rindflesch","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":82,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15932,"Title":"Automatically Identifying Health Outcome Information in MEDLINE Records","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2006","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380197/","OldItemId":"","Abstract":"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes.; ; Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements.; ; Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10.; ; Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10.; ; Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicinemodel approach in timely delivery of information critical to clinical decision support at the point of service.","Comments":"","TypeName":"","Authors":"Demner-Fushman D, Few B, Hauser SE, Thoma GR","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":83,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15933,"Title":"A study of structured clinical abstracts and the semantic classification of sentences","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2007","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.aclweb.org/anthology/W07-1016.pdf","OldItemId":"","Abstract":"This paper describes experiments in classi-fying sentences of medical abstracts into anumber of semantic classes given by sectionheadings in structured abstracts. Using con-ditional random fields, we obtainF-scoresranging from 0.72 to 0.97. By using a smallset of sentences that appear under thePAR-TICPANTSheading, we demonstrate that it ispossible to recognize sentences that describepopulation characteristics of a study. Wepresent a detailed study of the structure ofabstracts of randomized clinical trials, andexamine how sentences labeled underPAR-TICIPANTScould be used to summarize thepopulation group.","Comments":"","TypeName":"","Authors":"Chung GY, Coiera E","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":72,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15935,"Title":"Improving Medical Information Retrieval with PICO Element Detection","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2010","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://link.springer.com/content/pdf/10.1007%2F978-3-642-12275-0.pdf","OldItemId":"","Abstract":"Without a well formulated and structured question, it can be very difficult and time consuming for physicians to identify appropriate resources and search for the best available evidence for medical treatment in evidence-based medicine (EBM). In EBM, clinical studies and questions involve four aspects: Population/Problem (P), Intervention (I), Comparison (C) and Outcome (O), which are known as PICO elements. It is intuitively more advantageous to use these elements in Information Retrieval (IR). In this paper, we first propose an approach to automatically identify the PICO elements in documents and queries. We test several possible approaches to use the identified elements in IR. Experiments show that it is a challenging task to determine accurately PICO elements. However, even with noisy tagging results, we can still take advantage of some PICO elements, namely I and P elements, to enhance the retrieval process, and this allows us to obtain significantly better retrieval effectiveness than the state-of-the-art methods.","Comments":"","TypeName":"","Authors":"Boudin F, Shi L, Nie JY","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":76,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15936,"Title":"Overview of the ALTA 2012 Shared Task","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2012","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.aclweb.org/anthology/U12-1017.pdf","OldItemId":"","Abstract":"The ALTA shared task ran for the third time in2012, with the aim of bringing research studentstogether to work on the same task and data set,and compare their methods in a current researchproblem. The task was based on a recent studyto build classifiers for automatically labeling sen-tences to a pre-defined set of categories, in the do-main of Evidence Based Medicine (EBM). Thepartaking groups demonstrated strong skills thisyear, outperforming our proposed benchmark sys-tems. In this overview paper we explain the pro-cess of building the benchmark classifiers anddata set, and present the submitted systems andtheir performance.","Comments":"","TypeName":"","Authors":"Amini I, Martinez D, Molla D","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2031,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15939,"Title":"Trialstreamer: a living, automatically updated database of clinical trial reports","ParentTitle":"","ShortTitle":"","DateCreated":"9/24/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.google.com/search?q=Trialstreamer:+a+living,+automatically+updated+database+of+clinical+trial+reports","OldItemId":"","Abstract":"Objective Randomized controlled trials (RCTs) are the gold standard method for evaluating whether a treatment works in healthcare, but can be difficult to find and make use of. We describe the development and evaluation of a system to automatically find and categorize all new RCT reports. Materials and Methods Trialstreamer, continuously monitors PubMed and the WHO International Clinical Trials Registry Platform (ICTRP), looking for new RCTs in humans using a validated classifier. We combine machine learning and rule-based methods to extract information from the RCT abstracts, including free-text descriptions of trial populations, interventions and outcomes (the 'PICO') and map these snippets to normalised MeSH vocabulary terms. We additionally identify sample sizes, predict the risk of bias, and extract text conveying key findings. We store all extracted data in a database which we make freely available for download, and via a search portal, which allows users to enter structured clinical queries. Results are ranked automatically to prioritize larger and higher-quality studies. Results As of May 2020, we have indexed 669,895 publications of RCTs, of which 18,485 were published in the first four months of 2020 (144/day). We additionally include 303,319 trial registrations from ICTRP. The median trial sample size in the RCTs was 66. Conclusions We present an automated system for finding and categorising RCTs. This yields a novel resource: A database of structured information automatically extracted for all published RCTs in humans. We make daily updates of this database available on our website (trialstreamer.robotreviewer.net).","Comments":"","TypeName":"","Authors":"Iain J Marshall, Benjamin Nye, Jol Kuiper, Anna Noel-Storr, Rachel Marshall, Rory Maclean, Frank Soboczenski, Ani Nenkova, James Thomas, Byron C Wallace","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15968,"Title":"Clinical Context-Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation","ParentTitle":"","ShortTitle":"","DateCreated":"11/02/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.2196/19810","OldItemId":"","Abstract":"Automatic text summarization (ATS) enables users to retrieve meaningful evidence from big data of biomedical repositories to make complex clinical decisions. Deep neural and recurrent networks outperform traditional machine-learning techniques in areas of natural language processing and computer vision; however, they are yet to be explored in the ATS domain, particularly for medical text summarization. Traditional approaches in ATS for biomedical text suffer from fundamental issues such as an inability to capture clinical context, quality of evidence, and purpose-driven selection of passages for the summary. We aimed to circumvent these limitations through achieving precise, succinct, and coherent information extraction from credible published biomedical resources, and to construct a simplified summary containing the most informative content that can offer a review particular to clinical needs. In our proposed approach, we introduce a novel framework, termed Biomed-Summarizer, that provides quality-aware Patient/Problem, Intervention, Comparison, and Outcome (PICO)-based intelligent and context-enabled summarization of biomedical text. Biomed-Summarizer integrates the prognosis quality recognition model with a clinical context-aware model to locate text sequences in the body of a biomedical article for use in the final summary. First, we developed a deep neural network binary classifier for quality recognition to acquire scientifically sound studies and filter out others. Second, we developed a bidirectional long-short term memory recurrent neural network as a clinical context-aware classifier, which was trained on semantically enriched features generated using a word-embedding tokenizer for identification of meaningful sentences representing PICO text sequences. Third, we calculated the similarity between query and PICO text sequences using Jaccard similarity with semantic enrichments, where the semantic enrichments are obtained using medical ontologies. Last, we generated a representative summary from the high-scoring PICO sequences aggregated by study type, publication credibility, and freshness score. Evaluation of the prognosis quality recognition model using a large dataset of biomedical literature related to intracranial aneurysm showed an accuracy of 95.41% (2562/2686) in terms of recognizing quality articles. The clinical context-aware multiclass classifier outperformed the traditional machine-learning algorithms, including support vector machine, gradient boosted tree, linear regression, K-nearest neighbor, and nave Bayes, by achieving 93% (16127/17341) accuracy for classifying five categories: aim, population, intervention, results, and outcome. The semantic similarity algorithm achieved a significant Pearson correlation coefficient of 0.61 (0-1 scale) on a well-known BIOSSES dataset (with 100 pair sentences) after semantic enrichment, representing an improvement of 8.9% over baseline Jaccard similarity. Finally, we found a highly positive correlation among the evaluations performed by three domain experts concerning different metrics, suggesting that the automated summarization is satisfactory. By employing the proposed method Biomed-Summarizer, high accuracy in ATS was achieved, enabling seamless curation of research evidence from the biomedical literature to use for clinical decision-making.","Comments":"","TypeName":"","Authors":"Afzal, Alam, Malik, Malik","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":86,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":16128,"Title":"EBM+: Advancing Evidence-Based Medicine via two level automatic identification of Populations, Interventions, Outcomes in medical literature","ParentTitle":"","ShortTitle":"","DateCreated":"11/02/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1016/j.artmed.2020.101949","OldItemId":"","Abstract":"Evidence-Based Medicine (EBM) has been an important practice for medical practitioners. However, as the number of medical publications increases dramatically, it is becoming extremely difficult for medical experts to review all the contents available and make an informative treatment plan for their patients. A variety of frameworks, including the PICO framework which is named after its elements (Population, Intervention, Comparison, Outcome), have been developed to enable fine-grained searches, as the first step to faster decision making. In this work, we propose a novel entity recognition system that identifies PICO entities within medical publications and achieves state-of-the-art performance in the task. This is achieved by the combination of four 2D Convolutional Neural Networks (CNNs) for character feature extraction, and a Highway Residual connection to facilitate deep Neural Network architectures. We further introduce a PICO Statement classifier, that identifies sentences that not only contain all PICO entities but also answer questions stated in PICO. To facilitate this task we also introduce a high quality dataset, manually annotated by medical practitioners. With the combination of our proposed PICO Entity Recognizer and PICO Statement classifier we aim to advance EBM and enable its faster and more accurate practice.","Comments":"","TypeName":"","Authors":"Stylianou, Razis, Goulis, Vlahavas","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":16957,"Title":"Aceso: PICO-guided Evidence Summarization on Medical Literature","ParentTitle":"","ShortTitle":"","DateCreated":"11/02/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1109/JBHI.2020.2984704","OldItemId":"","Abstract":"Evidence-Based Medicine (EBM) aims to apply the best available evidence gained from scientific methods to clinical decision making. A generally accepted criterion to formulate evidence is to use the PICO framework, where PICO stands for Problem/Population, Intervention, Comparison, and Outcome. Automatic extraction of PICO-related sentences from medical literature is crucial to the success of many EBM applications. In this work, we present our Aceso system, which automatically generates PICO-based evidence summaries from medical literature. In Aceso <sup>1</sup>, we adopt an active learning paradigm, which helps to minimize the cost of manual labeling and to optimize the quality of summarization with limited labeled data. An UMLS2Vec model is proposed to learn a vector representation of medical concepts in UMLS <sup>2</sup>, and we fuse the embedding of medical knowledge with textual features in summarization. The evaluation shows that our approach is better on identifying PICO sentences against state-of-the-art studies and outperforms baseline methods on producing high-quality evidence summaries.","Comments":"","TypeName":"","Authors":"Zhang, Geng, Zhang, Lu, Gao, Mei","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":17134,"Title":"Evidence Inference 20: More Data, Better Models","ParentTitle":"","ShortTitle":"","DateCreated":"11/06/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2005.04177","OldItemId":"","Abstract":"How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled systematic reviews of medical literature to inform care.   NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The Evidence Inference dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that chemotherapy performed better than surgery for five-year survival rates of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25\\%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an abstract only (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at http://evidence-inference.ebm-nlp.com/.","Comments":"","TypeName":"","Authors":"['Jay DeYoung', 'Eric Lehman', 'Ben Nye', 'Iain J. Marshall', 'Byron C. Wallace']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":62,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":17138,"Title":"Unlocking the Power of Deep PICO Extraction: Step-wise Medical NER   Identification","ParentTitle":"","ShortTitle":"","DateCreated":"11/06/2020","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2005.06601","OldItemId":"","Abstract":"The PICO framework (Population, Intervention, Comparison, and Outcome) is usually used to formulate evidence in the medical domain. The major task of PICO extraction is to extract sentences from medical literature and classify them into each class. However, in most circumstances, there will be more than one evidences in an extracted sentence even it has been categorized to a certain class. In order to address this problem, we propose a step-wise disease Named Entity Recognition (DNER) extraction and PICO identification method. With our method, sentences in paper title and abstract are first classified into different classes of PICO, and medical entities are then identified and classified into P and O. Different kinds of deep learning frameworks are used and experimental results show that our method will achieve high performance and fine-grained extraction results comparing with conventional PICO extraction works.","Comments":"","TypeName":"","Authors":"['Tengteng Zhang', 'Yiqin Yu', 'Jing Mei', 'Zefang Tang', 'Xiang Zhang', 'Shaochun Li']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":17677,"Title":"MS2: Multi-Document Summarization of Medical Studies","ParentTitle":"","ShortTitle":"","DateCreated":"08/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2104.06486","OldItemId":"","Abstract":"To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and highly manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS^2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system's generated summaries. Data and models are available at https://github.com/allenai/ms2","Comments":"","TypeName":"","Authors":"['Jay DeYoung', 'Iz Beltagy', 'Madeleine van Zuylen', 'Bailey Kuehl', 'Lucy Lu Wang']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":18405,"Title":"A neuro-symbolic method for understanding free-text medical evidence","ParentTitle":"","ShortTitle":"","DateCreated":"08/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1093/jamia/ocab077","OldItemId":"","Abstract":"We introduce Medical evidence Dependency (MD)-informed attention, a novel neuro-symbolic model for understanding free-text clinical trial publications with generalizability and interpretability. We trained one head in the multi-head self-attention model to attend to the Medical evidence Ddependency (MD) and to pass linguistic and domain knowledge on to later layers (MD informed). This MD-informed attention model was integrated into BioBERT and tested on 2 public machine reading comprehension benchmarks for clinical trial publications: Evidence Inference 2.0 and PubMedQA. We also curated a small set of recently published articles reporting randomized controlled trials on COVID-19 (coronavirus disease 2019) following the Evidence Inference 2.0 guidelines to evaluate the model's robustness to unseen data. The integration of MD-informed attention head improves BioBERT substantially in both benchmark tasks-as large as an increase of +30% in the F1 score-and achieves the new state-of-the-art performance on the Evidence Inference 2.0. It achieves 84% and 82% in overall accuracy and F1 score, respectively, on the unseen COVID-19 data. MD-informed attention empowers neural reading comprehension models with interpretability and generalizability via reusable domain knowledge. Its compositionality can benefit any transformer-based architecture for machine reading comprehension of free-text medical evidence.","Comments":"","TypeName":"","Authors":"Kang, Turfah, Kim, Perotte, Weng","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":89,"ItemAttributeFullTextDetails":[]},{"AttributeId":63,"ItemAttributeFullTextDetails":[]},{"AttributeId":64,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":44,"ItemAttributeFullTextDetails":[]},{"AttributeId":47,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":18910,"Title":"Toward assessing clinical trial publications for reporting transparency","ParentTitle":"","ShortTitle":"","DateCreated":"08/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1016/j.jbi.2021.103717","OldItemId":"","Abstract":"To annotate a corpus of randomized controlled trial (RCT) publications with the checklist items of CONSORT reporting guidelines and using the corpus to develop text mining methods for RCT appraisal. We annotated a corpus of 50 RCT articles at the sentence level using 37 fine-grained CONSORT checklist items. A subset (31 articles) was double-annotated and adjudicated, while 19 were annotated by a single annotator and reconciled by another. We calculated inter-annotator agreement at the article and section level using MASI (Measuring Agreement on Set-Valued Items) and at the CONSORT item level using Krippendorff's . We experimented with two rule-based methods (phrase-based and section header-based) and two supervised learning approaches (support vector machine and BioBERT-based neural network classifiers), for recognizing 17 methodology-related items in the RCT Methods sections. We created CONSORT-TM consisting of 10,709 sentences, 4,845 (45%) of which were annotated with 5,246 labels. A median of 28 CONSORT items (out of possible 37) were annotated per article. Agreement was moderate at the article and section levels (average MASI: 0.60 and 0.64, respectively). Agreement varied considerably among individual checklist items (Krippendorff's = 0.06-0.96). The model based on BioBERT performed best overall for recognizing methodology-related items (micro-precision: 0.82, micro-recall: 0.63, micro-F1: 0.71). Combining models using majority vote and label aggregation further improved precision and recall, respectively. Our annotated corpus, CONSORT-TM, contains more fine-grained information than earlier RCT corpora. Low frequency of some CONSORT items made it difficult to train effective text mining models to recognize them. For the items commonly reported, CONSORT-TM can serve as a testbed for text mining methods that assess RCT transparency, rigor, and reliability, and support methods for peer review and authoring assistance. Minor modifications to the annotation scheme and a larger corpus could facilitate improved text mining models. CONSORT-TM is publicly available at https://github.com/kilicogluh/CONSORT-TM.","Comments":"","TypeName":"","Authors":"Kilicoglu, Rosemblat, Hoang, Wadhwa, Peng, Maliki, Schneider, Ter Riet","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":78,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":90,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":79,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":91,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":18952,"Title":"A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine","ParentTitle":"","ShortTitle":"","DateCreated":"08/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1186/s12911-021-01395-z","OldItemId":"","Abstract":"The large volume of medical literature makes it difficult for healthcare professionals to keep abreast of the latest studies that support Evidence-Based Medicine. Natural language processing enhances the access to relevant information, and gold standard corpora are required to improve systems. To contribute with a new dataset for this domain, we collected the Clinical Trials for Evidence-Based Medicine in Spanish (CT-EBM-SP) corpus. We annotated 1200 texts about clinical trials with entities from the Unified Medical Language System semantic groups: anatomy (ANAT), pharmacological and chemical substances (CHEM), pathologies (DISO), and lab tests, diagnostic or therapeutic procedures (PROC). We doubly annotated 10% of the corpus and measured inter-annotator agreement (IAA) using F-measure. As use case, we run medical entity recognition experiments with neural network models. This resource contains 500 abstracts of journal articles about clinical trials and 700 announcements of trial protocols (292 173 tokens). We annotated 46 699 entities (13.98% are nested entities). Regarding IAA agreement, we obtained an average F-measure of 85.65% (4.79, strict match) and 93.94% (3.31, relaxed match). In the use case experiments, we achieved recognition results ranging from 80.28% (00.99) to 86.74% (00.19) of average F-measure. Our results show that this resource is adequate for experiments with state-of-the-art approaches to biomedical named entity recognition. It is freely distributed at: http://www.lllf.uam.es/ESP/nlpmedterm_en.html . The methods are generalizable to other languages with similar available sources.","Comments":"","TypeName":"","Authors":"Campillos-Llanos, Valverde-Mateos, Capllonch-Carrin, Moreno-Sandoval","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":86,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":19187,"Title":"Artificial Intelligence Clinical Evidence Engine for Automatic Identification, Prioritization, and Extraction of Relevant Clinical Oncology Research","ParentTitle":"","ShortTitle":"","DateCreated":"08/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1200/CCI.20.00087","OldItemId":"","Abstract":"We developed a system to automate analysis of the clinical oncology scientific literature from bibliographic databases and match articles to specific patient cohorts to answer specific questions regarding the efficacy of a treatment. The approach attempts to replicate a clinician's mental processes when reviewing published literature in the context of a patient case. We describe the system and evaluate its performance. We developed separate ground truth data sets for each of the tasks described in the paper. The first ground truth was used to measure the natural language processing (NLP) accuracy from approximately 1,300 papers covering approximately 3,100 statements and approximately 25 concepts; performance was evaluated using a standard F1 score. The ground truth for the expert classifier model was generated by dividing papers cited in clinical guidelines into a training set and a test set in an 80:20 ratio, and performance was evaluated for accuracy, sensitivity, and specificity. The NLP models were able to identify individual attributes with a 0.7-0.9 F1 score, depending on the attribute of interest. The expert classifier machine learning model was able to classify the individual records with a 0.93 accuracy (95% CI, 0.9 to 0.96, <i>P</i> &lt; .0001), and sensitivity and specificity of 0.95 and 0.91, respectively. Using a decision boundary of 0.5 for the positive (expert) label, the classifier demonstrated an F1 score of 0.92. The system identified and extracted evidence from the oncology literature with a high degree of accuracy, sensitivity, and specificity. This tool enables timely access to the most relevant biomedical literature, providing critical support to evidence-based practice in areas of rapidly evolving science.","Comments":"","TypeName":"","Authors":"Saiz, Sanders, Stevens, Nielsen, Britt, Yuravlivker, Preininger, Jackson","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":90,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":21316,"Title":"Enhancing evidence-based medicine with natural language argumentative analysis of clinical trials","ParentTitle":"","ShortTitle":"","DateCreated":"11/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1016/j.artmed.2021.102098","OldItemId":"","Abstract":"In the latest years, the healthcare domain has seen an increasing interest in the definition of intelligent systems to support clinicians in their everyday tasks and activities. Among others, also the field of Evidence-Based Medicine is impacted by this twist, with the aim to combine the reasoning frameworks proposed thus far in the field with mining algorithms to extract structured information from clinical trials, clinical guidelines, and Electronic Health Records. In this paper, we go beyond the state of the art by proposing a new end-to-end pipeline to address argumentative outcome analysis on clinical trials. More precisely, our pipeline is composed of (i) an Argument Mining module to extract and classify argumentative components (i.e., evidence and claims of the trial) and their relations (i.e., support, attack), and (ii) an outcome analysis module to identify and classify the effects (i.e., improved, increased, decreased, no difference, no occurrence) of an intervention on the outcome of the trial, based on PICO elements. We annotated a dataset composed of more than 500 abstracts of Randomized Controlled Trials (RCT) from the MEDLINE database, leading to a labeled dataset with 4198 argument components, 2601 argument relations, and 3351 outcomes on five different diseases (i.e., neoplasm, glaucoma, hepatitis, diabetes, hypertension). We experiment with deep bidirectional transformers in combination with different neural architectures (i.e., LSTM, GRU and CRF) and obtain a macro F1-score of.87 for component detection and.68 for relation prediction, outperforming current state-of-the-art end-to-end Argument Mining systems, and a macro F1-score of.80 for outcome classification.","Comments":"","TypeName":"","Authors":"Mayer, Marro, Cabrio, Villata","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":28,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":21231,"Title":"Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations","ParentTitle":"","ShortTitle":"","DateCreated":"11/03/2021","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.google.com/search?q=Understanding+Clinical+Trial+Reports:+Extracting+Medical+Entities+and+Their+Relations.","OldItemId":"","Abstract":"The best evidence concerning comparative treatment effectiveness comes from clinical trials, the results of which are reported in unstructured articles. Medical experts must manually extract information from articles to inform decision-making, which is time-consuming and expensive. Here we consider the <i>end-to-end</i> task of both (a) extracting treatments and outcomes from full-text articles describing clinical trials (entity identification) and, (b) inferring the reported results for the former with respect to the latter (relation extraction). We introduce new data for this task, and evaluate models that have recently achieved state-of-the-art results on similar tasks in Natural Language Processing. We then propose a new method motivated by how trial results are typically presented that outperforms these purely data-driven baselines. Finally, we run a fielded evaluation of the model with a non-profit seeking to identify existing drugs that might be re-purposed for cancer, showing the potential utility of end-to-end evidence extraction systems.","Comments":"","TypeName":"","Authors":"Nye, DeYoung, Lehman, Nenkova, Marshall, Wallace","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":22861,"Title":"Automated tabulation of clinical trial results: A joint entity and   relation extraction approach with transformer-based language representations","ParentTitle":"","ShortTitle":"","DateCreated":"12/06/2022","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2112.05596","OldItemId":"","Abstract":"Evidence-based medicine, the practice in which healthcare professionals refer to the best available evidence when making decisions, forms the foundation of modern healthcare. However, it relies on labour-intensive systematic reviews, where domain specialists must aggregate and extract information from thousands of publications, primarily of randomised controlled trial (RCT) results, into evidence tables. This paper investigates automating evidence table generation by decomposing the problem across two language processing tasks: \\textit{named entity recognition}, which identifies key entities within text, such as drug names, and \\textit{relation extraction}, which maps their relationships for separating them into ordered tuples. We focus on the automatic tabulation of sentences from published RCT abstracts that report the results of the study outcomes. Two deep neural net models were developed as part of a joint extraction pipeline, using the principles of transfer learning and transformer-based language representations. To train and test these models, a new gold-standard corpus was developed, comprising almost 600 result sentences from six disease areas. This approach demonstrated significant advantages, with our system performing well across multiple natural language processing tasks and disease areas, as well as in generalising to disease domains unseen during training. Furthermore, we show these results were achievable through training our models on as few as 200 example sentences. The final system is a proof of concept that the generation of evidence tables can be semi-automated, representing a step towards fully automating systematic reviews.","Comments":"","TypeName":"","Authors":"['Jetsun Whitton', 'Anthony Hunter']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":90,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":92,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":23005,"Title":"Assessment of contextualised representations in detecting outcome   phrases in clinical trials","ParentTitle":"","ShortTitle":"","DateCreated":"12/06/2022","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2203.03547","OldItemId":"","Abstract":"Automating the recognition of outcomes reported in clinical trials using machine learning has a huge potential of speeding up access to evidence necessary in healthcare decision-making. Prior research has however acknowledged inadequate training corpora as a challenge for the Outcome detection (OD) task. Additionally, several contextualized representations like BERT and ELMO have achieved unparalleled success in detecting various diseases, genes, proteins, and chemicals, however, the same cannot be emphatically stated for outcomes, because these models have been relatively under-tested and studied for the OD task. We introduce \"EBM-COMET\", a dataset in which 300 PubMed abstracts are expertly annotated for clinical outcomes. Unlike prior related datasets that use arbitrary outcome classifications, we use labels from a taxonomy recently published to standardize outcome classifications. To extract outcomes, we fine-tune a variety of pre-trained contextualized representations, additionally, we use frozen contextualized and context-independent representations in our custom neural model augmented with clinically informed Part-Of-Speech embeddings and a cost-sensitive loss function. We adopt strict evaluation for the trained models by rewarding them for correctly identifying full outcome phrases rather than words within the entities i.e. given an outcome \"systolic blood pressure\", the models are rewarded a classification score only when they predict all 3 words in sequence, otherwise, they are not rewarded. We observe our best model (BioBERT) achieve 81.5\\% F1, 81.3\\% sensitivity and 98.0\\% specificity. We reach a consensus on which contextualized representations are best suited for detecting outcomes from clinical-trial abstracts. Furthermore, our best model outperforms scores published on the original EBM-NLP dataset leader-board scores.","Comments":"","TypeName":"","Authors":"['Micheal Abaho', 'Danushka Bollegala', 'Paula R Williamson', 'Susanna Dodd']","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":23572,"Title":"Improvement of intervention information detection for automated clinical literature screening during systematic review","ParentTitle":"","ShortTitle":"","DateCreated":"12/06/2022","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1016/j.jbi.2022.104185","OldItemId":"","Abstract":"Systematic literature review (SLR) is a crucial method for clinicians and policymakers to make their decisions in a flood of new clinical studies. Because manual literature screening in SLR is a highly laborious task, its automation by natural language processing (NLP) has been welcomed. Although intervention is a key information for literature screening, NLP models for its detection in previous works have not shown adequate performance. In this work, we first design an algorithm for automated construction of high-quality intervention labels by utilizing information retrieved from a clinical trial database. We then design another algorithm for improving model's recall and F1 score by imposing adaptive weights on training instances in the loss function. The intervention detection model trained on the weighted datasets is tested with the Evidence-Based Medicine NLP (EBM-NLP) corpus, and shows 9.7% and 4.0% improvements respectively in recall and F1 score compared to the previous state-of-the-art model on the corpus. The proposed algorithms can boost automation of literature screening during SLR in the clinical domain. Copyright  2022 The Author(s). Published by Elsevier Inc. All rights reserved.","Comments":"","TypeName":"","Authors":"Tsubota, Tadashi; ; Bollegala, Danushka; ; Zhao, Yang; ; Jin, Yingzi; ; Kozu, Tomotake","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":30,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":23753,"Title":"PICO entity extraction for preclinical animal literature","ParentTitle":"","ShortTitle":"","DateCreated":"12/06/2022","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1186/s13643-022-02074-4","OldItemId":"","Abstract":"BACKGROUND: Natural language processing could assist multiple tasks in systematic reviews to reduce workflow, including the extraction of PICO elements such as study populations, interventions, comparators and outcomes. The PICO framework provides a basis for the retrieval and selection for inclusion of evidence relevant to a specific systematic review question, and automatic approaches to PICO extraction have been developed particularly for reviews of clinical trial findings. Considering the difference between preclinical animal studies and clinical trials, developing separate approaches is necessary. Facilitating preclinical systematic reviews will inform the translation from preclinical to clinical research.; ; METHODS: We randomly selected 400 abstracts from the PubMed Central Open Access database which described in vivo animal research and manually annotated these with PICO phrases for Species, Strain, methods of Induction of disease model, Intervention, Comparator and Outcome. We developed a two-stage workflow for preclinical PICO extraction. Firstly we fine-tuned BERT with different pre-trained modules for PICO sentence classification. Then, after removing the text irrelevant to PICO features, we explored LSTM-, CRF- and BERT-based models for PICO entity recognition. We also explored a self-training approach because of the small training corpus.; ; RESULTS: For PICO sentence classification, BERT models using all pre-trained modules achieved an F1 score of over 80%, and models pre-trained on PubMed abstracts achieved the highest F1 of 85%. For PICO entity recognition, fine-tuning BERT pre-trained on PubMed abstracts achieved an overall F1 of 71% and satisfactory F1 for Species (98%), Strain (70%), Intervention (70%) and Outcome (67%). The score of Induction and Comparator is less satisfactory, but F1 of Comparator can be improved to 50% by applying self-training.; ; CONCLUSIONS: Our study indicates that of the approaches tested, BERT pre-trained on PubMed abstracts is the best for both PICO sentence classification and PICO entity recognition in the preclinical abstracts. Self-training yields better performance for identifying comparators and strains. Copyright  2022. The Author(s).","Comments":"","TypeName":"","Authors":"Wang, Qianying; ; Liao, Jing; ; Lapata, Mirella; ; Macleod, Malcolm","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":40,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":66,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":24105,"Title":"An annotated corpus of clinical trial publications supporting schema-based relational information extraction","ParentTitle":"","ShortTitle":"","DateCreated":"12/06/2022","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://dx.doi.org/10.1186/s13326-022-00271-7","OldItemId":"","Abstract":"BACKGROUND: The evidence-based medicine paradigm requires the ability to aggregate and compare outcomes of interventions across different trials. This can be facilitated and partially automatized by information extraction systems. In order to support the development of systems that can extract information from published clinical trials at a fine-grained and comprehensive level to populate a knowledge base, we present a richly annotated corpus at two levels. At the first level, entities that describe components of the PICO elements (e.g., population's age and pre-conditions, dosage of a treatment, etc.) are annotated. The second level comprises schema-level (i.e., slot-filling templates) annotations corresponding to complex PICO elements and other concepts related to a clinical trial (e.g. the relation between an intervention and an arm, the relation between an outcome and an intervention, etc.).; ; RESULTS: The final corpus includes 211 annotated clinical trial abstracts with substantial agreement between annotators at the entity and scheme level. The mean Kappa value for the glaucoma and T2DM corpora was 0.74 and 0.68, respectively, for single entities. The micro-averaged F1 score to measure inter-annotator agreement for complex entities (i.e. slot-filling templates) was 0.81. The BERT-base baseline method for entity recognition achieved average micro- F1 scores of 0.76 for glaucoma and 0.77 for diabetes with exact matching.; ; CONCLUSIONS: In this work, we have created a corpus that goes beyond the existing clinical trial corpora, since it is annotated in a schematic way that represents the classes and properties defined in an ontology. Although the corpus is small, it has fine-grained annotations and could be used to fine-tune pre-trained machine learning models and transformers to the specific task of extracting information about clinical trial abstracts.For future work, we will use the corpus for training information extraction systems that extract single entities, and predict template slot-fillers (i.e., class data/object properties) to populate a knowledge base that relies on the C-TrO ontology for the description of clinical trials. The resulting corpus and the code to measure inter-annotation agreement and the baseline method are publicly available at https://zenodo.org/record/6365890. Copyright  2022. The Author(s).","Comments":"","TypeName":"","Authors":"Sanchez-Graillet, Olivia; ; Witte, Christian; ; Grimm, Frank; ; Cimiano, Philipp","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":54,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":25157,"Title":"Constructing Artificial Data for Fine-tuning for Low-Resource Biomedical Text Tagging with Applications in PICO Annotation","ParentTitle":"","ShortTitle":"","DateCreated":"3/21/2023","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2020","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.48550/arXiv.1910.09255","OldItemId":"","Abstract":"Biomedical text tagging systems are plagued by the dearth of labeled training data. There have been recent attempts at using pre-trained encoders to deal with this issue. Pre-trained encoder provides representation of the input text which is then fed to task-specific layers for classification. The entire network is fine-tuned on the labeled data from the target task. Unfortunately, a low-resource biomedical task often has too few labeled instances for satisfactory fine-tuning. Also, if the label space is large, it contains few or no labeled instances for majority of the labels. Most biomedical tagging systems treat labels as indexes, ignoring the fact that these labels are often concepts expressed in natural language e.g. `Appearance of lesion on brain imaging'. To address these issues, we propose constructing extra labeled instances using label-text (i.e. label's name) as input for the corresponding label-index (i.e. label's index). In fact, we propose a number of strategies for manufacturing multiple artificial labeled instances from a single label. The network is then fine-tuned on a combination of real and these newly constructed artificial labeled instances. We evaluate the proposed approach on an important low-resource biomedical task called \\textit{PICO annotation}, which requires tagging raw text describing clinical trials with labels corresponding to different aspects of the trial i.e. PICO (Population, Intervention/Control, Outcome) characteristics of the trial. Our empirical results show that the proposed method achieves a new state-of-the-art performance for PICO annotation with very significant improvements over competitive baselines.","Comments":"","TypeName":"","Authors":"Gaurav Singh; Zahra Sabet; John Shawe-Taylor; James Thomas","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":25158,"Title":"Detect and Classify  Joint Span Detection and Classification for Health Outcomes","ParentTitle":"","ShortTitle":"","DateCreated":"3/21/2023","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://www.google.com/search?q=Detect+and+Classify++Joint+Span+Detection+and+Classification+for+Health+Outcomes","OldItemId":"","Abstract":"A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and classification is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.","Comments":"","TypeName":"","Authors":"Abaho Michael; Bollegala Danushka; Williamson Paula R; Dodd Susanna","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":25174,"Title":"Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations","ParentTitle":"","ShortTitle":"","DateCreated":"3/21/2023","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.48550/arxiv.2109.02254","OldItemId":"","Abstract":"The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which requires finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening - the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in human-machine pipeline for turning low-quality crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews.","Comments":"","TypeName":"","Authors":"Liu Shifeng; Sun Yifang; Li Bing; Wang Wei; Bourgeois Florence T; Dunn Adam G","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":25189,"Title":"CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies","ParentTitle":"","ShortTitle":"","DateCreated":"3/21/2023","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1109/bigdata55660.2022.10020807","OldItemId":"","Abstract":"Clinical Cohort Studies (CCS), such as randomized clinical trials, are a great source of documented clinical research. Ideally, a clinical expert inspects these articles for exploratory analysis ranging from drug discovery for evaluating the efficacy of existing drugs in tackling emerging diseases to the first test of newly developed drugs. However, more than 100 articles are published daily on a single prevalent disease like COVID-19 in PubMed. As a result, it can take days for a physician to find articles and extract relevant information. Can we develop a system to sift through the long list of these articles faster and document the crucial takeaways from each of these articles? In this work, we propose CCS Explorer, an end-to-end system for relevance prediction of sentences, extractive summarization, and patient, outcome, and intervention entity detection from CCS. CCS Explorer is packaged in a web-based graphical user interface where the user can provide any disease name. CCS Explorer then extracts and aggregates all relevant information from articles on PubMed based on the results of an automatically generated query produced on the back-end. For each task, CCS Explorer fine-tunes pre-trained language representation models based on transformers with additional layers. The models are evaluated using two publicly available datasets. CCS Explorer obtains a recall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence relevance prediction using BioBERT and achieves an average Micro F1-Score of 77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus, CCS Explorer can reliably extract relevant information to summarize articles, saving time by $\\sim \\text{660}\\times$.","Comments":"","TypeName":"","Authors":"Al-Hussaini Irfan; An Davi Nakajima; Lee Albert J; Bi Sarah; Mitchell Cassie S","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":25193,"Title":"Automated detection of over- and under-dispersion in baseline tables in randomised controlled trials","ParentTitle":"","ShortTitle":"","DateCreated":"3/21/2023","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.12688/f1000research.123002.1","OldItemId":"","Abstract":"<ns3:p><ns3:bold>Background</ns3:bold>: Papers describing the results of a randomised trial should include a baseline table that compares the characteristics of randomised groups. Researchers who fraudulently generate trials often unwittingly create baseline tables that are implausibly similar (under-dispersed) or have large differences between groups (over-dispersed). I aimed to create an automated algorithm to screen for under- and over-dispersion in the baseline tables of randomised trials.</ns3:p><ns3:p> <ns3:bold>Methods</ns3:bold>: Using a cross-sectional study I examined 2,245 randomised controlled trials published in health and medical journals on <ns3:italic>PubMed Central</ns3:italic>. I estimated the probability that a trial's baseline summary statistics were under- or over-dispersed using a Bayesian model that examined the distribution of t-statistics for the between-group differences, and compared this with an expected distribution without dispersion. I used a simulation study to test the ability of the model to find under- or over-dispersion and compared its performance with an existing test of dispersion based on a uniform test of p-values. My model combined categorical and continuous summary statistics, whereas the uniform uniform test used only continuous statistics.</ns3:p><ns3:p> <ns3:bold>Results</ns3:bold>: The algorithm had a relatively good accuracy for extracting the data from baseline tables, matching well on the size of the tables and sample size. Using t-statistics in the Bayesian model out-performed the uniform test of p-values, which had many false positives for skewed, categorical and rounded data that were not under- or over-dispersed. For trials published on <ns3:italic>PubMed Central</ns3:italic>, some tables appeared under- or over-dispersed because they had an atypical presentation or had reporting errors. Some trials flagged as under-dispersed had groups with strikingly similar summary statistics.</ns3:p><ns3:p> <ns3:bold>Conclusions</ns3:bold>: Automated screening for fraud of all submitted trials is challenging due to the widely varying presentation of baseline tables. The Bayesian model could be useful in targeted checks of suspected trials or authors.</ns3:p>","Comments":"","TypeName":"","Authors":"Barnett Adrian G","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2032,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":25244,"Title":"Not so weak PICO: leveraging weak supervision for participants, interventions, and outcomes recognition for systematic review automation","ParentTitle":"","ShortTitle":"","DateCreated":"3/21/2023","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1093/jamiaopen/ooac107","OldItemId":"","Abstract":"Abstract Objective The aim of this study was to test the feasibility of PICO (participants, interventions, comparators, outcomes) entity extraction using weak supervision and natural language processing. Methodology We re-purpose more than 127 medical and nonmedical ontologies and expert-generated rules to obtain multiple noisy labels for PICO entities in the evidence-based medicine (EBM)-PICO corpus. These noisy labels are aggregated using simple majority voting and generative modeling to get consensus labels. The resulting probabilistic labels are used as weak signals to train a weakly supervised (WS) discriminative model and observe performance changes. We explore mistakes in the EBM-PICO that could have led to inaccurate evaluation of previous automation methods. Results In total, 4081 randomized clinical trials were weakly labeled to train the WS models and compared against full supervision. The models were separately trained for PICO entities and evaluated on the EBM-PICO test set. A WS approach combining ontologies and expert-generated rules outperformed full supervision for the participant entity by 1.71% macro-F1. Error analysis on the EBM-PICO subset revealed 1823% erroneous token classifications. Discussion Automatic PICO entity extraction accelerates the writing of clinical systematic reviews that commonly use PICO information to filter health evidence. However, PICO extends to more entitiesPICOS (Sstudy type and design), PICOC (Ccontext), and PICOT (Ttimeframe) for which labelled datasets are unavailable. In such cases, the ability to use weak supervision overcomes the expensive annotation bottleneck. Conclusions We show the feasibility of WS PICO entity extraction using freely available ontologies and heuristics without manually annotated data. Weak supervision has encouraging performance compared to full supervision but requires careful design to outperform it.","Comments":"","TypeName":"","Authors":"Dhrangadhariya Anjani; Mller Henning","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":59,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15192596,"Title":"SciBERT: A pretrained language model for scientific text","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2019","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"http://dx.doi.org/10.18653/v1/d19-1371","OldItemId":"","Abstract":"","Comments":"","TypeName":"","Authors":"Beltagy, Iz, Lo, Kyle, Cohan, Arman","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15192595,"Title":"Enhancing PICOS Information Extraction with UIE and ERNIE-Health","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://link.springer.com/chapter/10.1007/978-981-97-1717-0_17","OldItemId":"","Abstract":"In the vast sea of medical papers lie numerous crucial pieces of information. Faced with the challenge of unstructured medical literature, it becomes a formidable task to automatically extract the necessary key information. In this paper, we delve into the CHIP2023 evaluation task 5, utilizing UIE and ERNIE-Health to accomplish the extraction of PICOS key information from medical papers. Our results indicate that the framework we propose has achieved commendable performance.","Comments":"","TypeName":"","Authors":"Zhang, Lei, Tian, Wei, Zheng, Yuan  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15192594,"Title":"LLM Collaboration PLM Improves Critical Information Extraction Tasks in Medical Articles","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://link.springer.com/chapter/10.1007/978-981-97-1717-0_16","OldItemId":"","Abstract":"With the development of modern medical informatics and databases, medical professionals are increasingly inclined to use evidence-based medicine to guide their learning and work. Evidence-based medicine requires a large amount of data and literature information, where most search processes are keyword retrieval. Therefore, anticipating these key information through the model can play an important role in optimizing the query. In the past, the PLM (Pre-trained Language Model) model was mainly used for information extraction, but due to the complexity of the sequence semantic structure and task diversity, it is difficult for traditional PLM to achieve the desired effect. With the advancement of LLM (Large Language Model) technology, these issues can now be well managed. In this paper, we discuss the information extraction evaluation task CHIP-PICOS, and finally decompose it into classification and information extraction sub-problems, applying PLM and LLM respectively, and analyzing the advantages and disadvantages and differences between PLM and LLM. The results show that our framework has achieved significant performance.","Comments":"","TypeName":"","Authors":"Cao, Mengyuan, Wang, Hang, Liu, Xiaoming  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188615,"Title":"An extensive benchmark study on biomedical text generation and mining with ChatGPT","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4386530347","OldItemId":"","Abstract":"In recent years, the development of natural language process (NLP) technologies and deep learning hardware has led to significant improvement in large language models (LLMs). The ChatGPT, the state-of-the-art LLM built on GPT-3.5 and GPT-4, shows excellent capabilities in general language understanding and reasoning. Researchers also tested the GPTs on a variety of NLP-related tasks and benchmarks and got excellent results. With exciting performance on daily chat, researchers began to explore the capacity of ChatGPT on expertise that requires professional education for human and we are interested in the biomedical domain.To evaluate the performance of ChatGPT on biomedical-related tasks, this article presents a comprehensive benchmark study on the use of ChatGPT for biomedical corpus, including article abstracts, clinical trials description, biomedical questions, and so on. Typical NLP tasks like named entity recognization, relation extraction, sentence similarity, question and answering, and document classification are included. Overall, ChatGPT got a BLURB score of 58.50 while the state-of-the-art model had a score of 84.30. Through a series of experiments, we demonstrated the effectiveness and versatility of ChatGPT in biomedical text understanding, reasoning and generation, and the limitation of ChatGPT build on GPT-3.5.All the datasets are available from BLURB benchmark https://microsoft.github.io/BLURB/index.html. The prompts are described in the article.","Comments":"","TypeName":"","Authors":"Chen, Qijie, Sun, Haotong, Liu, Haoyang  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188613,"Title":"PICO to PICOS: Weak Supervision to Extend Datasets with New Labels","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4401821073","OldItemId":"","Abstract":"Hand-labelling clinical corpora can be costly and inflexible, requiring re-annotation every time new classes need to be extracted. PICO (Participant, Intervention, Comparator, Outcome) information extraction can expedite conducting systematic reviews to answer clinical questions. However, PICO frequently extends to other entities such as Study type and design, trial context, and timeframe, requiring manual re-annotation of existing corpora. In this paper, we adapt Snorkels weak supervision methodology to extend clinical corpora to new entities without extensive hand labelling. Specifically, we enrich the EBM-PICO corpus with new entities through an example of Study type and design extraction. Using weak supervision, we obtain programmatic labels on 4,081 EBM-PICO documents, achieving an F1-score of 85.02% on the test set.","Comments":"","TypeName":"","Authors":"Dhrangadhariya, Anjani, Manzo, Gaetano, Mller, Henning","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188612,"Title":"BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4384656653","OldItemId":"","Abstract":"Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a significant margin (1.5%-21.5% absolute improvement) and is able to generate augmentations that are both more factual and diverse. Code: https://github.com/Sreyan88/BioAug.","Comments":"","TypeName":"","Authors":"Ghosh, Sreyan, Tyagi, Utkarsh, Kumar, Sonal  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188611,"Title":"Biomedical Abstract Sentence Classification by BERT-Based Reading Comprehension","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4376877121","OldItemId":"","Abstract":"","Comments":"","TypeName":"","Authors":"Jiang, Chengyang, Fan, Yao-Chung","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":97,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":68,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188610,"Title":"Text classification models for assessing the completeness of randomized controlled trial publications based on CONSORT reporting guidelines","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4402620497","OldItemId":"","Abstract":"Complete and transparent reporting of randomized controlled trial publications (RCTs) is essential for assessing their credibility. We aimed to develop text classification models for determining whether RCT publications report CONSORT checklist items. Using a corpus annotated with 37 fine-grained CONSORT items, we trained sentence classification models (PubMedBERT fine-tuning, BioGPT fine-tuning, and in-context learning with GPT-4) and compared their performance. We assessed the impact of data augmentation methods (Easy Data Augmentation (EDA), UMLS-EDA, text generation and rephrasing with GPT-4) on model performance. We also fine-tuned section-specific PubMedBERT models (e.g., Methods) to evaluate whether they could improve performance compared to the single full model. We performed 5-fold cross-validation and report precision, recall, F","Comments":"","TypeName":"","Authors":"Jiang, Lan, Lan, Ming-Shong, Menke, Jan  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":63,"ItemAttributeFullTextDetails":[]},{"AttributeId":64,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188608,"Title":"Automated information extraction model enhancing traditional Chinese medicine RCT evidence extraction (Evi-BERT): algorithm development and validation","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4401622044","OldItemId":"","Abstract":"Background In the field of evidence-based medicine, randomized controlled trials (RCTs) are of critical importance for writing clinical guidelines and providing guidance to practicing physicians. Currently, RCTs rely heavily on manual extraction, but this method has data breadth limitations and is less efficient. Objectives To expand the breadth of data and improve the efficiency of obtaining clinical evidence, here, we introduce an automated information extraction model for traditional Chinese medicine (TCM) RCT evidence extraction. Methods We adopt the Evidence-Bidirectional Encoder Representation from Transformers (Evi-BERT) for automated information extraction, which is combined with rule extraction. Eleven disease types and 48,523 research articles from the China National Knowledge Infrastructure (CNKI), WanFang Data, and VIP databases were selected as the data source for extraction. We then constructed a manually annotated dataset of TCM clinical literature to train the model, including ten evidence elements and 24,244 datapoints. We chose two models, BERT-CRF and BiLSTM-CRF, as the baseline, and compared the training effects with Evi-BERT and Evi-BERT combined with rule expression (RE). Results We found that Evi-BERT combined with RE achieved the best performance (precision score = 0.926, Recall = 0.952, F1 score = 0.938) and had the best robustness. We totally summarized 113 pieces of rule datasets in the regulation extraction procedure. Our model dramatically expands the amount of data that can be searched and greatly improves efficiency without losing accuracy. Conclusion Our work provided an intelligent approach to extracting clinical evidence for TCM RCT data. Our model can help physicians reduce the time spent reading journals and rapidly speed up the screening of clinical trial evidence to help generate accurate clinical reference guidelines. Additionally, we hope the structured clinical evidence and structured knowledge extracted from this study will help other researchers build large language models in TCM.","Comments":"","TypeName":"","Authors":"Li, Yizhen, Luan, Zhongzhi, Liu, Yixing  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":66,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":98,"ItemAttributeFullTextDetails":[]},{"AttributeId":44,"ItemAttributeFullTextDetails":[]},{"AttributeId":45,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":89,"ItemAttributeFullTextDetails":[]},{"AttributeId":84,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":28,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188607,"Title":"AI-Driven Evidence Synthesis: Data Extraction of Randomized Controlled Trials with Large Language Models","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4399857258","OldItemId":"","Abstract":"","Comments":"","TypeName":"","Authors":"Liu, Jiayi, Ge, Long, Lai, Honghao  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":78,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":43,"ItemAttributeFullTextDetails":[]},{"AttributeId":45,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":69,"ItemAttributeFullTextDetails":[]},{"AttributeId":22,"ItemAttributeFullTextDetails":[]},{"AttributeId":79,"ItemAttributeFullTextDetails":[]},{"AttributeId":86,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188605,"Title":"Evaluation of SURUS: a Named Entity Recognition System to Extract Knowledge from Interventional Study Records","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4399273570","OldItemId":"","Abstract":"BACKGROUND Medical decision-making is commonly guided by systematic analysis of peer-reviewed scientific literature, published as systematic literature reviews (SLRs). These analyses are cumbersome to conduct as they require large amounts of time and subject matter expertise to be available. Automated extraction of key datapoints from clinical publications could speed up the process of systematic literature review assembly. To this end, we built, trained and validated SURUS, a named entity recognition (NER) system comprised of a Bidirectional Encoder Representations from Transformers (BERT) model trained on a highly granular dataset. The aim of this study was to assess the quality of classification of critical elements in clinical study abstracts by SURUS, in particular the patient, intervention, comparator and outcome (PICO) elements and elements of study design. DATASET &amp; METHODS The PubMedBERT-based model was trained and evaluated using a dataset of 400 interventional study abstracts, manually annotated by experts using 25 labels with a total of 39,531 annotations according to a strict annotation guideline, with Cohens  inter-annotator agreement of 0.81. We evaluated in-domain quality, and assessed out-of-domain quality of the system by testing it on out-of-domain abstracts of other disease areas and observational study types. Finally, we tested the utility of SURUS by comparing its predictions to expert-assigned PICO and study design (PICOS) classifications. RESULTS The SURUS NER system achieved an overall F1 score of 0.95, with minor deviation between labels. In addition, SURUS achieved a NER F1 of 0.90 for out-of-domain therapeutic area abstracts and 0.84 for observational study abstracts. Finally, SURUS showed considerable utility when compared to expert-assigned PICOS classifications of interventional studies, with an F1 of 0.89 and a recall of 0.96. CONCLUSION To our knowledge, with an F1 score of 0.95, SURUS ranks among the best-performing models available to date for conducting exhaustive systematic literature analyses. A strict guideline and high inter-annotation agreement resulted in high-quality in-domain medical entity of a finetuned BERT-based model, which was largely preserved during extensive out-of-domain evaluation, indicating its utility across other indication areas and study types. Current approaches in the field lack the granularity in training data and versatility demonstrated by the SURUS approach, thereby making the latter the preferred choice for automated extraction and classification tasks in the clinical literature domain. We think that this approach sets a new standard in medical literature analysis and paves the way for creating highly granular datasets of labelled entities that can be used for downstream analysis outside of traditional SLRs.","Comments":"","TypeName":"","Authors":"Peeters, Casper, Vijverberg, Koen, Pouwer,,  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188604,"Title":"Automated Mass Extraction of Over 680,000 PICOs from Clinical Study Abstracts Using Generative AI: A Proof-of-Concept Study","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4402858850","OldItemId":"","Abstract":"Generative artificial intelligence (GenAI) shows promise in automating key tasks involved in conducting systematic literature reviews (SLRs), including screening, bias assessment and data extraction. This potential automation is increasingly relevant as pharmaceutical developers face challenging requirements for timely and precise SLRs using the population, intervention, comparator and outcome (PICO) framework, such as those under the impending European Union (EU) Health Technology Assessment Regulation 2021/2282 (HTAR). This proof-of-concept study aimed to evaluate the feasibility, accuracy and efficiency of using GenAI for mass extraction of PICOs from PubMed abstracts.","Comments":"","TypeName":"","Authors":"Reason, Tim, Langham, Julia, Gimblett, Andy","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":41,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":18,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188603,"Title":"EvidenceTriangulator: A Large Language Model Approach to Synthesizing Causal Evidence across Study Designs","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4392939106","OldItemId":"","Abstract":"Abstract In managing chronic diseases, the role of social determinants like lifestyle and diet is crucial. A comprehensive strategy combining biomedical and lifestyle changes is necessary for optimal health. However, the complexity of evidence from varied study designs on lifestyle interventions poses a challenge to decision-making. To tackle this challenge, our work focused on leveraging large language model to construct a dataset primed for evidence triangulation. This approach automates the process of gathering and preparing evidence for analysis, thereby simplifying the integration of reliable insights and reducing the dependency on labor-intensive manual curation. Our approach, validated by expert evaluations, demonstrates significant utility, especially illustrated through a case study on reduced salt intake and its effect on blood pressure. This highlights the potential of leveraging large language models to enhance evidence-based decision-making in health care.","Comments":"","TypeName":"","Authors":"Shi, Xuanyu, Zhao, Wenjing, Wang, Jing  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188602,"Title":"PICO Classification Using Domain-Specific Features","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4392126278","OldItemId":"","Abstract":"This Paper proposes technique for extracting important information from clinical trial data. In order to extract important information PICO framework is used. PICO framework identifies the sentences in any given medical literature that fall under any one of the four categories: Participants/Problem (P), Intervention (I), Comparison (C), and Outcome (O). In this paper, machine learning classifiers are implemented to automatically detect PICO elements. The classification task is performed on the annotated clinical trial dataset. Instead of using a predefined word embedding like tf-idf, bag-of-word, or any other embedding domain-specific features are used as input to the machine learning classifiers. Domain-specific features were obtained using multiple entity extractors. These entity extractors extract entities related to a particular domain i.e., the biomedical domain. Extracted entities are converted into binary vectors and treated as features for the model. Our Proposed models perform better when compared to various strong baseline models.","Comments":"","TypeName":"","Authors":"Singh,,, Sharan, Aditi","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188601,"Title":"Fine-tuning large neural language models for biomedical natural language processing","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4365511667","OldItemId":"","Abstract":"Large neural language models have transformed modern natural language processing (NLP) applications. However, fine-tuning such models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for low-resource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT- BASE models, while layerwise decay is more effective for BERT- LARGE and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, domain-specific vocabulary and pretraining facilitate robust models for fine-tuning. Based on these findings, we establish a new state of the art on a wide range of biomedical NLP applications.","Comments":"","TypeName":"","Authors":"Tinn, Robert, Cheng, Hao, ,   et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188599,"Title":"Extracting the Sample Size From Randomized Controlled Trials in Explainable Fashion Using Natural Language Processing","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4400491757","OldItemId":"","Abstract":"Background Extracting the sample size from randomized controlled trials (RCTs) remains a challenge to developing better search functionalities or automating systematic reviews. Most current approaches rely on the sample size being explicitly mentioned in the abstract. Methods 847 RCTs from high-impact medical journals were tagged with six different entities that could indicate the sample size. A named entity recognition (NER) model was trained to extract the entities and then deployed on a test set of 150 RCTs. The entities performance in predicting the actual number of trial participants who were randomized was assessed and possible combinations of the entities were evaluated to create predictive models. Results The most accurate model could make predictions for 64.7% of trials in the test set, and the resulting predictions were within 10% of the ground truth in 96.9% of cases. A less strict model could make a prediction for 96.0% of trials, and its predictions were within 10% of the ground truth in 88.2% of cases. Conclusion Training a named entity recognition model to predict the sample size from randomized controlled trials is feasible, not only if the sample size is explicitly mentioned but also if the sample size can be calculated, e.g., by adding up the number of patients in each arm.","Comments":"","TypeName":"","Authors":"Windisch, Paul, Dennstdt, Fabio, Koechli, Carole  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":6,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188597,"Title":"A pipeline for medical literature search and its evaluation","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4365454127","OldItemId":"","Abstract":"One database commonly used by clinicians for searching the medical literature and practicing evidence-based medicine is PubMed. As the literature grows, it has become challenging for users to find the relevant material quickly because most of the time the relevant results are not on the top. In this article, we propose a search and ranking pipeline to improve the search results based on relevancy. We first propose an ensemble model consisting of three classifiers: bidirectional long-short-term memory conditional random field (bi-LSTM-CRF), support vector machine and naive Bayes to recognise PICO (patient, intervention, comparison, outcome) elements from abstracts. The ensemble was trained on an annotated corpus consisting of 5000 abstracts split into 4000 and 1000 training and testing data, respectively. The ensemble recorded an accuracy of 93%. We then retrieved around 927,000 articles from PubMed for the years 20172021 (access date 16 April 2021). For every abstract, we extracted and grouped all P, I and O terms, and stored these groups along with the article ID in a separate database. During the search, every P, I and O term of the query is searched only in its corresponding group of every abstract. The scoring method simply counts the number of matches between the querys P, I and O elements and the words in P, I and O groups, respectively. The abstracts are sorted by the number of matches and the top five abstracts are listed using their pre-stored abstract IDs. A comprehensive user study was conducted where 60 different queries were formulated and used to generate ranked search results using both PubMed and our proposed model. Five medical professionals assessed the ranked search results and marked every item as relevant or non-relevant. Both models were compared using precision@K and mean-average-precision@K metrics where K is 5. For most of the queries, our model produced higher precision@K values than PubMed. The mean-average-precision@K value of our model is also higher than PubMed (0.83 versus 0.67).","Comments":"","TypeName":"","Authors":"Zafar, Imamah, Wali, Aamir, Kunwar,,  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":15188596,"Title":"Task-Specific Model Allocation Medical Papers PICOS Information Extraction","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://openalex.org/works/W4392951372","OldItemId":"","Abstract":"In the field of medical research, extracting PICOS information which includes population, intervention, Comparison, outcomes, and study design from medical papers, plays a significant role in improving search efficiency and guiding clinical practice. Based on the PICOS key information extraction task released from the China Health Information Processing Conference (CHIP 2023), we proposed a method based on Task-Specific Model Allocation, which selects different models according to the characteristics of the abstract and title data. Additionally, incorporating techniques such as multi-task learning and model fusion. Our method achieved first place on the leaderboard with an F1 score of 0.78 on List A and 0.81 on List B.","Comments":"","TypeName":"","Authors":"Zhang, Qi, Qu, Jing, Zhao, Qingbo  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800913,"Title":"Boundary-aware Dual Biaffine Model for Sequential Sentence Classification in Biomedical Documents","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1109/tcbb.2024.3376566","OldItemId":"","Abstract":"Assigning appropriate rhetorical roles, such as background, intervention, and outcome, to sentences in biomedical documents can streamline the process for physicians to locate evidence and resources for medical treatment and decision-making. While sequence labeling and span-based methods are frequently employed for this task, the former disregards a document's semantic structure, resulting in a lack of semantic coherence across continuous sentences. Span-based approaches, on the other hand, either necessitate the enumeration of all potential spans, which can be time-consuming, or may lead to the misclassification of sentences over extended spans. Consequently, an approach is required that models the semantic structure of documents explicitly and captures boundary information to achieve precise and effective sentence labeling in biomedical documents. To address these challenges, we propose a new approach, the boundary-aware dual biaffine model, which explicitly models the semantic structure of documents and incorporates boundary information via a dual biaffine layer. We introduce a dynamic programming algorithm to minimize missing labels and overlapping predictions, and achieve globally optimal decoding results. We evaluate our approach on three benchmark datasets, namely PubMed 20k RCT, PubMed-PICO and NICTA-PIBOSO. The experimental results demonstrate that our approach outperforms strong baselines and achieves state-of-the-art performance on PubMed 20k RCT and PubMed-PICO. Additionally, our method also achieves competitive results on NICTA-PIBOSO. Availability: Our codes and data will be available at: https://github.com/CSU-NLP-Group/Sequential-Sentence-Classification.","Comments":"","TypeName":"","Authors":"Duan, J., Guo, H., Jiang, H.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800911,"Title":"AlpaPICO: Extraction of PICO frames from clinical trial documents using LLMs","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1016/j.ymeth.2024.04.005","OldItemId":"","Abstract":"In recent years, there has been a surge in the publication of clinical trial reports, making it challenging to conduct systematic reviews. Automatically extracting Population, Intervention, Comparator, and Outcome (PICO) from clinical trial studies can alleviate the traditionally time-consuming process of manually scrutinizing systematic reviews. Existing approaches of PICO frame extraction involves supervised approach that relies on the existence of manually annotated data points in the form of BIO label tagging. Recent approaches, such as In-Context Learning (ICL), which has been shown to be effective for a number of downstream NLP tasks, require the use of labeled examples. In this work, we adopt ICL strategy by employing the pretrained knowledge of Large Language Models (LLMs), gathered during the pretraining phase of an LLM, to automatically extract the PICO-related terminologies from clinical trial documents in unsupervised set up to bypass the availability of large number of annotated data instances. Additionally, to showcase the highest effectiveness of LLM in oracle scenario where large number of annotated samples are available, we adopt the instruction tuning strategy by employing Low Rank Adaptation (LORA) to conduct the training of gigantic model in low resource environment for the PICO frame extraction task. More specifically, both of the proposed frameworks utilize AlpaCare as base LLM which employs both few-shot in-context learning and instruction tuning techniques to extract PICO-related terms from the clinical trial reports. We applied these approaches to the widely used coarse-grained datasets such as EBM-NLP, EBM-COMET and fine-grained datasets such as EBM-NLP(rev) and EBM-NLP(h). Our empirical results show that our proposed ICL-based framework produces comparable results on all the version of EBM-NLP datasets and the proposed instruction tuned version of our framework produces state-of-the-art results on all the different EBM-NLP datasets. Our project is available at https://github.com/shrimonmuke0202/AlpaPICO.git.","Comments":"","TypeName":"","Authors":"Ghosh, M., Mukherjee, S., Ganguly, A.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":89,"ItemAttributeFullTextDetails":[]},{"AttributeId":63,"ItemAttributeFullTextDetails":[]},{"AttributeId":84,"ItemAttributeFullTextDetails":[]},{"AttributeId":64,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":97,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800908,"Title":"Methodological information extraction from randomized controlled trial publications: a pilot study","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://pubmed.ncbi.nlm.nih.gov/37128457/","OldItemId":"","Abstract":"Most biomedical information extraction (IE) approaches focus on entity types such as diseases, drugs, and genes, and relations such as gene-disease associations. In this paper, we introduce the task of methodological IE to support fine-grained quality assessment of randomized controlled trial (RCT) publications. We draw from the Ontology of Clinical Research (OCRe) and the CONSORT reporting guidelines for RCTs to create a categorization of relevant methodological characteristics. In a pilot annotation study, we annotate a corpus of 70 full-text publications with these characteristics. We also train baseline named entity recognition (NER) models to recognize these items in RCT publications using several training sets with different negative sampling strategies. We evaluate the models at span and document levels. Our results show that it is feasible to use natural language processing (NLP) and machine learning for fine-grained extraction of methodological information. We propose that our models, after improvements, can support assessment of methodological quality in RCT publications. Our annotated corpus, models, and code are publicly available at https://github.com/kellyhoang0610/RCTMethodologyIE.","Comments":"","TypeName":"","Authors":"Hoang, L., Guan, Y., Kilicoglu, H.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":99,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800907,"Title":"Towards precise PICO extraction from abstracts of randomized controlled trials using a section-specific learning approach","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1093/bioinformatics/btad542","OldItemId":"","Abstract":"MOTIVATION: Automated extraction of participants, intervention, comparison/control, and outcome (PICO) from the randomized controlled trial (RCT) abstracts is important for evidence synthesis. Previous studies have demonstrated the feasibility of applying natural language processing (NLP) for PICO extraction. However, the performance is not optimal due to the complexity of PICO information in RCT abstracts and the challenges involved in their annotation. RESULTS: We propose a two-step NLP pipeline to extract PICO elements from RCT abstracts: (i) sentence classification using a prompt-based learning model and (ii) PICO extraction using a named entity recognition (NER) model. First, the sentences in abstracts were categorized into four sections namely background, methods, results, and conclusions. Next, the NER model was applied to extract the PICO elements from the sentences within the title and methods sections that include &gt;96% of PICO information. We evaluated our proposed NLP pipeline on three datasets, the EBM-NLPmoddataset, a randomly selected and reannotated dataset of 500 RCT abstracts from the EBM-NLP corpus, a dataset of 150 COVID-19 RCT abstracts, and a dataset of 150 Alzheimer's disease (AD) RCT abstracts. The end-to-end evaluation reveals that our proposed approach achieved an overall micro F1 score of 0.833 on the EBM-NLPmod dataset, 0.928 on the COVID-19 dataset, and 0.899 on the AD dataset when measured at the token-level and an overall micro F1 score of 0.712 on EBM-NLPmod dataset, 0.850 on the COVID-19 dataset, and 0.805 on the AD dataset when measured at the entity-level. AVAILABILITY: Our codes and datasets are publicly available at https://github.com/BIDS-Xu-Lab/section_specific_annotation_of_PICO. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.","Comments":"","TypeName":"","Authors":"Hu, Y., Keloth, V. K., Raja, K.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800904,"Title":"EvidenceMap: a three-level knowledge representation for medical evidence computation and comprehension","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1093/jamia/ocad036","OldItemId":"","Abstract":"OBJECTIVE: To develop a computable representation for medical evidence and to contribute a gold standard dataset of annotated randomized controlled trial (RCT) abstracts, along with a natural language processing (NLP) pipeline for transforming free-text RCT evidence in PubMed into the structured representation. MATERIALS AND METHODS: Our representation, EvidenceMap, consists of 3 levels of abstraction: Medical Evidence Entity, Proposition and Map, to represent the hierarchical structure of medical evidence composition. Randomly selected RCT abstracts were annotated following EvidenceMap based on the consensus of 2 independent annotators to train an NLP pipeline. Via a user study, we measured how the EvidenceMap improved evidence comprehension and analyzed its representative capacity by comparing the evidence annotation with EvidenceMap representation and without following any specific guidelines. RESULTS: Two corpora including 229 disease-agnostic and 80 COVID-19 RCT abstracts were annotated, yielding 12 725 entities and 1602 propositions. EvidenceMap saves users 51.9% of the time compared to reading raw-text abstracts. Most evidence elements identified during the freeform annotation were successfully represented by EvidenceMap, and users gave the enrollment, study design, and study Results sections mean 5-scale Likert ratings of 4.85, 4.70, and 4.20, respectively. The end-to-end evaluations of the pipeline show that the evidence proposition formulation achieves F1 scores of 0.84 and 0.86 in the adjusted random index score. CONCLUSIONS: EvidenceMap extends the participant, intervention, comparator, and outcome framework into 3 levels of abstraction for transforming free-text evidence from the clinical literature into a computable structure. It can be used as an interoperable format for better evidence retrieval and synthesis and an interpretable representation to efficiently comprehend RCT findings.","Comments":"","TypeName":"","Authors":"Kang, T., Sun, Y., Kim, J. H.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800898,"Title":"SEETrials: Leveraging Large Language Models for Safety and Efficacy Extraction in Oncology Clinical Trials","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1101/2024.01.18.24301502","OldItemId":"","Abstract":"BACKGROUND: Initial insights into oncology clinical trial outcomes are often gleaned manually from conference abstracts. We aimed to develop an automated system to extract safety and efficacy information from study abstracts with high precision and fine granularity, transforming them into computable data for timely clinical decision-making. METHODS: We collected clinical trial abstracts from key conferences and PubMed (2012-2023). The SEETrials system was developed with four modules: preprocessing, prompt modeling, knowledge ingestion and postprocessing. We evaluated the system's performance qualitatively and quantitatively and assessed its generalizability across different cancer types- multiple myeloma (MM), breast, lung, lymphoma, and leukemia. Furthermore, the efficacy and safety of innovative therapies, including CAR-T, bispecific antibodies, and antibody-drug conjugates (ADC), in MM were analyzed across a large scale of clinical trial studies. RESULTS: SEETrials achieved high precision (0.958), recall (sensitivity) (0.944), and F1 score (0.951) across 70 data elements present in the MM trial studies Generalizability tests on four additional cancers yielded precision, recall, and F1 scores within the 0.966-0.986 range. Variation in the distribution of safety and efficacy-related entities was observed across diverse therapies, with certain adverse events more common in specific treatments. Comparative performance analysis using overall response rate (ORR) and complete response (CR) highlighted differences among therapies: CAR-T (ORR: 88%, 95% CI: 84-92%; CR: 95%, 95% CI: 53-66%), bispecific antibodies (ORR: 64%, 95% CI: 55-73%; CR: 27%, 95% CI: 16-37%), and ADC (ORR: 51%, 95% CI: 37-65%; CR: 26%, 95% CI: 1-51%). Notable study heterogeneity was identified (&gt;75% I (2) heterogeneity index scores) across several outcome entities analyzed within therapy subgroups. CONCLUSION: SEETrials demonstrated highly accurate data extraction and versatility across different therapeutics and various cancer domains. Its automated processing of large datasets facilitates nuanced data comparisons, promoting the swift and effective dissemination of clinical insights.","Comments":"","TypeName":"","Authors":"Lee, K., Paek, H., Huang, L. C.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":19,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":88,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800896,"Title":"A Sample Size Extractor for RCT Reports","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2021","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.3233/shti220151","OldItemId":"","Abstract":"Sample size is an important indicator of the power of randomized controlled trials (RCTs). In this paper, we designed a total sample size extractor using a combination of syntactic and machine learning methods, and evaluated it on 300 Covid-19 abstracts (Covid-Set) and 100 generic RCT abstracts (General-Set). To improve the performance, we applied transfer learning from a large public corpus of annotated abstracts. We achieved an average F1 score of 0.73 on the Covid-Set testing set, and 0.60 on the General-Set using exact matches. The F1 scores for loose matches on both datasets were over 0.74. Compared with the state-of-the-art tool, our extractor reports total sample sizes directly and improved F1 scores by at least 4% without transfer learning. We demonstrated that transfer learning improved the sample size extraction accuracy and minimized human labor on annotations.","Comments":"","TypeName":"","Authors":"Lin, F., Liu, H., Moon, P.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800894,"Title":"Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1186/s13643-023-02351-w","OldItemId":"","Abstract":"BACKGROUND: Evidence-based medicine requires synthesis of research through rigorous and time-intensive systematic literature reviews (SLRs), with significant resource expenditure for data extraction from scientific publications. Machine learning may enable the timely completion of SLRs and reduce errors by automating data identification and extraction. METHODS: We evaluated the use of machine learning to extract data from publications related to SLRs in oncology (SLR 1) and Fabry disease (SLR 2). SLR 1 predominantly contained interventional studies and SLR 2 observational studies. Predefined key terms and data were manually annotated to train and test bidirectional encoder representations from transformers (BERT) and bidirectional long-short-term memory machine learning models. Using human annotation as a reference, we assessed the ability of the models to identify biomedical terms of interest (entities) and their relations. We also pretrained BERT on a corpus of 100,000 open access clinical publications and/or enhanced context-dependent entity classification with a conditional random field (CRF) model. Performance was measured using the F(1) score, a metric that combines precision and recall. We defined successful matches as partial overlap of entities of the same type. RESULTS: For entity recognition, the pretrained BERT+CRF model had the best performance, with an F(1) score of 73% in SLR 1 and 70% in SLR 2. Entity types identified with the highest accuracy were metrics for progression-free survival (SLR 1, F(1) score 88%) or for patient age (SLR 2, F(1) score 82%). Treatment arm dosage was identified less successfully (F(1) scores 60% [SLR 1] and 49% [SLR 2]). The best-performing model for relation extraction, pretrained BERT relation classification, exhibited F(1) scores higher than 90% in cases with at least 80 relation examples for a pair of related entity types. CONCLUSIONS: The performance of BERT is enhanced by pretraining with biomedical literature and by combining with a CRF model. With refinement, machine learning may assist with manual data extraction for SLRs.","Comments":"","TypeName":"","Authors":"Panayi, A., Ward, K., Benhadji-Schaff, A.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":63,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800893,"Title":"Artificial Intelligence to Automate Network Meta-Analyses: Four Case Studies to Evaluate the Potential Application of Large Language Models","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1007/s41669-024-00476-9","OldItemId":"","Abstract":"BACKGROUND: The emergence of artificial intelligence, capable of human-level performance on some tasks, presents an opportunity to revolutionise development of systematic reviews and network meta-analyses (NMAs). In this pilot study, we aim to assess use of a large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4]) to automatically extract data from publications, write an R script to conduct an NMA and interpret the results. METHODS: We considered four case studies involving binary and time-to-event outcomes in two disease areas, for which an NMA had previously been conducted manually. For each case study, a Python script was developed that communicated with the LLM via application programming interface (API) calls. The LLM was prompted to extract relevant data from publications, to create an R script to be used to run the NMA and then to produce a small report describing the analysis. RESULTS: The LLM had a &gt; 99% success rate of accurately extracting data across 20 runs for each case study and could generate R scripts that could be run end-to-end without human input. It also produced good quality reports describing the disease area, analysis conducted, results obtained and a correct interpretation of the results. CONCLUSIONS: This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error. This is provided that routine technical checks are performed, as recommend for human-conducted analyses. Whilst not currently 100% consistent, LLMs are likely to improve with time.","Comments":"","TypeName":"","Authors":"Reason, T., Benbow, E., Langham, J.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":48,"ItemAttributeFullTextDetails":[]},{"AttributeId":28,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":40,"ItemAttributeFullTextDetails":[]},{"AttributeId":41,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800892,"Title":"Comparing generative and extractive approaches to information extraction from abstracts describing randomized clinical trials","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1186/s13326-024-00305-2","OldItemId":"","Abstract":"BACKGROUND: Systematic reviews of Randomized Controlled Trials (RCTs) are an important part of the evidence-based medicine paradigm. However, the creation of such systematic reviews by clinical experts is costly as well as time-consuming, and results can get quickly outdated after publication. Most RCTs are structured based on the Patient, Intervention, Comparison, Outcomes (PICO) framework and there exist many approaches which aim to extract PICO elements automatically. The automatic extraction of PICO information from RCTs has the potential to significantly speed up the creation process of systematic reviews and this way also benefit the field of evidence-based medicine. RESULTS: Previous work has addressed the extraction of PICO elements as the task of identifying relevant text spans or sentences, but without populating a structured representation of a trial. In contrast, in this work, we treat PICO elements as structured templates with slots to do justice to the complex nature of the information they represent. We present two different approaches to extract this structured information from the abstracts of RCTs. The first approach is an extractive approach based on our previous work that is extended to capture full document representations as well as by a clustering step to infer the number of instances of each template type. The second approach is a generative approach based on a seq2seq model that encodes the abstract describing the RCT and uses a decoder to infer a structured representation of a trial including its arms, treatments, endpoints and outcomes. Both approaches are evaluated with different base models on a manually annotated dataset consisting of RCT abstracts on an existing dataset comprising 211 annotated clinical trial abstracts for Type 2 Diabetes and Glaucoma. For both diseases, the extractive approach (with flan-t5-base) reached the best F1 score, i.e. 0.547 ( 0.006 ) for type 2 diabetes and 0.636 ( 0.006 ) for glaucoma. Generally, the F1 scores were higher for glaucoma than for type 2 diabetes and the standard deviation was higher for the generative approach. CONCLUSION: In our experiments, both approaches show promising performance extracting structured PICO information from RCTs, especially considering that most related work focuses on the far easier task of predicting less structured objects. In our experimental results, the extractive approach performs best in both cases, although the lead is greater for glaucoma than for type 2 diabetes. For future work, it remains to be investigated how the base model size affects the performance of both approaches in comparison. Although the extractive approach currently leaves more room for direct improvements, the generative approach might benefit from larger models.","Comments":"","TypeName":"","Authors":"Witte, C., Schmidt, D. M., Cimiano, P.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":100,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800884,"Title":"Zero-shot information extraction for clinical meta-analysis using large language models","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://aclanthology.org/2023.bionlp-1.37","OldItemId":"","Abstract":"Meta-analysis of randomized clinical trials (rcts) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. this study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (rcts) at scale. we perform a detailed comparison of the performance of these models in zero-shot prompt-based information extraction from a diverse set of rcts to traditional manual annotation methods. we analyze the results for two different meta-analyses aimed at drug repurposing in cancer therapy pharmacovigilience in chronic myeloid leukemia. our findings reveal that the best model for the two demonstrated tasks, chatgpt can generally extract correct information and identify when the desired information is missing from an article. we additionally conduct a systematic error analysis, documenting the prevalence of diverse error types encountered during the process of prompt-based information extraction.","Comments":"","TypeName":"","Authors":"Kartchner, David, Ramalingam, Selvi, Al-Hussaini, Irfan  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":39,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":41,"ItemAttributeFullTextDetails":[]},{"AttributeId":28,"ItemAttributeFullTextDetails":[]},{"AttributeId":40,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":48,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":25,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800882,"Title":"Intra-template entity compatibility based slot-filling for clinical trial information extraction","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://aclanthology.org/2022.bionlp-1.18","OldItemId":"","Abstract":"We present a deep learning based information extraction system that can extract the design and results of a published abstract describing a randomized controlled trial (rct). in contrast to other approaches, our system does not regard the pico elements as flat objects or labels but as structured objects. we thus model the task as the one of filling a set of templates and slots; our two-step approach recognizes relevant slot candidates as a first step and assigns them to a corresponding template as second step, relying on a learned pairwise scoring function that models the compatibility of the different slot values. we evaluate the approach on a dataset of 211 manually annotated abstracts for type 2 diabetes and glaucoma, showing the positive impact of modelling intra-template entity compatibility. as main benefit, our approach yields a structured object for every rct abstract that supports the aggregation and summarization of clinical trial results across published studies and can facilitate the task of creating a systematic review or meta-analysis.","Comments":"","TypeName":"","Authors":"Witte, Christian, Cimiano, Philipp","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":91,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":93,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800881,"Title":"Distant-cto: a zero cost, distantly supervised approach to improve low-resource entity extraction using clinical trials literature","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2022","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://aclanthology.org/2022.bionlp-1.34","OldItemId":"","Abstract":"Pico recognition is an information extraction task for identifying participant, intervention, comparator, and outcome information from clinical literature. manually identifying pico information is the most time-consuming step for conducting systematic reviews (sr), which is already labor-intensive. a lack of diversified and large, annotated corpora restricts innovation and adoption of automated pico recognition systems. the largest-available pico entity/span corpus is manually annotated which is too expensive for a majority of the scientific community. to break through the bottleneck, we propose distant-cto, a novel distantly supervised pico entity extraction approach using the clinical trials literature, to generate a massive weakly-labeled dataset with more than a million `intervention' and `comparator' entity annotations. we train distant ner (named-entity recognition) models using this weakly-labeled dataset and demonstrate that it outperforms even the sophisticated models trained on the manually annotated dataset with a 2% f1 improvement over the intervention entity of the pico benchmark and more than 5% improvement when combined with the manually annotated dataset. we investigate the generalizability of our approach and gain an impressive f1 score on another domain-specific pico benchmark. the approach is not only zero-cost but is also scalable for a constant stream of pico entity annotations.","Comments":"","TypeName":"","Authors":"Dhrangadhariya, Anjani, M\"uller, Henning","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":4,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":86,"ItemAttributeFullTextDetails":[]},{"AttributeId":79,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":22,"ItemAttributeFullTextDetails":[]},{"AttributeId":69,"ItemAttributeFullTextDetails":[]},{"AttributeId":21,"ItemAttributeFullTextDetails":[]},{"AttributeId":20,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800880,"Title":"Sectioning of biomedical abstracts: a sequence of sequence classification task","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2201.07112","OldItemId":"","Abstract":"Rapid growth of the biomedical literature has led to many advances in the biomedical text mining field. among the vast amount of information, biomedical article abstracts are the easily accessible sources. however, the number of the structured abstracts, describing the rhetorical sections with one of background, objective, method, result and conclusion categories is still not considerable. exploration of valuable information in the biomedical abstracts can be expedited with the improvements in the sequential sentence classification task. deep learning based models has great performance/potential in achieving significant results in this task. however, they can often be overly complex and overfit to specific data. in this project, we study a state-of-the-art deep learning model, which we called ssn-4 model here. we investigate different components of the ssn-4 model to study the trade-off between the performance and complexity. we explore how well this model generalizes to a new data set beyond randomized controlled trials (rct) dataset. we address the question that whether word embeddings can be adjusted to the task to improve the performance. furthermore, we develop a second model that addresses the confusion pairs in the first model. results show that ssn-4 model does not appear to generalize well beyond rct dataset.","Comments":"","TypeName":"","Authors":"Karabulut, Mehmet Efruz, Vijay-Shanker, K.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800878,"Title":"Jointly extracting interventions, outcomes, and findings from rct reports with llms","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2305.03642","OldItemId":"","Abstract":"Results from randomized controlled trials (rcts) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. however, results from rcts are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. this onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. in this work we propose and evaluate a text-to-text model built on instruction-tuned large language models (llms) to jointly extract interventions, outcomes, and comparators (ico elements) from clinical abstracts, and infer the associated results reported. manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning llms for this purpose realizes considerable ($\\sim$20 point absolute f1 score) gains over the previous sota. we perform ablations and error analyses to assess aspects that contribute to model performance, and to highlight potential directions for further improvements. we apply our model to a collection of published rcts through mid-2022, and release a searchable database of structured findings: http://ico-relations.ebm-nlp.com","Comments":"","TypeName":"","Authors":"Wadhwa, Somin, DeYoung, Jay, Nye, Benjamin  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":87,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":99,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800873,"Title":"A span-based model for extracting overlapping pico entities from rct publications","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2401.06791","OldItemId":"","Abstract":"Objectives extraction of pico (populations, interventions, comparison, and outcomes) entities is fundamental to evidence retrieval. we present a novel method picox to extract overlapping pico entities. materials and methods picox first identifies entities by assessing whether a word marks the beginning or conclusion of an entity. then it uses a multi-label classifier to assign one or more pico labels to a span candidate. picox was evaluated using one of the best-performing baselines, ebm-nlp, and three more datasets, i.e., pico-corpus, and rct publications on alzheimer's disease or covid-19, using entity-level precision, recall, and f1 scores. results picox achieved superior precision, recall, and f1 scores across the board, with the micro f1 score improving from 45.05 to 50.87 (p &lt;&lt; 0.01). on the pico-corpus, picox obtained higher recall and f1 scores than the baseline and improved the micro recall score from 56.66 to 67.33. on the covid-19 dataset, picox also outperformed the baseline and improved the micro f1 score from 77.10 to 80.32. on the ad dataset, picox demonstrated comparable f1 scores with higher precision when compared to the baseline. conclusion picox excels in identifying overlapping entities and consistently surpasses a leading baseline across multiple datasets. ablation studies reveal that its data augmentation strategy effectively minimizes false positives and improves precision.","Comments":"","TypeName":"","Authors":"Zhang, Gongbo, Zhou, Yiliang, Hu, Yan  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":7,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":62,"ItemAttributeFullTextDetails":[]},{"AttributeId":56,"ItemAttributeFullTextDetails":[]},{"AttributeId":36,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800872,"Title":"Lstm-based deep neural network with a focus on sentence representation for sequential sentence classification in medical scientific abstracts","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2401.15854","OldItemId":"","Abstract":"The sequential sentence classification task within the domain of medical abstracts, termed as ssc, involves the categorization of sentences into pre-defined headings based on their roles in conveying critical information in the abstract. in the ssc task, sentences are sequentially related to each other. for this reason, the role of sentence embeddings is crucial for capturing both the semantic information between words in the sentence and the contextual relationship of sentences within the abstract, which then enhances the ssc system performance. in this paper, we propose a lstm-based deep learning network with a focus on creating comprehensive sentence representation at the sentence level. to demonstrate the efficacy of the created sentence representation, a system utilizing these sentence embeddings is also developed, which consists of a convolutional-recurrent neural network (c-rnn) at the abstract level and a multi-layer perception network (mlp) at the segment level. our proposed system yields highly competitive results compared to state-of-the-art systems and further enhances the f1 scores of the baseline by 1.0%, 2.8%, and 2.6% on the benchmark datasets pudmed 200k rct, pudmed 20k rct and nicta-piboso, respectively. this indicates the significant impact of improving sentence representation on boosting model performance.","Comments":"","TypeName":"","Authors":"Lam, Phat, Pham, Lam, Nguyen, Tin  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":101,"ItemAttributeFullTextDetails":[]},{"AttributeId":100,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800871,"Title":"Factpico: factuality evaluation for plain language summarization of medical evidence","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2402.11456","OldItemId":"","Abstract":"Plain language summarization with llms can be useful for improving textual accessibility of technical content. but how factual are these summaries in a high-stakes domain like medicine? this paper presents factpico, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (rcts), which are the basis of evidence-based medicine and can directly inform patient treatment. factpico consists of 345 plain language summaries of rct abstracts generated from three llms (i.e., gpt-4, llama-2, and alpaca), with fine-grained evaluation and natural language rationales from experts. we assess the factuality of critical elements of rcts in those summaries: populations, interventions, comparators, outcomes (pico), as well as the reported findings concerning these. we also evaluate the correctness of the extra information (e.g., explanations) added by llms. using factpico, we benchmark a range of existing factuality metrics, including the newly devised ones based on llms. we find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.","Comments":"","TypeName":"","Authors":"Joseph, Sebastian Antony, Chen, Lily, Trienes, Jan  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":60,"ItemAttributeFullTextDetails":[]},{"AttributeId":49,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":97,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":50,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800869,"Title":"Automatically extracting numerical results from randomized controlled trials with large language models","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2405.01686","OldItemId":"","Abstract":"Meta-analyses statistically aggregate the findings of different randomized controlled trials (rcts) to assess treatment effectiveness. because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. however, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. ideally, language technologies would permit fully automatic meta-analysis, on demand. this requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (nlp) models to date. in this work, we evaluate whether modern large language models (llms) can reliably perform this task. we annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. using this dataset, we evaluate the performance of seven llms applied zero-shot for the task of conditionally extracting numerical findings from trial reports. we find that massive llms that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). however, llms -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. this work charts a path toward fully automatic meta-analysis of rcts via llms, while also highlighting the limitations of existing models for this aim.","Comments":"","TypeName":"","Authors":"Yun, Hye Sun, Pogrebitskiy, David, Marshall, Iain J.  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":17,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":26,"ItemAttributeFullTextDetails":[]},{"AttributeId":30,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":102,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":100,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800868,"Title":"Exploring the use of a large language model for data extraction in systematic reviews: a rapid feasibility study","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2405.14445","OldItemId":"","Abstract":"This paper describes a rapid feasibility study of using gpt-4, a large language model (llm), to (semi)automate data extraction in systematic reviews. despite the recent surge of interest in llms there is still a lack of understanding of how to design llm-based automation tools and how to robustly evaluate their performance. during the 2023 evidence synthesis hackathon we conducted two feasibility studies. firstly, to automatically extract study characteristics from human clinical, animal, and social science domain studies. we used two studies from each category for prompt-development; and ten for evaluation. secondly, we used the llm to predict participants, interventions, controls and outcomes (picos) labelled within 100 abstracts in the ebm-nlp dataset. overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences). causal inference methods and study design were the data extraction items with the most errors. in the pico study, participants and intervention/control showed high accuracy (&gt;80%), outcomes were more challenging. evaluation was done manually; scoring methods such as bleu and rouge showed limited value. we observed variability in the llms predictions and changes in response quality. this paper presents a template for future evaluations of llms in the context of data extraction for systematic review automation. our results show that there might be value in using llms, for example as second or third reviewers. however, caution is advised when integrating models such as gpt-4 into tools. further research on stability and reliability in practical settings is warranted for each type of data that is processed by the llm.","Comments":"","TypeName":"","Authors":"Schmidt, Lena, Hair, Kaitlyn, Graziozi, Sergio  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":65,"ItemAttributeFullTextDetails":[]},{"AttributeId":96,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":31,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800864,"Title":"Accelerating clinical evidence synthesis with large language models","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://export.arxiv.org/abs/2406.17755","OldItemId":"","Abstract":"Automatic medical discovery by ai is a dream of many. one step toward that goal is to create an ai model to understand clinical studies and synthesize clinical evidence from the literature. clinical evidence synthesis currently relies on systematic reviews of clinical trials and retrospective analyses from medical literature. however, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating evidence. we introduce trialmind, a generative ai-based pipeline for conducting medical systematic reviews, encompassing study search, screening, and data extraction phases. we utilize large language models (llms) to drive each pipeline component while incorporating human expert oversight to minimize errors. to facilitate evaluation, we also create a benchmark dataset trialreviewbench, a custom dataset with 870 annotated clinical studies from 25 meta-analysis papers across various medical treatments. our results demonstrate that trialmind significantly improves the literature review process, achieving high recall rates (0.897-1.000) in study searching from over 20 million pubmed studies and outperforming traditional language model embeddings-based methods in screening (recall@20 of 0.227-0.246 vs. 0.000-0.102). furthermore, our approach surpasses direct gpt-4 performance in result extraction, with accuracy ranging from 0.65 to 0.84. we also support clinical evidence synthesis in forest plots, as validated by eight human annotators who preferred trialmind over the gpt-4 baseline with a winning rate of 62.5%-100% across the involved reviews. our findings suggest that an llm-based clinical evidence synthesis approach, such as trialmind, can enable reliable and high-quality clinical evidence synthesis to improve clinical research efficiency.","Comments":"","TypeName":"","Authors":"Wang, Zifeng, Cao, Lang, Danek, Benjamin  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":38,"ItemAttributeFullTextDetails":[]},{"AttributeId":61,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":74,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":11,"ItemAttributeFullTextDetails":[]},{"AttributeId":13,"ItemAttributeFullTextDetails":[]},{"AttributeId":12,"ItemAttributeFullTextDetails":[]},{"AttributeId":15,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":94,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800861,"Title":"Automated clinical knowledge graph generation framework for evidence based medicine.","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2023","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1016/j.eswa.2023.120964","OldItemId":"","Abstract":"","Comments":"","TypeName":"","Authors":"Fakhare, Alam, Hamed,,, Khalid,,","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""},{"Codes":[{"AttributeId":33,"ItemAttributeFullTextDetails":[]},{"AttributeId":34,"ItemAttributeFullTextDetails":[]},{"AttributeId":42,"ItemAttributeFullTextDetails":[]},{"AttributeId":35,"ItemAttributeFullTextDetails":[]},{"AttributeId":1,"ItemAttributeFullTextDetails":[]},{"AttributeId":2,"ItemAttributeFullTextDetails":[]},{"AttributeId":3,"ItemAttributeFullTextDetails":[]},{"AttributeId":75,"ItemAttributeFullTextDetails":[]},{"AttributeId":5,"ItemAttributeFullTextDetails":[]},{"AttributeId":57,"ItemAttributeFullTextDetails":[]},{"AttributeId":53,"ItemAttributeFullTextDetails":[]},{"AttributeId":8,"ItemAttributeFullTextDetails":[]},{"AttributeId":10,"ItemAttributeFullTextDetails":[]},{"AttributeId":9,"ItemAttributeFullTextDetails":[]},{"AttributeId":23,"ItemAttributeFullTextDetails":[]},{"AttributeId":14,"ItemAttributeFullTextDetails":[]},{"AttributeId":95,"ItemAttributeFullTextDetails":[]},{"AttributeId":2033,"ItemAttributeFullTextDetails":[]}],"Outcomes":[],"ItemId":14800860,"Title":"Blinktextsubscriptlstm: biolinkbert and lstm based approach for extraction of pico frame from clinical trial text.","ParentTitle":"","ShortTitle":"","DateCreated":"10/10/2024","CreatedBy":"LS","DateEdited":"","EditedBy":"","Year":"2024","Month":"","StandardNumber":"","City":"","Country":"","Publisher":"","Institution":"","Volume":"","Pages":"","Edition":"","Issue":"","Availability":"","URL":"https://doi.org/10.1145/3632410.3632442","OldItemId":"","Abstract":"","Comments":"","TypeName":"","Authors":"Madhusudan, Ghosh, Shrimon, Mukherjee, Payel, Santra  et al.","ParentAuthors":"","DOI":"","Keywords":"","ItemStatus":"","ItemStatusTooltip":"","QuickCitation":""}];
  const summaryAttribute = "";
  const checkboxCheckedSvg = '<svg id="checked" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M0 0h24v24H0z" fill="none"/>' +
    '<path d="M19 3H5c-1.11 0-2 .9-2 2v14c0 1.1.89 2 2 2h14c1.11 0 2-.9 2-2V5c0-1.1-.89-2-2-2zm-9 14l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>' +
    '</svg>';
  const checkboxUncheckedSvg = '<svg id="unchecked" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M19 5v14H5V5h14m0-2H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2z"/>' +
    '<path d="M0 0h24v24H0z" fill="none"/>' +
    '</svg>';
  const checkboxIndeterminateSvg = '<svg id="indeterminate" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">' +
    '<defs>' +
    '<path d="M0 0h24v24H0z" id="a"/>' +
    '</defs>' +
    '<clipPath id="b">' +
    '<use overflow="visible" xlink:href="#a"/>' +
    '</clipPath>' +
    '<path clip-path="url(#b)" d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-2 10H7v-2h10v2z"/>' +
    '</svg>';
  const radioCheckedSvg = '<svg id="checked" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M12 7c-2.76 0-5 2.24-5 5s2.24 5 5 5 5-2.24 5-5-2.24-5-5-5zm0-5C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8-3.58 8-8 8z"/>' +
    '<path d="M0 0h24v24H0z" fill="none"/>' +
    '</svg>';
  const radioUncheckedSvg = '<svg id="unchecked" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8-3.58 8-8 8z"/>' +
    '<path d="M0 0h24v24H0z" fill="none"/>' +
    '</svg>';
  const arrowUpSvg = '<svg class="btnRowCollapse arrowUp" id="arrowUp" fill="#000000" width="18" height="18" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M20.24 18.804c0 0.116-0.058 0.247-0.145 0.334l-0.725 0.725c-0.087 0.087-0.203 0.145-0.334 0.145-0.116 0-0.247-0.058-0.334-0.145l-5.702-5.702-5.702 5.702c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-0.725-0.725c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l6.761-6.761c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l6.761 6.761c0.087 0.087 0.145 0.218 0.145 0.334zM20.24 13.232c0 0.116-0.058 0.247-0.145 0.334l-0.725 0.725c-0.087 0.087-0.203 0.145-0.334 0.145-0.116 0-0.247-0.058-0.334-0.145l-5.702-5.702-5.702 5.702c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-0.725-0.725c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l6.761-6.761c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l6.761 6.761c0.087 0.087 0.145 0.218 0.145 0.334z"/>' +
    '<path d="M0 0h24v24H0z" fill="none"/>' +
    '</svg>';
  const arrowDownSvg = '<svg class="btnRowCollapse arrowDown" id="arrowDown" fill="#000000" width="18" height="18" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M20.24 12.768c0 0.116-0.058 0.247-0.145 0.334l-6.761 6.761c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-6.761-6.761c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l0.725-0.725c0.087-0.087 0.203-0.145 0.334-0.145 0.116 0 0.247 0.058 0.334 0.145l5.702 5.702 5.702-5.702c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l0.725 0.725c0.087 0.087 0.145 0.218 0.145 0.334zM20.24 7.196c0 0.116-0.058 0.247-0.145 0.334l-6.761 6.761c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-6.761-6.761c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l0.725-0.725c0.087-0.087 0.203-0.145 0.334-0.145 0.116 0 0.247 0.058 0.334 0.145l5.702 5.702 5.702-5.702c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l0.725 0.725c0.087 0.087 0.145 0.218 0.145 0.334z"/>' +
    '<path fill="none" d="M0 0h24v24H0V0z"/>' +
    '</svg>';
  const arrowLeftSvg = '<svg class="btnColCollapse arrowLeft" id="arrowLeft" fill="#000000" width="18" height="18" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M14.436 19.036c0 0.116-0.058 0.247-0.145 0.334l-0.725 0.725c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-6.761-6.761c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l6.761-6.761c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l0.725 0.725c0.087 0.087 0.145 0.218 0.145 0.334s-0.058 0.247-0.145 0.334l-5.702 5.702 5.702 5.702c0.087 0.087 0.145 0.218 0.145 0.334zM20.008 19.036c0 0.116-0.058 0.247-0.145 0.334l-0.725 0.725c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-6.761-6.761c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l6.761-6.761c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l0.725 0.725c0.087 0.087 0.145 0.218 0.145 0.334s-0.058 0.247-0.145 0.334l-5.702 5.702 5.702 5.702c0.087 0.087 0.145 0.218 0.145 0.334z"/>' +
    '<path fill="none" d="M0 0h24v24H0V0z"/>' +
    '</svg>';
  const arrowRightSvg = '<svg class="btnColCollapse arrowRight" id="arrowRight" fill="#000000" width="18" height="18" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">' +
    '<path d="M14.436 13c0 0.116-0.058 0.247-0.145 0.334l-6.761 6.761c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-0.725-0.725c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l5.702-5.702-5.702-5.702c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l0.725-0.725c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l6.761 6.761c0.087 0.087 0.145 0.218 0.145 0.334zM20.008 13c0 0.116-0.058 0.247-0.145 0.334l-6.761 6.761c-0.087 0.087-0.218 0.145-0.334 0.145s-0.247-0.058-0.334-0.145l-0.725-0.725c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l5.702-5.702-5.702-5.702c-0.087-0.087-0.145-0.218-0.145-0.334s0.058-0.247 0.145-0.334l0.725-0.725c0.087-0.087 0.218-0.145 0.334-0.145s0.247 0.058 0.334 0.145l6.761 6.761c0.087 0.087 0.145 0.218 0.145 0.334z"/>' +
    '<path fill="none" d="M0 0h24v24H0V0z"/>' +
    '</svg>';
  const refreshSvg = '<svg class="btnRefresh" id="refresh" fill="#000000" width="18" height="18" viewBox="0 0 24 24" width="24"><path d="M0 0h24v24H0z" fill="none"/><path d="M17.65 6.35C16.2 4.9 14.21 4 12 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08c-.82 2.33-3.04 4-5.65 4-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"/></svg>';
  const topHeaderColors = ["#555555","#5a5a5a","#646464","#747474"];
  const topHeaderFontColor = "#FFFFFF";
  const sideHeaderColors = ["#555555","#5a5a5a","#646464","#747474"];
  const sideHeaderFontColor = "#FFFFFF";
  const dataColor = "#0275d8";
  const collapseColumnHeaders = undefined;
  const collapseRowHeaders = undefined;
  const allowRISDownload = true;
  const showRecordCount = false;
  const showColumnDescriptions = false;
  const showRowDescriptions = false;

  const $window = $(window);
  const $announce = $('#announce');
  const $pivotTable = $('.pivot-table');
  const $pivotBody = $('.body');
  const $textualBody = $('.textual-body');
  const $topHead = $('.top-head');
  const $textWrapp = $('.text-wrapp');
  const $topHeadTable = $('table', $topHead);
  const $topHeadWrapper = $('.top-head-wrapper');
  const $sideHead = $('.side-head');
  const $leftTable = $('.left-table');
  const $sideHeadTable = $('table', $sideHead);
  const $footer = $('.footer');
  const $menu = $('.menu');
  const $veil = $('.veil');
  const $reader = $('.reader');
  const $accessibility = $('.accessibility');
  const $attributeTooltip = $('.attribute-tooltip');
  const $attributeTooltipContent = $('.attribute-tooltip .content')
  const $recordCount = $('.record-count');

  let $header = $('.header');
  let chartType = 'bubble';
  let filterMode = 'default';

  //On page load force focus
  $(".menu").focus();
  $(document).ready(function () {
    var inputs = $('.menu').find('button').filter(':visible');

    var firstInput = inputs.first();
    var lastInput = inputs.last();

    /*set focus on first input*/
    firstInput.focus();

    /*redirect last tab to first input*/
    lastInput.on('keydown', function (e) {
      if ((e.which === 9 && !e.shiftKey)) {
        e.preventDefault();
        firstInput.focus();
      }
    });

    /*redirect first shift+tab to last input*/
    firstInput.on('keydown', function (e) {
      if ((e.which === 9 && e.shiftKey)) {
        e.preventDefault();
        lastInput.focus();
      }
    });
  });

  /**
   * @param {String} value
   */
  function safeParseInt(value) {
    const parsed = parseInt(value, 10);
    if (isNaN(parsed)) {
      return 0;
    }
    return parsed;
  }

  /**
   * Convert reference object from the eppi mapper to a RIS entry
   * @param referenceItem
   */
  function exportItemToRIS(reference, selectedKeywords) {
    const calend = [
      'Jan',
      'Feb',
      'Mar',
      'Apr',
      'May',
      'Jun',
      'Jul',
      'Aug',
      'Sep',
      'Oct',
      'Nov',
      'Dec',
    ];
    const newLine = '\r\n';
    let res = 'TY  - ';
    let tmp = '';
    switch (reference.TypeId) {
      case 14:
        res += 'JOUR' + newLine;
        break;
      case 1:
        res += 'RPRT' + newLine;
        break;
      case 2:
        res += 'BOOK' + newLine;
        break;
      case 3:
        res += 'CHAP' + newLine;
        break;
      case 4:
        res += 'THES' + newLine;
        break;
      case 5:
        res += 'CONF' + newLine;
        break;
      case 6:
        res += 'ELEC' + newLine;
        break;
      case 7:
        res += 'ELEC' + newLine;
        break;
      case 8:
        res += 'ADVS' + newLine;
        break;
      case 10:
        res += 'MGZN' + newLine;
        break;
      default:
        res += 'GEN' + newLine;
        break;
    }
    res += 'T1  - ' + reference.Title + newLine;
    if (reference.TypeId == 10 || reference.TypeId == 14)
      res += 'JF  - ' + reference.ParentTitle + newLine;
    else res += 'T2  - ' + reference.ParentTitle + newLine;
    for (let au of reference.Authors.split(';')) {
      tmp = au.trim();
      if (tmp != '') res += 'A1  - ' + tmp + newLine;
    }
    for (let au of reference.ParentAuthors.split(';')) {
      tmp = au.trim();
      if (tmp != '') res += 'A2  - ' + tmp + newLine;
    }
    res +=
      'KW  - eppi-reviewer4' +
      newLine +
      (reference.Keywords != null && reference.Keywords.length > 2
        ? reference.Keywords.trim() + newLine
        : '');
    let tmpDate = '';
    let Month = safeParseInt(reference.Month);

    if (!Month || Month < 1 || Month > 12) {
      Month =
        1 + reference.Month.length > 2
          ? calend.indexOf(reference.Month.substring(0, 3)) + 1
          : 0;
    }
    let yr = safeParseInt(reference.Year);
    if (reference.Year !== '' && yr) {
      if (yr > 0) {
        if (yr < 20) yr += 1900;
        else if (yr < 100) yr += 2000;
        if (yr.toString().length == 4) {
          res += 'PY  - ' + yr.toString() + newLine;
          if (Month != 0) {
            tmpDate +=
            reference.Year +
              '/' +
              (Month.toString().length == 1
                ? '0' + Month.toString()
                : Month.toString()) +
              '//';
          } else {
            tmpDate += reference.Year + '///' + reference.Month; //"y1  - "
          }
        }
      }
    }
    if (tmpDate.length > 0) {
      res += 'DA  - ' + tmpDate + newLine;
      res += 'Y1  - ' + tmpDate;

      //little trick: edition information is supposed to be the additional info at the end of the
      //Y1 filed. For Thesis pubtype (4) we use the edition field to hold the thesys type,
      //the following finishes up the Y1 field keeping all this into account

      if (reference.TypeId == 4 && reference.Edition.length > 0)
        res += newLine + 'KW  - ' + reference.Edition + newLine;
      else if (reference.Edition.length > 0) res += ' ' + reference.Edition + newLine;
      else res += newLine;
    } else if (reference.TypeId == 4 && reference.Edition.length > 0) {
      res += newLine + 'KW  - ' + reference.Edition + newLine;
    } //end of little trick

    res += 'AB  - ' + reference.Abstract + newLine;
    if (reference.DOI.length > 0) res += 'DO  - ' + reference.DOI + newLine;
    res += 'VL  - ' + reference.Volume + newLine;
    res += 'IS  - ' + reference.Issue + newLine;
    let split = '-';
    Yr = reference.Pages.indexOf(split);
    if (Yr > 0) {
      let pgs = reference.Pages.split(split);
      res += 'SP  - ' + pgs[0] + newLine;
      res += 'EP  - ' + pgs[1] + newLine;
    } else if (reference.Pages.length > 0) res += 'SP  - ' + reference.Pages + newLine;
    res +=
      'CY  - ' +
      reference.City +
      (reference.Country.length > 0 ? ' ' + reference.Country : '') +
      newLine;
    if (reference.URL.length > 0) res += 'UR  - ' + reference.URL + newLine;
    if (reference.Availability.length > 0) res += 'AV  - ' + reference.Availability + newLine;
    if (reference.Publisher.length > 0) res += 'PB  - ' + reference.Publisher + newLine;
    if (reference.StandardNumber.length > 0)
      res += 'SN  - ' + reference.StandardNumber + newLine;
    res += 'U1  - ' + reference.ItemId.toString() + newLine;
    res += 'U2  - ' + selectedKeywords + newLine;
    if (reference.OldItemId.length > 0) res += 'U3  - ' + reference.OldItemId + newLine;

    res += 'N1  - ' + reference.Comments + newLine;

    res += 'ER  - ' + newLine + newLine;

    res = res.replace('     ', ' ');
    res = res.replace('    ', ' ');
    res = res.replace('   ', ' ');
    res = res.replace('   ', ' ');
    return res;
  }

  /**
   *
   * @param {Array} references - array of eppi reference objects
   */
  function handleRisDownloadButtonClick(references){
    const $downloadButton = $('#risDownload');
    const selectedKeywords = $('li.checked', '.reader-filter').map(function() { return $(this).text().trim() }).get().join('; ');

    // remove existing onclick handlers
    $downloadButton.unbind();
    $downloadButton.on('click', (e) => {
      const fileBlob = createRISFile(references, selectedKeywords);
      downloadRISFile(fileBlob);
    });
  }

  /**
   * Create a RIS file blcb from the given references.
   * @param selectedReferences - list of references to create a RIS file from
   * @returns {Blob}
   */
  function createRISFile(selectedReferences, selectedKeywords){
    const len = selectedReferences.length;
    let risData = '';
    for(let i = 0; i < len; i++){
      risData += exportItemToRIS(selectedReferences[i], selectedKeywords);
    }
    return new Blob([risData], { type: 'text/plain' });

  }

  /**
   * Download the File blob of the references.
   * @param {Blob} file
  */
  function downloadRISFile(file){
    const blobUrl = URL.createObjectURL(file);
    // Save the need for file-save.js
    const link = document.createElement("a");
    link.href = blobUrl;
    link.download = `references.ris`;
    link.click();
  }

  /**
   * Gets the colour for the headings.
   * @param stepNumber
   * @returns {number}
   */
  function getColor(side, stepNumber) {
    // let base = 24;
    // let steps = 5;
    // let step = 8;
    //
    // // stops a step being greater than 5
    // stepNumber = stepNumber % steps;
    // return base + (step * stepNumber);
    stepNumber = stepNumber % 4;
    if (side)
      return sideHeaderColors[stepNumber];
    else
      return topHeaderColors[stepNumber];
  }

  /**
   * Builds the table used as the column headers of the pivot table.
   */
  function buildTableColHead() {
    let tableHeadHtml = '';
    let rowClass = '';
    let styles = [];

    for (let i = 0; i < csvData.totalColDepth; i++) {
      let cellClass = '';

      if (i > 0 && (i + 1) < csvData.totalColDepth) {
        rowClass = 'header-can-hide'
      } else if ((i + 1) === csvData.totalColDepth) {
        cellClass = 'clickable-col'
        rowClass = ''
      } else {
        rowClass = ''
      }

      const row = csvData.rows[i];
      tableHeadHtml += '<tr class="' + rowClass + '">';

      for (let j = 0; j < row.length; j++) {
        const col = row[j];

        const colSpan = i === 0 && j === 0 ? csvData.totalColBreadth : col.span;

        let style = '';
        let title = '';

        if (i === 0) {
          style = 'style="background-color:' + getColor(false, 0) + ';color:' + topHeaderFontColor + ';"';
        }

        if (i === 1) {
          style = 'style="background-color:' + getColor(false, j) + ';color:' + topHeaderFontColor + ';"';

          for (let k = 0; k < colSpan; k++)
          {
            styles.push(style);
          }
        }

        if (i > 1) {
          style = styles[j];
        }
        
        if (showColumnDescriptions && col.setDescription.length > 0) {
          title = col.setDescription;
        }

        tableHeadHtml += '<th class="level-' + i + ' ' + cellClass + '" colspan="' + colSpan + '" title="' + title + '" ' + style + ' data-id="' + col.id + '">';
        tableHeadHtml += '<div>';

        if (i === 1) {
          tableHeadHtml += arrowLeftSvg;
          tableHeadHtml += arrowRightSvg;
          tableHeadHtml += refreshSvg;
        }

        tableHeadHtml += '<span>';
        tableHeadHtml += col.title;
        tableHeadHtml += '</span></div>';
        tableHeadHtml += '</th>'
      }

      tableHeadHtml += '</tr>';
    }

    $('table thead', $topHead).append(tableHeadHtml);
    topColHeight = $('table', $topHead).height();
    $('table', $topHead).height(topColHeight);
  }

  /*
  Build text view table headers
  */
  function buildTextHeaderTable() {
    let textHead = '';
    let rowClass = '';
    let styles = [];

    for (let i = 0; i < csvData.totalColDepth; i++) {
      let cellClass = '';

      if (i > 0 && (i + 1) < csvData.totalColDepth) {
        rowClass = 'header-can-hide'
      } else if ((i + 1) === csvData.totalColDepth) {
        cellClass = 'clickable-col'
        rowClass = ''
      } else {
        rowClass = ''
      }

      const row = csvData.rows[i];
      textHead += '<tr class="' + rowClass + '">';

      for (let j = 0; j < row.length; j++) {
        const col = row[j];
        const colSpan = i === 0 && j === 0 ? csvData.totalColBreadth : col.span;

        let style = '';
        let title = '';

        if (i === 0) {
          style = 'style="background-color:' + getColor(false, 0) + ';color:' + topHeaderFontColor + ';"';
        }

        if (i === 1) {
          style = 'style="background-color:' + getColor(false, j) + ';color:' + topHeaderFontColor + ';"';

          for (let k = 0; k < colSpan; k++) {
            styles.push(style);
          }
        }

        if (i > 1) {
          style = styles[j];
        }

        if (showColumnDescriptions && col.setDescription.length > 0) {
          title = col.setDescription;
        }

        textHead += '<th class="table-width level-' + i + ' ' + cellClass + '" colspan="' + colSpan + '" title="' + title + '" ' + style + ' data-id="' + col.id + '">';
        textHead += '<div>';

        textHead += '<span>';
        textHead += col.title;
        textHead += '</span></div>';
        textHead += '</th>'
      }

      textHead += '</tr>';
    }

    $('table thead', $textWrapp).append(textHead);
    topColHeight = $('table', $textWrapp).height();
    $('table', $textWrapp).height(topColHeight);
  }

  /**
   * Builds the table used as the row headers of the pivot table.
   */
  function buildTableRowHead() {
    let tableHeadHtml = '';
    let rowClass = '';
    let lightnessLevel = 0;
    let backgroundStyle = '';

    for (let i = csvData.totalColDepth; i < csvData.rows.length; i++) {
      const row = csvData.rows[i];
      tableHeadHtml += '<tr>';

      for (let j = 0; j < row.length; j++) {
        const col = row[j];
        const rowSpan = i === csvData.totalColDepth && j === 0 ? csvData.totalRowBreadth : col.span;
        const level = csvData.totalColDepth - row.length + j;

        let title = '';

        if (level <= 0) {
          backgroundStyle = 'background-color:' + getColor(true, 0) + ';color:' + sideHeaderFontColor + ';';
        }

        if (level === 1) {
          backgroundStyle = 'background-color:' + getColor(true, lightnessLevel) + ';color:' + sideHeaderFontColor + ';';
          lightnessLevel += 1;
        }

        if (level > 0 && (level + 1) < csvData.totalColDepth) {
          rowClass = 'header-can-hide'
        } else if ((level + 1) === csvData.totalColDepth) {
          rowClass = 'clickable-row'
        }

        if (showRowDescriptions && col.setDescription.length > 0) {
          title = col.setDescription;
        }

        tableHeadHtml += '<th class="level-' + level + ' ' + rowClass + '" rowspan="' + rowSpan + '" title="' + title + '" style="' + backgroundStyle + '" data-id="' + col.id + '">';
        tableHeadHtml += '<div>';

        if (level === 1) {
          tableHeadHtml += arrowUpSvg;
          tableHeadHtml += arrowDownSvg;
          tableHeadHtml += refreshSvg;
        }

        tableHeadHtml += '<span>';
        tableHeadHtml += col.title;
        tableHeadHtml += '</span></div>';
        tableHeadHtml += '</th>'
      }

      tableHeadHtml += '</tr>';
    }

    $('table tbody', $sideHead).append(tableHeadHtml);
    sideColWidth = $('table', $sideHead).width();
    $('table', $sideHead).width(sideColWidth);
  }

  /**
   * Creates the table columns of a row for the pivot table.
   */
  function buildTableColumns(cols, $colHeaders, rowIds) {
    let colIndex = 0;
    let tableColHtml = '';

    $colHeaders.each(function(colHeaderIndex, colHeader) {
      const $colHeader = $(colHeader);
      const colSpan = parseInt($colHeader.attr('colspan'));

      if ($colHeader.hasClass('collapsed')) {
        const colIds = [];

        for (let i = 0; i < colSpan; i++) {
          colIds.push(cols[colIndex].id);
          colIndex++;
        }

        tableColHtml += '<td class="cell" ' +
          'data-colid="' + colIds.join(',') + '" ' +
          'data-rowid="' + rowIds.join(',') + '">' +
          '</td>';
      } else {
        for (let i = 0; i < colSpan; i++) {
          tableColHtml += '<td class="cell" ' +
            'data-colid="' + cols[colIndex].id + '" ' +
            'data-rowid="' + rowIds.join(',') + '">' +
            '</td>';
          colIndex++;
        }
      }
    });

    return tableColHtml;
  }

  /**
   * Creates the table rows of the pivot table.
   */
  function buildTableRows() {
    let tableRowHtml = '';
    let rowIndex = csvData.totalColDepth;

    const cols = csvData.rows[csvData.totalColDepth - 1];
    const $colHeaders = $('.top-head .level-1');
    const $rowHeaders = $('.side-head .level-1');

    $rowHeaders.each(function(rowHeaderIndex, rowHeader) {
      const rowIds = [];
      const $rowHeader = $(rowHeader);
      const rowSpan = parseInt($rowHeader.attr('rowspan'));

      if ($rowHeader.hasClass('collapsed')) {
        tableRowHtml += '<tr>';
        for (let i = 0; i < rowSpan; i++) {
          const row = csvData.rows[rowIndex];
          rowIds.push(row[row.length - 1].id);
          rowIndex++;
        }
        tableRowHtml += buildTableColumns(cols, $colHeaders, rowIds);
        tableRowHtml += '</tr>';
      } else {
        tableRowHtml += '<tr>';
        for (let i = 0; i < rowSpan; i++) {
          const row = csvData.rows[rowIndex];
          tableRowHtml += buildTableColumns(cols, $colHeaders, [row[row.length - 1].id]);
          rowIndex++;
          tableRowHtml += '</tr>';
        }
      }
    });

    $('table tbody', $pivotBody).html(tableRowHtml);
  }

  /**
   * Gets references that match the given row and column lists.
   * @param rowIdList
   * @param colIdList
   * @returns {Array}
   * @constructor
   */
  function getFilteredReferences(rowIdList, colIdList) {
    const references = [];
    const refMatches = referenceData
      .filter((reference) => {
        if (!reference.hasOwnProperty('Codes')) return false;

        //#region This Code Does All The Reader Filtering
        if (rowIdList.length > 0 && colIdList.length > 0) { // if we have selection in both column and row....
          for (const rowId of rowIdList) {  // for each row in the list of rows
            for (const colId of colIdList) {  // get a column in the list of columns
              if (reference.Codes // of the references that have codes,, does this particular reference have at least 2 codes that match rowid and colid
                .filter((code) => code.AttributeId === rowId || code.AttributeId === colId).length >= 2){
                  return true;
                }
            }
          }
        } else {
          for (const code of reference.Codes) {
            if (rowIdList.length === 0) {  // No row intersections to check for.
              if (colIdList.indexOf(code.AttributeId) >= 0) return true;
            } else if (colIdList.length === 0) {  // No column intersections to check for.
              if (rowIdList.indexOf(code.AttributeId) >= 0) return true;
            }
          }
        }
        //#endregion

        return false;
      });

    for (const reference of refMatches) {
      //#region does filtering in the settings menu (affects map display)
      if (filterMode === 'default'){
        //#region Check all the filters. (Parent AND > Child OR)
        const parentFilters = filters.filter((item) => item.checked);
        let refMatchesFilter = parentFilters.length === 0;

        for (const parentFilter of parentFilters) {
          refMatchesFilter = false;

          for (const childFilter of parentFilter.children.filter((item) => item.checked)) {
            const childFilterMatch = reference.Codes.filter((code) => code.AttributeId === childFilter.id).length > 0;

            if (childFilterMatch) {
              refMatchesFilter = true;
              break; // exit this loop --> immediately goes into execution of the parentFilter loop; leaves childfilter loops
                      // result is we end up not checking each selected child item for a match,, resulting in an "OR" condition; "AND" condition would include all selected child items
            }
          }

          // if we do not match in the child filters, then we know it's not AND and we
          // can break.
          if (!refMatchesFilter) break;
        }

        // if we do not match all active parent filters then we do not count this.
        if (!refMatchesFilter) continue;

        references.push(reference);
        //#endregion
      } else if (filterMode === 'and') {
        //#region Check all the filters. (Parent AND == Child AND)
        const parentFilters = filters.filter((item) => item.checked);
        let refMatchesFilter = parentFilters.length === 0;

        for (const parentFilter of parentFilters) { // when checking each parent filter item,
          refMatchesFilter = false; // reset match to false
          const childFilters = parentFilter.children.filter((item) => item.checked);  // filter out child filter items

          for (const childFilter of childFilters) { // each child filter item
            refMatchesFilter = false; // reset match to false
            const childFilterMatch = reference.Codes.filter((code) => code.AttributeId === childFilter.id).length > 0;  // check if child filter item's code matches any reference code

            if (childFilterMatch) { // if child filter item matches at least 1 reference code,
              refMatchesFilter = true;  // set match to true
              // continue this loop --> we do not immediately go into execution of the parentFilter loop; stay in childfilter loops
              // result is we check each selected child item for a match, returning only references which do match, resulting in an "AND" condition
            } else {
              // refMatchesFilter = false;
              break;
            }
          }

          if (!refMatchesFilter) break;
        }

        if (!refMatchesFilter) continue;

        references.push(reference);
        //#endregion
      } else if (filterMode === 'or') {
        //#region Check all the filters. (Parent OR == Child OR)
        const parentFilters = filters.filter((item) => item.checked);
        let refMatchesFilter = parentFilters.length === 0;

        for (const parentFilter of parentFilters) {
          const childFilters = parentFilter.children.filter((item) => item.checked);

          for (const childFilter of childFilters) {
            const childFilterMatch = reference.Codes.filter((code) => code.AttributeId === childFilter.id).length > 0;

            if (childFilterMatch) {
              refMatchesFilter = true;
              break;
            }
          }

          if (!refMatchesFilter) continue;
        }

        if (!refMatchesFilter){
        } else {
          references.push(reference);
        }
        //#endregion
      }
      //#endregion
    }

    return references;
  }

  /**
   * Builds the legend.
   */
  function buildLegend() {
    if (segmentAttributes === null) return;

    let legendHtml = '';

    segmentAttributes.forEach((segment) => {
      legendHtml += '<span class="legend-item" data-description="' + segment.attribute.AttributeSetDescription + '">' +
          '<span class="dot" style="background-color: ' + segment.color + '"></span>' +
          '<span class="label">' + segment.attribute.AttributeName + '</span></span>'
    })

    $('.legend').html(legendHtml);
  }

  /**
   * Creates a pie png image as a data url.
   */
  function createPiePng(counts) {
    let currentPoint = 0;
    let conicGrad = '';

    for (const count of counts) {
      currentPoint += count.width;
      if (conicGrad.length > 0) conicGrad += ', ';
      conicGrad += count.color + ' 0 ' + currentPoint + '%';
    }

    if (currentPoint < 100) {
      if (conicGrad.length > 0) conicGrad += ', ';
      conicGrad += '#eeeeee 0';
    }

    const gradient = new ConicGradient({
      stops: conicGrad,
      size: 97
    });

    const png = gradient.png;

    // Clean up memory issues once we have our PNG.
    $(gradient.canvas).empty().remove();
    delete gradient.canvas;

    return png;
  }

  /**
   * Create the mosiac svg
   *
   * @param {Number} dimension
   * @param {Array} references
   */
  function createMosaic(dimension, references) {
    let x = 0;
    let y = 0;

    let html = `<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg">
   <defs>
   <pattern id="grid" width="${dimension}" height="${dimension}" patternUnits="userSpaceOnUse">
   <path d="M ${dimension} 0 L 0 0 0 ${dimension}" fill="none" stroke="white" stroke-width="0.5" />
   </pattern>
   </defs >
    <rect width="100%" height="100%" fill="url(#grid)" />`;

    for (const ref of references) {
      html += `<rect width="${dimension}" height="${dimension}" fill="${ref}" x="${x}" y="${y}" stroke="white" stroke-width="0.5" />`;
      x += dimension;

      if (x > 90) {
        x = 0;
        y += dimension
      }
    }

    html += '</svg >';


    return html;
  }

  /**
   * Creates the initial zero counts for the block.
   * @param rowIds
   * @param colIds
   */
  function createInitialCounts(rowIds, colIds) {
    let counts = [];

    if (segmentAttributes.length === 0) {
      counts.push({
        id: rowIds.join('_') + '-' + colIds.join('_'),
        attribute: null,
        color: dataColor,
        count: 0,
        width: 0,
        size: 0
      })
    } else {
      for (const segment of segmentAttributes) {
        counts.push({
          id: segment.attribute.AttributeId,
          attribute: segment.attribute,
          color: segment.color,
          isLight: segment.isLight,
          count: 0,
          width: 0,
          size: 0
        })
      }
    }

    return counts
  }

  /**
   * Calculate the dimensions of the blocks that will go in the svg grid
   *
   * @param {Number} minRequiredBlocks minimum required mosiac tiles (equal to max number of references)
   * @param {Number} containerDimension dimensions of the container
   */
  function getBlockSize(minRequiredBlocks, containerDimension) {
    let number = Math.round(Math.sqrt(minRequiredBlocks));
    while (containerDimension % number != 0 && number < containerDimension) {
      number = number + 1;
    }
    return containerDimension / number;
  }

  /**
   * Updates all the reference counts and creates the visuals.
   */
  function updateReferenceCounts() {
    let maxCount = 0;

    // Remove old html and detach events.
    $('.cell', $pivotBody).off();
    $('.pie-wrapper').off().detach().remove();
    $('.data-wrapper').off().detach().remove();

    $('.cell', $pivotBody).each((index, cell) => {
      const $cell = $(cell);
      let colIds = $cell.data('colid')
      let rowIds = $cell.data('rowid');

      if (isNaN(colIds)) {
        colIds = colIds.split(',').map(id => parseInt(id));
      } else {
        colIds = [colIds]
      }

      if (isNaN(rowIds)) {
        rowIds = rowIds.split(',').map(id => parseInt(id));
      } else {
        rowIds = [rowIds]
      }

      const references = getFilteredReferences(rowIds, colIds);
      const counts = createInitialCounts(rowIds, colIds);

      let totalCount = 0;

      for (const reference of references) {
        for (const count of counts) {
          if (count.attribute === null) {
            totalCount += 1;
            count.count += 1;
            continue;
          }

          const countMatched = reference.Codes
            .filter((code) => code.AttributeId === count.id).length > 0

          if (countMatched) {
            count.count += 1;
            totalCount += 1;
          }
        }
      }

      if (totalCount === 0) {
        $cell.addClass('none');
      } else {
        $cell.removeClass('none');
      }

      if (totalCount > maxCount) {
        maxCount = totalCount
      }

      for (const count of counts) {
        count.width = totalCount === 0 || count.count === 0 ? 0 : Math.round(count.count / totalCount * 100);

        // Check if the count is a sane amount to see, if not, make it the minimum.
        if (count.width < 8 && count.count !== 0) count.width = 8
      }

      // Sort counts descending.
      counts.sort(function(a, b) { return b.count - a.count});

      $cell.data('totalSize', 0);
      $cell.data('totalCount', totalCount);
      $cell.data('counts', counts);

      let wrapper = '';

      if (counts.length > 0) {
        switch (chartType) {
          case 'bubble':
          case 'heat':
          case 'text':
            wrapper = '<div class="data-wrapper">'

            let counter = 1;

            for (const count of counts) {
              if (count.count === 0) continue;

              let style = 'background-color:' + count.color + ';';

              if (!count.isLight) {
                style += 'color:#ffffff;';
              }

              wrapper += '<div id="' + count.id + '" ' +
                'class="data" ' +
                'style="' + style + '">';

              if (chartType === 'text') {
                if (count.attribute !== null) {
                  wrapper += `<span class="count">${count.count}</span><span class="name">${count.attribute.AttributeName}</span>`;
                } else if (count.count > 1) {
                  wrapper += `<span class="count">${count.count}</span><span class="name">Records</span>`;
                } else {
                  wrapper += `<span class="count">${count.count}</span><span class="name">Record</span>`;
                }
              }

              wrapper += '</div>';

              if (chartType !== 'text' && counter % 2 === 0) {
                wrapper += '<div class="break"></div>'
              }

              counter ++;
            }

            wrapper += '</div>'

            break;
          case 'mosaic':
            wrapper = '<div class="mosaic-wrapper"></div>'
            break;
          case 'donut':
            const png = createPiePng(counts);
            const piePngStyle = png.length > 0 ? ' style="background-image: url(' + png + ')"' : '';
            wrapper = `<div class="pie-wrapper"><div class="pie"${piePngStyle}></div><div class="pie-hole"></div></div>`
            break;
        }
      }

      $cell.html(wrapper);
    });

    // Update the size of the chart elements.
    $('.cell', $pivotBody).each((index, cell) => {
      const $cell = $(cell);
      const totalCount = $cell.data('totalCount');
      const counts = $cell.data('counts');

      for (const count of counts) {
        count.size = count.count === 0 ? 0 : Math.round(count.count / maxCount * 100);
        // Check if the count is a sane amount to see, if not, make it the minimum.
        if (count.size < 8 && count.count !== 0) count.size = 8
      }

      let totalSize = totalCount === 0 ? 0 : Math.round(totalCount / maxCount * 100);
      // Check if the count is a sane amount to see, if not, make it the minimum.
      if (totalSize < 8 && totalCount !== 0) totalSize = 8

      $cell.data('counts', counts);
      $cell.data('totalSize', totalSize);
      $cell.data('maxCount', maxCount);
    });

    // Setup events.
    setupTooltips();
    handleTableClick();
  }

  /**
   * Switches between the bubble and heat-map views.
   */
  function updateTableDataView() {
    $('.cell', $pivotBody).each((index, cell) => {
      const $cell = $(cell);
      const counts = $cell.data('counts');
      const totalSize = $cell.data('totalSize');
      const totalWidth = $cell.data('totalWidth');
      const maxCount = $cell.data('maxCount');
      const blockSize = 100;

      switch (chartType) {
        case 'bubble':
          $('.data-wrapper', $cell).css({
            'display': 'flex'
          });
          $('.data', $cell).css({
            'border-radius': '100%'
          });
          break;
        case 'heat':
          $('.data-wrapper', $cell).css({
            'display': 'flex',
            'opacity': (totalSize / 100)
          });
          break;
        case 'mosaic':
          $('.mosaic-wrapper', $cell).css({
            'display': 'block'
          });
          const dimension = getBlockSize(maxCount, 100);
          // Get colours from segments
          const referenceColours = counts.map(x => new Array(x.count).fill(x.color));
          const references = [].concat.apply([], referenceColours);
          let mosaic = createMosaic(dimension, references);
          $('.mosaic-wrapper', $cell).html(mosaic)
          break;
        case 'donut':
          const circleSize = (blockSize * totalSize / 100);
          let holeSize = totalSize - (circleSize / blockSize * 24);
          if (holeSize < 0) holeSize = 0;

          $('.pie-wrapper', $cell).css({
            'display': 'block'
          });

          $('.pie', $cell).css({
            'width': circleSize + '%',
            'height': circleSize + '%'
          });

          $('.pie-hole', $cell).css({
            'width': holeSize + '%',
            'height': holeSize + '%'
          });
          break;
        case 'text':
          break;
      }

      if (chartType !== 'text') {
        let maxBlockSize = blockSize;

        if (segmentAttributes.length > 1) {
          maxBlockSize = blockSize / (segmentAttributes.length / 2);
        }

        for (const [index, count] of counts.entries()) {
          let size = blockSize * count.size / 100;

          // Check if the count is a sane amount to see, if not, make it the minimum.
          if (size < 8 && count.count !== 0) size = 8;
          else if (size > maxBlockSize && count.count !== 0) size = maxBlockSize;

          $('#' + count.id, $cell).css({
            'height': chartType === 'bubble' ? size + 'px' : '100%',
            'width': chartType === 'bubble' ? size + 'px' : count.width + '%'
          });
        }

        setTimeout(function () {
          $('.data-wrapper', $cell).css({
            'opacity': chartType === 'heat' ? totalWidth / 100 : 1
          });
        }, 500);
      }
    });
  }

  /**
   * Cross browser functionality to request full screen access.
   * https://developer.mozilla.org/en-US/docs/Web/API/Fullscreen_API#Toggling_fullscreen_mode
   */
  function toggleFullScreen() {
    if (!document.fullscreenElement &&    // alternative standard method
      !document.mozFullScreenElement && !document.webkitFullscreenElement && !document.msFullscreenElement ) {  // current working methods
      if (document.documentElement.requestFullscreen) {
        document.documentElement.requestFullscreen();
      } else if (document.documentElement.msRequestFullscreen) {
        document.documentElement.msRequestFullscreen();
      } else if (document.documentElement.mozRequestFullScreen) {
        document.documentElement.mozRequestFullScreen();
      } else if (document.documentElement.webkitRequestFullscreen) {
        document.documentElement.webkitRequestFullscreen(Element.ALLOW_KEYBOARD_INPUT);
      }
    } else {
      if (document.exitFullscreen) {
        document.exitFullscreen();
      } else if (document.msExitFullscreen) {
        document.msExitFullscreen();
      } else if (document.mozCancelFullScreen) {
        document.mozCancelFullScreen();
      } else if (document.webkitExitFullscreen) {
        document.webkitExitFullscreen();
      }
    }
  }

  /**
   * Helper function to build the full table.
   */
  function buildTable() {
    buildTableColHead();
    buildTextHeaderTable();
    buildTableRowHead();
    buildTableRows();

    // Update data in the table.
    updateReferenceCounts();
    updateTableDataView();
  }

  /**
   * Sets the filter state of the the given filter ID by either adding it or
   * removing it from a list of unchecked filter IDs.
   * @param checkFilters
   * @param filterId
   * @param klass
   */
  function setFilterState(checkFilters, filterId, klass) {
    for (const filter of checkFilters) {
      if (filterId === filter.id) {
        filter.checked = klass === 'checked' || klass === 'indeterminate';
      }

      if (filter.children.length > 0) {
        setFilterState(filter.children, filterId, klass);
      }
    }
  }

  /**
   * Updates the filter's parent node's state.
   * @param $target
   */
  function updateFilterParentNodeCheckedState($target) {
    let hasChecked = false;
    let hasUnchecked = false;
    let hasIndeterminate = false;

    $target.next().children().each(function () {
      const klass = $(this).attr('class');

      if (klass === 'checked') {
        hasChecked = true;
      } else if (klass === 'unchecked') {
        hasUnchecked = true;
      } else {
        hasIndeterminate = true;
      }
    })

    let klass = '';

    if (hasChecked && !hasUnchecked && !hasIndeterminate) {
      klass = 'checked'
    } else if (!hasChecked && hasUnchecked && !hasIndeterminate) {
      klass = 'unchecked'
    } else if ((hasChecked && hasUnchecked && !hasIndeterminate) ||
      (hasChecked && !hasUnchecked && hasIndeterminate) ||
      (!hasChecked && hasUnchecked && hasIndeterminate) ||
      (hasChecked && hasUnchecked && hasIndeterminate)) {
      klass = 'indeterminate'
    }

    if (klass.length > 0) {
      const filterId = $target.data('id');
      setFilterState(filters, filterId, klass);

      $target
        .removeClass($target.attr('class'))
        .addClass(klass);
    }
  }

  /**
   * Updates the filter's node state and calls it's children r
   * recursively.
   * @param $target
   * @param newKlass
   */
  function updateFilterNodeCheckedState($target, newKlass = '') {
    const klass = $target.attr('class');
    $('#filterClearButton').attr('disabled',false);

    if (newKlass.length === 0) {
      if (klass === 'checked') {
        newKlass = 'unchecked';
      } else {
        newKlass = 'checked';
      }
    }

    const $next = $target.next();

    if ($next.is('ul')) {
      $next.children().each(function () {
        updateFilterNodeCheckedState($(this), newKlass);
      })
    }

    if ($target.is('li')) {
      const filterId = $target.data('id');
      setFilterState(filters, filterId, newKlass);

      $target
        .removeClass(klass)
        .addClass(newKlass);

      const $parent = $target.parent().prev();
      updateFilterParentNodeCheckedState($parent);
    }
  }

  /**
   * Update's the filter type and the checked state of the radios.
   * @param $filters
   * @param $target
   */
  function updateFilterMode($filters, $target) {
    const klass = $target.attr('class');
    if (klass === 'checked') return;

    filterMode = $target.data('id');

    $('.filter-type-wrapper li', $filters)
        .removeClass('checked')
        .addClass('unchecked');
    $target
        .removeClass('unchecked')
        .addClass('checked');
  }

  /**
   * Creates a filter node recursively.
   * @param filters
   * @returns {string}
   */
  function createFilterNode(filters) {
    let filterHtml = '<ul>';

    for (const filter of filters) {
      filterHtml += '<li class="unchecked" data-id="' + filter.id + '">' +
        checkboxCheckedSvg +
        checkboxUncheckedSvg +
        checkboxIndeterminateSvg +
        ' <span>' + filter.label + '</span></li>';

      if (filter.children.length > 0) filterHtml += createFilterNode(filter.children);
    }

    filterHtml += '</ul>';
    return filterHtml;
  }

  /**
   * Creates the elements for the settings panel and hooks up
   * the events.
   */
  function createSettingsPanel() {
    const $filters = $('.settings');

    // Open filter if it should be auto open.
    if (autoOpenFilter) {
      $filters.css({display: 'block'});
      $filters.addClass('open');
      $veil.addClass('open');
      settingsFocusTrap.activate();
    }

    let settingsHtml = '<div class="title clearfix">' +
      '<span id="settingsTitle">Filters</span>' +
      '<button class="btnSettings right" id="close" role="button" aria-label="Close filters">close</button>' +
      '<button class="btnSettings left disabled" id="update" role="button" aria-label="Update filters">update</button>' +
      '</div>';
    settingsHtml += createFiltersPanel();

    $filters.html(settingsHtml);

    $('.filter-type-wrapper li', $filters).on('click', (e) => {
      updateFilterMode($filters, $(e.currentTarget));
      $('.disabled').removeClass('disabled');
      $('#update', $filters).on('click', handleUpdate);
    });

    $('.filter-wrapper li', $filters).on('click', (e) => {
      updateFilterNodeCheckedState($(e.currentTarget));
      $('.disabled').removeClass('disabled');
      $('#update', $filters).on('click', handleUpdate);
    });

    $('.menu-settings').on('click', function () {
      $filters.css({display: 'block'});
      $filters.addClass('open');
      $veil.addClass('open');
      $filters.focus();
      settingsFocusTrap.activate();
    });    

    $('#filterClearButton').on('click', function () {
      const $filterButton = $('#filterClearButton');
      const $filteritems = $('.filter-wrapper ul > li');

      for (const item of $filteritems) {
        const $item = $(item);
        updateFilterNodeCheckedState($item, 'unchecked');
      }
      $filterButton.attr('disabled', true);
      $('.disabled').removeClass('disabled');
      $('#update', $filters).on('click', handleUpdate);
    })

    $('#close', $filters).on('click', function () {
      if (!$('#update', $filters).hasClass('disabled')) {
        if (!confirm('You have not updated the map with your changes. Are you sure you want to close?')) {
          return;
        }
      }

      $filters.removeClass('open');
      $veil.removeClass('open');
      $filters.css({display: 'none'});
      settingsFocusTrap.deactivate();
      $('.menu-settings').focus();
    });

    // Close modal on escape key
    $filters.on('keydown', function (event) {
      if (event.key === "Escape") {
        if (!$('#update', $filters).hasClass('disabled')) {
          if (!confirm('You have not updated the map with your changes. Are you sure you want to close?')) {
            return;
          }
        }
  
        $filters.removeClass('open');
        $veil.removeClass('open');
        $filters.css({display: 'none'});
        settingsFocusTrap.deactivate();
        $('.menu-settings').focus();
      }
    });

    function handleUpdate() {
      const $update = $(this);
      $update.addClass('busy');

      setTimeout(() => {
        updateReferenceCounts();
        updateTableDataView();
        UpdateRecordCount();

        $filters.removeClass('open');
        $veil.removeClass('open');
        $update.removeClass('busy')
          .addClass('disabled');
        $update.off('click');
        $filters.css({display: 'none'});
        settingsFocusTrap.deactivate();
        $('.menu-settings').focus();
      }, 300);
    }
  }

  /**
   * Creates the filter section elements for the settings panel.
   * @returns {string}
   */
  function createFiltersPanel() {
    let filterHtml = `<div class="filter-type-wrapper">
        <h2>Filter mode</h2>
        <ul>
          <li class="${filterMode === 'default' ? 'checked': 'unchecked'}" data-id="default">
            ${radioCheckedSvg}
            ${radioUncheckedSvg}
            <span>Default <small>(OR within sections, AND across sections)</small></span>
          </li> 
          <li class="${filterMode === 'and' ? 'checked': 'unchecked'}" data-id="and">
            ${radioCheckedSvg}
            ${radioUncheckedSvg}
            <span>And</span>
          </li> 
          <li class="${filterMode === 'or' ? 'checked': 'unchecked'}" data-id="or">
            ${radioCheckedSvg}
            ${radioUncheckedSvg}
            <span>Or</span>
          </li> 
        </ul>
      </div>
      <div class="filter-wrapper">
        <h2>Filters</h2>`;
      filterHtml +=
      '<button id="filterClearButton">Clear Filter</button>';
    filterHtml += createFilterNode(filters);
    filterHtml +=
    '</div>';

    return filterHtml;
  }

  function handleStyleClicked(e) {
    const styleButtons = document.querySelectorAll('button[data-style]');

    chartType = e.currentTarget.dataset.style;

    for (var i = 0; i < styleButtons.length; i++) {
      styleButtons[i].ariaChecked = false;
    }

    e.currentTarget.ariaChecked = true;

    if (chartType === 'text') {
      $pivotTable.addClass('text-view');

      if (segmentAttributes.length > 4) {
        $pivotTable.addClass('text-view-large');
      }
    } else {
      $pivotTable.removeClass('text-view');
      $pivotTable.removeClass('text-view-large');
    }

    adjustTable();
    updateReferenceCounts();
    updateTableDataView();

    $announce.html('Map changed to ' + chartType + 'view.');
  }

  function setupStyleSwitch() {
    var styleButtons = document.querySelectorAll('button[data-style]');
    var donutStyleButton = document.querySelectorAll('button[data-style="donut"]');

    for (var i = 0; i < styleButtons.length; i++) {
      styleButtons[i].onclick = handleStyleClicked;
    }

    if (segmentAttributes.length === 0 && donutStyleButton.length > 0) {
      donutStyleButton[0].style.display = 'none';
    }
  }

  /**
   * Creates the tooltip for the cell as it's hovered over.
   */
  function setupTooltips() {
    $('.cell').hover((e) => {
      if (chartType === 'text') return;

      // Hover.
      const $target = $(e.currentTarget);
      const totalCount = $target.data('totalCount');

      if (totalCount <= 0) {
        return
      }

      const counts = $target.data('counts');

      let tooltipHtml = '<div class="tooltip">';

      for (const count of counts) {
        if (count.count === 0) continue;

        let title = 'Record';

        if (count.count > 1) {
          title = 'Records';
        }

        if (count.attribute !== null) {
          title = count.attribute.AttributeName
        }

        tooltipHtml += '<div class="count">' +
          '<span style="background-color: ' + count.color + ';">' + count.count + '</span> ' +
          title +
          '</div>';
      }

      tooltipHtml += '</div>';

      $(tooltipHtml)
        .appendTo('body')
        .fadeIn('fast');
    }, (e) => {
      // Hover out.
      $('.tooltip').remove();
      delete $('.tooltip');
    }).mousemove((e) => {
      $('.tooltip')
        .css({
          top: e.pageY + 10,
          left: e.pageX + 10
        });
    });
  }

  /**
   * Creates the meta item in the reader for the reference key
   * passed in.
   * @param key
   * @param reference
   * @returns {string}
   */
  function createMetaItem(key, reference) {
    let metaItem = '';
    const renameKeys = {
      'Journal': 'ParentTitle'
    }
    let tempKey = renameKeys[key];
    if (reference[tempKey] == undefined) {
      tempKey = key
    }

    if (tempKey !== summaryAttribute && reference[tempKey] !== undefined && reference[tempKey].length > 0) {

      let label = '';
      label = key

      metaItem += '<div class="meta-data-item clearfix ' + tempKey + '">';
      metaItem += '<label>' + label.replace(/([^A-Z])([A-Z])/g, '$1 $2') + '</label>';
      metaItem += '<span>' + reference[tempKey] + '</span>';
      metaItem += '</div>'
    }

    return metaItem;
  }

  /**
   * Creates the elements to read a review.
   * @param refId
   */
  function selectReference(refId) {
    const references = referenceData.filter((reference) => {
      return reference.ItemId === refId;
    });

    if (references.length === 0) {
      $('.read').html('');
      return;
    }

    const reference = references[0];
    let refHtml = '<h2>' + reference.Title + '</h2>';
    refHtml += '<hr>';

    if (reference.Abstract !== undefined && reference.Abstract.length > 0) {
      refHtml += '<p>' + reference.Abstract.replace(/\r/g, '<br>') + '</p>';
      refHtml += '<hr>';
    } else if (summaryAttribute.length > 0 && reference[summaryAttribute] !== undefined && reference[summaryAttribute].length > 0) {
      refHtml += '<p>' + reference[summaryAttribute].replace(/\r/g, '<br>') + '</p>';
      refHtml += '<hr>';
    }

    if (reference.URL !== undefined && reference.URL.length > 0) {
      refHtml += '<ol class="refs">';
      refHtml += '<li><a class="small" href="' + reference.URL + '" target="_blank"> Read full article:' + reference.Title + '</a></li>';

      if (reference.DOI === undefined || reference.DOI.length === 0) {
        refHtml += '</ol>';
        refHtml += '<hr>';
      }
    }

    if (reference.DOI !== undefined && reference.DOI.length > 0) {
      if (reference.URL === undefined || reference.URL.length === 0) {
        refHtml += '<ol class="refs">';
      }

      refHtml += '<li><a class="small" href="' + reference.DOI + '" target="_blank"> Read full article:' + reference.Title + '</a></li>';
      refHtml += '</ol>';
      refHtml += '<hr>';
    }

    refHtml += '<div class="meta-data">';
    for (const key of metaProperties) {
      refHtml += createMetaItem(key, reference);
    }

    var matchedExturlAttributes = [];
    refHtml += "<div> <hr>";
    // checking for matches with external Attribute URLs to display in reader
    for (const attribute of externalURLedAttributes) {
      matchedExturlAttributes = reference.Codes.filter(code =>
        code.AttributeId === attribute.AttributeId);
      if (matchedExturlAttributes.length > 0 && attribute.ExtURL !== "" && attribute.ExtURL !== undefined) {
        refHtml +=
        '<span>' + attribute.AttributeName + '<br>' +
          '<a href="' + attribute.ExtURL + '" target="_blank">' + attribute.ExtURL + '</a>' +
        '</span><br>';
      }
    }
    refHtml +='</div>';

    refHtml += '</div>';

    $('.read', $reader).html(refHtml);
    $('#Article').attr("aria-labeledby", "ArticleTitle" + refId);
    $announce.html(reference.Title + 'has been selected.');
  }

  /**
   * Updates the reader to show the newly filtered/changed references.
   * @param references
   */
  function updateReferenceReader(references, justOpened = false) {
    $('.refMenuItem').off();

    const readerOrder = $('#RefSortOrder').val();

    if (readerOrder === 'title') {
      references.sort(function (a, b) {
        if (a.Title > b.Title) return 1;
        else if (a.Title < b.Title) return -1;
        else return 0;
      });
    } else if (readerOrder === 'author') {
      references.sort(function (a, b) {
        if (a.Authors > b.Authors) return 1;
        else if (a.Authors < b.Authors) return -1;
        else return 0;
      });
    } else if (readerOrder === 'date') {
      references.sort(function (a, b) {

        let aDateStr = '';
        let bDateStr = '';

        if (a.Month.length !== 0) {
          aDateStr += a.Month + ' 01, ';
        }
        aDateStr += a.Year;

        if (b.Month.length !== 0) {
          bDateStr += b.Month + ' 01, ';
        }
        bDateStr += b.Year;

        const aDate = Date.parse(aDateStr);
        const bDate = Date.parse(bDateStr);

        if (aDate > bDate) return 1;
        else if (aDate < bDate) return -1;
        else return 0;
      });
    }

    let title = '';
    let textToRead = '';

    if (justOpened) {
      textToRead = 'Reader modal opened. '
    }

    if (references.length === 0) {
      textToRead += 'No search results available.'
    } else
    if (references.length === 1) {
      title = references.length + ' Record'
      textToRead += references.length + ' search result available.'
    } else {
      title = references.length + ' Records'
      textToRead += references.length + ' search results are available.'
    }

    $groupingSelect = $('.grouping-opts');

    let refsHtml = '';

    if ($groupingSelect.val() === 'none') {   // no grouping
      //#region ADDS ALL THE STUDIES INTO AN UNSEGMENTED LIST
      refsHtml = '<ul>';

      for (const ref of references) {
        let colors = [];

        for (const code of ref.Codes) {
          for (const segment of segmentAttributes) {
            if (code.AttributeId === segment.attribute.AttributeId) {
              colors.push(segment.color);
            }
          }
        }

        refsHtml += '<li class="refMenuItem" data-refid="' + ref.ItemId + '" segmented="no">' +
          '<button aria-controls="Article">' +
          '<div class="title" id="ArticleTitle' + ref.ItemId + '">' + ref.Title + '</div>' +
          '<div class="auth">' + ref.Authors + '</div>';

        refsHtml += '<div class="date">';

        if (ref.Month.length !== 0) {
          refsHtml += ref.Month + ', ';
        }

        refsHtml += ref.Year;

        for (let color of colors) {
          refsHtml += '<span class="refMenuItemLegend" style="background-color: ' + color + '" />';
        }

        refsHtml += '</div>';
        refsHtml += '</button>';
        refsHtml += '</li>';
      }

      refsHtml += '</ul>';
      // #endregion
    } else if ($groupingSelect.val() === 'segment') {  // group by segments
      //#region ADD STUDIES INTO INDIVUDUAL UNORDERED LISTS
      refsHtml = '<ul class="segmented">';

      for (const segment of segmentAttributes){
        refsHtml +=
        '<li style="border-left:2px solid ' + segment.color + ';">' +
          '<div class="segment-title" style="border-bottom: 3px solid ' + segment.color + ';border-left: 5px solid ' + segment.color + '">'
            + segment.attribute.AttributeName +
          '</div>' +
          '<ul>';
            for (const ref of references) {
              let colors = [];

              for (const code of ref.Codes) {
                for (const segment of segmentAttributes){
                  if (code.AttributeId === segment.attribute.AttributeId) {
                    colors.push(segment.color);
                  }
                }
              }
              for (const code of ref.Codes) {
                if (code.AttributeId === segment.attribute.AttributeId) {
                  refsHtml +=
                  '<li class="refMenuItem" data-refid="' + ref.ItemId + '" segmented="yes">' +
                  '<button aria-controls="Article">' +
                  '<div class="title" id="ArticleTitle' + ref.ItemId + '">' + ref.Title + '</div>' +
                  '<div class="auth">' + ref.Authors + '</div>';

                  refsHtml += '<div class="date">';

                  if (ref.Month.length !== 0) {
                    refsHtml += ref.Month + ', ';
                  }

                  refsHtml += ref.Year;

                  for (let color of colors) {
                    refsHtml +=
                    '<span class="refMenuItemLegend" style="background-color: ' + color + '" />';
                  }

                  refsHtml += '</div>';
                  refsHtml += '</button>';
                  refsHtml += '</li>';
                }
              }
            }
          refsHtml +=
          '</ul>' +
        '</li>'
      }
      refsHtml +=
      '</ul>'
      //#endregion
    }

    // alert users on search results
    $announce.html(textToRead);
    $('.nav > ul', $reader).attr('id', 'results').attr('role', 'presentation');

    $('.title > div > span', $reader).html(title);
    $('.nav', $reader).html(refsHtml);

    $('.refMenuItem').on('click', (e) => {
      $('.refMenuItem').removeClass('selected');
      $('.rerefMenuItem > .title, .auth, .date').css('pointer-events','none');

      const $target = $(e.currentTarget);
      const refId = $target.data('refid');

      $target.addClass('selected');
      selectReference(refId);
    });

    if (references.length > 0) {
      $($('.refMenuItem')[0]).trigger('click');
    } else {
      selectReference(0);
    }

    handleRisDownloadButtonClick(references);
  }

  /**
   * Gets the reference code id's from the checked items in the reader
   * filter.
   * @returns {{rows: Array, cols: Array}}
   */
  function getReaderFilterSelection() {
    const colIdList = [];
    const rowIdList = [];

    $('.reader-filter .cols li').each((index, col) => {
      const $col = $(col);
      if ($col.attr('class') === 'checked') {
        colIdList.push($col.data('id'))
      }
    });

    $('.reader-filter .rows li').each((index, row) => {
      const $row = $(row);
      if ($row.attr('class') === 'checked') {
        rowIdList.push($row.data('id'))
      }
    });

    return {
      cols: colIdList,
      rows: rowIdList
    };
  }

  /**
   * Creates the reader selector HTML
   * @returns {string}
   */
  function buildReaderFilter(rowIdList, colIdList) {
    const readerTopFilter = csvData.rows[csvData.totalColDepth - 1]
    const readerSideFilter = []

    for (let i = csvData.totalColDepth; i < csvData.rows.length; i++) {
      const row = csvData.rows[i]
      readerSideFilter.push(row[row.length - 1])
    }

    let colParent = readerTopFilter.length === colIdList.length ? 'checked' : 'indeterminate';
    if (colIdList.length === 0) colParent = 'unchecked';

    let html = '<div class="reader-filter">' +
      '<button id="codeFilterClearButton" class="btn">Clear Filters</button>' +
      '<ul>' +
      '<li class="' + colParent + '" data-parent="true" title="' + csvData.rows[0][0].title + '">' +
      checkboxCheckedSvg +
      checkboxUncheckedSvg +
      checkboxIndeterminateSvg +
      ' <span>' + csvData.rows[0][0].title + '</span>' +
      '</li>' +
      '<ul class="cols">';

    for (const item of readerTopFilter)
    {
      const state = colIdList.indexOf(item.id) >= 0 ? 'checked' : 'unchecked';
      html += '<li class="' + state + '" data-id="' + item.id + '" data-parent="false" title="' + item.title + '">' +
        checkboxCheckedSvg +
        checkboxUncheckedSvg +
        checkboxIndeterminateSvg +
        ' <span>' + item.title + '</span>' +
        '</li>';
    }

    let rowParent = readerSideFilter.length === rowIdList.length ? 'checked' : 'indeterminate';
    if (rowIdList.length === 0) rowParent = 'unchecked';

    html += '</ul>';
    html += '<li class="' + rowParent + '" data-parent="true" title="' + csvData.rows[csvData.totalColDepth][0].title + '">' +
      checkboxCheckedSvg +
      checkboxUncheckedSvg +
      checkboxIndeterminateSvg +
      ' <span>' + csvData.rows[csvData.totalColDepth][0].title + '</span>' +
      '</li>';
    html += '<ul class="rows">';

    for (const item of readerSideFilter)
    {
      const state = rowIdList.indexOf(item.id) >= 0 ? 'checked' : 'unchecked';
      html += '<li class="' + state + '" data-id="' + item.id + '" data-parent="false" title="' + item.title + '">' +
        checkboxCheckedSvg +
        checkboxUncheckedSvg +
        checkboxIndeterminateSvg +
        ' <span>' + item.title + '</span>' +
        '</li>';
    }

    html += '</ul>';
    html += '</ul>';
    html += '</div>';
    return html
  }

  /**
   * Builds the reference reader.
   * @param selectedRowIds
   * @param selectedColIds
   */
  function buildReferenceReader(selectedRowIds, selectedColIds) {
    let risDownloadButton = '';

    if (allowRISDownload) {
      risDownloadButton = '<button id="risDownload" class="btn">Download Listed References</button>';
    }

    let readerHtml =
      '<div class="title clearfix">' +
        '<div>' +
        '<button class="btn close" aria-label="Close view records" role="button">X</button>' + 
        '<span></span>' +
        '</div><div>' +
        '<select id="filter-options" class="filter-opts">' +
          '<option class="opt-all" value="1"> All </option>' +
          '<option class="opt-title" value="2"> Title </option>' +
          '<option class="opt-abstract" value="3"> Abstract </option>' +
          '<option class="opt-author" value="4"> Author </option>' +
        '</select>' +
        '<input id="searchFilter" type="text" placeholder="Filter" class="reader-filter" aria-describedby="text-desc" aria-owns="results" aria-hidden="true" aria-activedescendant="">' +
        risDownloadButton +
        '</div>' +
      '</div>' +
      '<div class="content">' +
        buildReaderFilter(selectedRowIds, selectedColIds) +
        '<div class="navTainer">' +
          '<div class="navGroupSelect">' +
            '<label for="sgroup" style="width:70px;display:block;float:left;">Group by: </label>' +
            '<select id="sgroup" class="grouping-opts">' +
            '<option value="none">None</option>' +
            '<option value="segment">Segment</option>' +
            '</select>' +
          '</div>' +
          '<div class="ref-sort-order">' +
            '<label for="RefSortOrder" style="width:70px;display:block;float:left;">Sort by: </label>' +
            '<select id="RefSortOrder">' +
              '<option value="title">Title</option>' +
              '<option value="author">Author</option>' +
              '<option value="date">Date</option>' +
            '</select>' +
          '</div>' +
          '<div class="nav">' +
          '</div>' +
        '</div>' +
        '<div class="read" id="Article" role="region" aria-labeledby="" tabindex="-1"></div>' +
      '</div>';

    $reader.html(readerHtml);

    if (segmentAttributes.length <= 0) {
      $('.navGroupSelect').hide();
    }
  }

  /**
   * Performs the search on the reader.
   * @param event
   * @param references
   * @param $readerFilter
   * @param $filterSelect
   */
  function doReaderSearch(event, references, $readerFilter, $filterSelect) {
    if (event !== null && event.which === -1) return;

    const searchTerm = $readerFilter.val().toLowerCase().trim();
    const filterOption = $filterSelect.val();
    let searchReferences = [];

    if (searchTerm.length <= 0) {
      updateReferenceReader(references);
      return;
    }
    references.forEach((ref) => {
      if (filterOption == 1){   // search across All
        if (Object.keys(ref).indexOf('Abstract') > -1){
          let titleRefs = -1;
          let abstractRefs = -1;
          let authorRefs = -1;
          titleRefs = ref.Title.toLowerCase().indexOf(searchTerm);
          abstractRefs = ref.Abstract.toLowerCase().indexOf(searchTerm);
          authorRefs = ref.Authors.toLowerCase().indexOf(searchTerm);
          if (titleRefs !== -1 || abstractRefs !== -1 || authorRefs !== -1){
            searchReferences.push(ref);
          }
        }
      } else if (filterOption == 2){   // search by Title
        if (Object.keys(ref).indexOf('Title') > -1){
          if (ref.Title.toLowerCase().indexOf(searchTerm) !== -1){
            searchReferences.push(ref);
          }
        }
      } else if (filterOption == 3){   // search by Abstract
        if (Object.keys(ref).indexOf('Abstract') > -1){
          if (ref.Abstract.toLowerCase().indexOf(searchTerm) !== -1){
            searchReferences.push(ref);
          }
        }
      } else if (filterOption == 4){   // search by Author
        if (Object.keys(ref).indexOf('Authors') > -1){
          if (ref.Authors.toLowerCase().indexOf(searchTerm) !== -1){
            searchReferences.push(ref);
          }
        }
      }
    });

    // final line: always do the update
    updateReferenceReader(searchReferences);
  }

  function closeReader() {
    $reader.removeClass('open');
    $veil.removeClass('open');
    $reader.css({display: 'none'});
    $reader.html('');
    readerFocusTrap.deactivate();
  }

  /**
   * Shows the reader for the given row and column id lists.
   * @param rowIdList
   * @param colIdList
   */
  function toggleReader(rowIdList, colIdList) {
    const references = getFilteredReferences(rowIdList, colIdList);
    buildReferenceReader(rowIdList, colIdList);
    updateReferenceReader(references, true);
    const $readerFilter = $('.reader-filter');
    const $filterSelect = $('.filter-opts');

    // autofocus on the input on opening
    $reader.slideDown(function () {
      $("#searchFilter").focus();
    });

    $readerFilter.on('keydown', (e) => {
      doReaderSearch(e, references, $readerFilter, $filterSelect);
    });

    $filterSelect.on('change', (e) => {
      doReaderSearch(null, references, $readerFilter, $filterSelect);
    });

    $('.grouping-opts').on('change', (e) => {
      updateReferenceReader(references);
    });

    $('#RefSortOrder').on('change', (e) => {
      updateReferenceReader(references);
    });

    $('.reader-filter li').on('click', (e) => {
      const $target = $(e.currentTarget);
      const klass = $target.attr('class');
      const isParent = $target.data('parent');
      const newKlass = klass === 'checked' ? 'unchecked' : 'checked';
      $('#codeFilterClearButton').attr('disabled', false);

      $target
        .removeClass(klass)
        .addClass(newKlass);

      if (isParent) {
        $target.next().children().each((index, child) => {
          $(child)
            .removeClass('checked')
            .removeClass('unchecked')
            .addClass(newKlass);
        });
      } else {
        const parent = $target.parent().prev();
        $(parent)
          .removeClass('checked')
          .removeClass('unchecked')
          .addClass('indeterminate');
      }

      const newSelection = getReaderFilterSelection();
      const references = getFilteredReferences(newSelection.rows, newSelection.cols);

      updateReferenceReader(references);
    });

    $('#codeFilterClearButton').on('click', function() {
      const $filterItems = $('.reader-filter li');
      for (const item of $filterItems){
        const $item = $(item);
        if ($item.hasClass('unchecked')){
          // do nothing; already in target state
        } else if ($item.hasClass('checked')){
          $item.removeClass('checked');
          $item.addClass('unchecked');
        } else if ($item.hasClass('indeterminate')){
          $item.removeClass('indeterminate');
          $item.addClass('unchecked');
        }
      }

      $('#codeFilterClearButton').attr('disabled', true);
      const newSelection = getReaderFilterSelection();
      const references = getFilteredReferences(newSelection.rows, newSelection.cols);
      updateReferenceReader(references);
    });

    $reader.css({display: 'block'});
    $veil.addClass('open');
    $reader.addClass('open');
    readerFocusTrap.activate();

    // Close modal on escape key
    $reader.on('keydown', function (event) {
      if (event.key === "Escape") {
        closeReader();
      }
    });

    $('.close', $reader).on('click', () => {
      closeReader();
      $announce.html('View records closed.');
    });
  }

  /**
   * Creates the external url popup.
   * @param dataAxis
   * @param headerId
   * @param pageX
   * @param pageY
   */
  function createExtUrlPopup(dataAxis, headerId, pageX, pageY) {
    const filteredExtUrlAttr = externalURLedAttributes.filter(attr =>
      attr.AttributeId === headerId &&
      attr.AttributeName !== null &&
      attr.AttributeName !== undefined &&
      attr.AttributeName !== '' &&
      ((attr.AttributeDescription !== null &&
        attr.AttributeDescription !== undefined &&
        attr.AttributeDescription !== '') ||
        (attr.ExtURL !== undefined &&
        attr.ExtURL !== null &&
        attr.ExtURL !== '' &&
        attr.ExtType !== undefined &&
        attr.ExtType !== null &&
        attr.ExtType !== '')));
    const popupHasAttributeData = filteredExtUrlAttr.length > 0;

    if (!popupHasAttributeData) return false;

    let popUpHtml = '<div class="overlay-text" data-header-id="' + headerId + '" data-axis="' + dataAxis + '">';

    for (const attribute of filteredExtUrlAttr) {
      popUpHtml += '<h4>' + attribute.AttributeName + '</h4>';

      if (attribute.AttributeDescription !== null &&
        attribute.AttributeDescription !== undefined &&
        attribute.AttributeDescription.length > 0) {
        popUpHtml += '<p>' + attribute.AttributeDescription + '</p>';
      }

      if (attribute.ExtURL !== undefined &&
        attribute.ExtURL !== null &&
        attribute.ExtURL !== '' &&
        attribute.ExtType !== undefined &&
        attribute.ExtType !== null &&
        attribute.ExtType !== '') {
        popUpHtml += '<a class="overlay-link" href="' + attribute.ExtURL + '" target="_blank">' + attribute.ExtType + '</a>';
      }
    }

    popUpHtml += '<p class="text-center text-muted">(click to view records)</p></div>'

    $attributeTooltipContent.html(popUpHtml);
    $attributeTooltip.addClass('show');

    $('.close-tooltip').on('click', (e) => {
      $attributeTooltip.removeClass('show');
    });

    $attributeTooltipContent.on('click', (e) => {
      $attributeTooltip.removeClass('show');

      const $target = $('.overlay-text');
      const headerId = parseInt($target.data('header-id'));
      const axis = $target.attr('data-axis');

      if (axis === 'col') {
        toggleReader([], [headerId]);
      } else if (axis === 'row') {
        toggleReader([headerId], []);
      }
    });

    // dynamically calculate and adjust the elements position to always draw it within the window's bounds
    const overlayHeight = $attributeTooltip.height();
    const overlayWidth = $attributeTooltip.width();
    const pageHeight = $window.height();
    const pageWidth = $window.width();

    let top = pageY;
    let left = pageX;

    if ((overlayWidth + left) > pageWidth) {
      left -= overlayWidth - 10;
    } else {
      left += 10;
    }

    if ((overlayHeight + top) > pageHeight) {
      top -= overlayHeight - 10;
    } else {
      top += 10;
    }

    $attributeTooltip.css({
      top: top,
      left: left
    });

    return true;
  }

  /**
   * Handles the table click to show the reader dialog.
   */
  function handleTableClick() {
    //#region Header Click shows popup
    // when clicking the row (i.e - side headers)
    $('.clickable-row').on('click', (e) => {
      const $target = $(e.currentTarget);
      const headerId = $target.data('id');
      const createdPopup = createExtUrlPopup('row', headerId, e.pageX, e.pageY);

      if (!createdPopup) {
        toggleReader([headerId], []);
      }
    });

    // when clicking the column (i.e - top headers)
    $('.clickable-col').on('click', (e) => {
      const $target = $(e.currentTarget);
      const headerId = $target.data('id');
      const createdPopup = createExtUrlPopup('col', headerId, e.pageX, e.pageY);

      if (!createdPopup){
        toggleReader([], [headerId]);
      }
    });
    //#endregion

    $('.cell').on('click', (e) => {
      const $target = $(e.currentTarget);
      const totalCount = $target.data('totalCount');

      if (totalCount === 0) {
        return;
      }

      let colIds = $target.data('colid');
      let rowIds = $target.data('rowid');

      if (isNaN(colIds)) {
        colIds = colIds.split(',').map(id => parseInt(id));
      } else {
        colIds = [colIds]
      }

      if (isNaN(rowIds)) {
        rowIds = rowIds.split(',').map(id => parseInt(id));
      } else {
        rowIds = [rowIds]
      }

      toggleReader(rowIds, colIds);
    });
  }

  /**
   *  handle the about button click to show the about map reader dialog
   */
  function handleAboutClick() {
    $('.menu-about').on('click', e => {
      let aboutHtml = '<div class="title clearfix">' +
        '<button class="btn close" aria-label="Close about" >X</button> <span>About This Map</span>' +
        '</div>' +
        '<div class="content">' +
        '<div class="read">' + aboutContent + '</div>' +
        '</div>';

      $reader.html(aboutHtml);

      $('.close', $reader).on('click', () => {
        closeReader();
        $announce.html('About this map closed.');
      });

      $reader.css({display: 'block'});
      $veil.addClass('open');
      $reader.addClass('open');
      readerFocusTrap.activate();

      // Close modal on escape key
      $reader.on('keydown', function (event) {
        if (event.key === "Escape") {
          closeReader();
        }
      });

      $announce.html('About this map opened.');
    });
  }

  /**
   *  handle the accessibility button click to show the accessibility dialog
   */
  function handleAccessiblityClick() {
    $('.menu-accessibility').on('click', e => {
      $('.close', $accessibility).on('click', () => {
        $accessibility.removeClass('open');
        $veil.removeClass('open');
        $accessibility.css({display: 'none'});
        accessibilityFocusTrap.deactivate();
        $announce.html('Accessibility closed.');
      });

      $accessibility.css({display: 'block'});
      $veil.addClass('open');
      $accessibility.addClass('open');
      accessibilityFocusTrap.activate();

      // Close modal on escape key
      $accessibility.on('keydown', function (event) {
        if (event.key === "Escape") {
          $accessibility.removeClass('open');
          $veil.removeClass('open');
          $accessibility.css({display: 'none'});
          accessibilityFocusTrap.deactivate();
        }
      });

      $announce.html('Accessibility opened.');
    });
  }

  /**
   * Handles the study submission click to show the information on how to
   * submit a new study to the map.
   */
  function handleStudySubmissionClick() {
    $('.menu-studysubmit').on('click', e => {
      let studySubmitHtml =
      '<div class="title clearfix">' +
        '<button class="btn close" aria-label="Close submit a study" >X</button> <span> Submit a Study </span>' +
      '</div>' +
      '<div class="content">' +
        '<div class="read">' + studySubmissionContent + '</div>' +
      '</div>';

      $reader.html(studySubmitHtml);

      $('.close', $reader).on('click',() => {
        closeReader();
        $announce.html('Submit a study closed.')
      });

      $reader.css({display: 'block'});
      $veil.addClass('open');
      $reader.addClass('open');
      readerFocusTrap.activate();

      // Close modal on escape key
      $reader.on('keydown', function (event) {
        if (event.key === "Escape") {
          closeReader();
        }
      });

      $announce.html('Submit a study opened.');
    });
  }

  /**
   * Makes the headers hide/show.
   */
  function handleExpandClick() {
    const $menuExpand = $('.menu-expand');
    const $topTable = $('table', $topHead);
    const $sideTable = $('table', $sideHead);

    $menuExpand.on('click', e => {

      if ($topTable.find('.collapsed').length > 0 || $sideTable.find('.collapsed').length > 0) {
        alert('You cannot hide the headers when they are collapsed.');
        return;
      }

      $menuExpand.toggleClass('active');
      
      if ($menuExpand.hasClass('active')) {
        $topTable.height('auto');
        $sideTable.width('auto');
        $('.inactive-text').attr('aria-hidden', 'true');
        $('.active-text').attr('aria-hidden', 'false');
      } else {
        $topTable.height(topColHeight);
        $sideTable.width(sideColWidth);
        $('.inactive-text').attr('aria-hidden', 'false');
        $('.active-text').attr('aria-hidden', 'true');
      }

      $('.header, .header-can-hide').slideToggle(200, function () {
          adjustTable();
      });
    });
  }

  /**
   * Toggles fullscreen mode.
   */
  function handleFullscreenClick() {
    const $menuFullscreen = $('.menu-fullscreen');

    $menuFullscreen.on('click', e => {
      toggleFullScreen();
      $menuFullscreen.toggleClass('active');

      if ($menuFullscreen.hasClass('active')) {
        $('.inactive-text').attr('aria-hidden', 'true');
        $('.active-text').attr('aria-hidden', 'false');
      } else {
        $('.inactive-text').attr('aria-hidden', 'false');
        $('.active-text').attr('aria-hidden', 'true');
      }
    });
  }

  /**
   * Toggles reader mode.
   */
  function handleReaderClick() {
    const $menuReader = $('.menu-reader');

    const colIdList = csvData.rows[csvData.totalColDepth - 1].map(item => item.id)
    const rowIdList = []

    for (let i = csvData.totalColDepth; i < csvData.rows.length; i++) {
      const row = csvData.rows[i]
      rowIdList.push(row[row.length - 1].id)
    }

    $menuReader.on('click', e => {
      toggleReader(rowIdList, colIdList);
      $menuReader.toggleClass('active');
    });
  }

  /**
   * Toggles the column collapse.
   * @param miss
   * @param count
   */
  function toggleTopColCollapse(miss, count) {
    const max = miss + count;

    $('.top-head tr').each((rowIndex, row) => {
      if (rowIndex <= 1) return;
      let total = 0;

      $(row).children().each((colIndex, cell) => {
        const $cell = $(cell);
        const colSpan = parseInt($cell.attr('colspan'));
        total += colSpan;

        if (total > miss && total <= max) {
          if (total - 1 === miss) {
            $cell.toggleClass('first');
          }

          if (total - 1 === max) {
            $cell.toggleClass('last');
          }

          $cell.toggleClass('collapsed');
        } else if (total > max) {
          return false;
        }
      });
    });

    buildTableRows();
    adjustTable();

    $pivotBody.trigger('scroll');

    // Update data in the table.
    updateReferenceCounts();
    updateTableDataView();
  }

  /**
   * Toggles the row collapse.
   * @param miss
   * @param count
   */
  function toggleSideColCollapse(miss, count) {
    const max = miss + count;
    let total = 0;

    $('.side-head .level-2').each((index, cell) => {
      const $cell = $(cell);
      const rowSpan = parseInt($cell.attr('rowspan'));
      total += rowSpan;

      if (total > miss && total <= max) {
        if (total - 1 === miss) {
          $cell.toggleClass('first');
        }

        if (total - 1 === max) {
          $cell.toggleClass('last');
        }

        $cell.toggleClass('collapsed')
      } else if (total > max) {
        return false;
      }
    });

    total = 0;

    $('.side-head .level-3').each((index, cell) => {
      const $cell = $(cell);
      const rowSpan = parseInt($cell.attr('rowspan'));
      total += rowSpan;

      if (total > miss && total <= max) {
        if (total - 1 === miss) {
          $cell.toggleClass('first');
        }

        if (total - 1 === max) {
          $cell.toggleClass('last');
        }

        $cell.toggleClass('collapsed')
      } else if (total > max) {
        return false;
      }
    });

    buildTableRows();
    adjustTable();

    $pivotBody.trigger('scroll');

    // Update data in the table.
    updateReferenceCounts();
    updateTableDataView();
  }

  /**
   * Collapse all the rows and columns in the map.
   */
  function collapseAll(){
    if (collapseColumnHeaders)
    {
      const $colHeaders =  $('.level-1')
        .filter(function() {
          return this.colSpan > 1;
        });

      $colHeaders.each((index, cell) => {
        const $cell = $(cell);
        
        $cell.toggleClass('collapsed');
        const count = parseInt($cell.attr('colspan'));
        let miss = 0;

        let $prev = $cell.prev();

        while ($prev.length > 0) {
          try {
            miss += parseInt($prev.attr('colspan'))
            $prev = $prev.prev();
          }
          catch (e) {
            break;
          }
        }

        if (count > 0) {
            toggleTopColCollapse(miss, count);
        }
      });
    }

    if (collapseRowHeaders)
    {
      const $rowHeaders =  $('.level-1').filter(function() {
        return this.rowSpan > 1;
      });

      $rowHeaders.each((index, cell) => {
        const $cell = $(cell);
        $cell.toggleClass('collapsed');

        const count = parseInt($cell.attr('rowspan'));
        let miss = 0;
        $('.side-head .level-1').each((index, otherCell) => {
          const $otherCell = $(otherCell);

          if ($otherCell.text() !== $cell.text()) {
            miss += parseInt($otherCell.attr('rowspan'));
          } else {
            return false;
          }
        });

        if (count > 0) {
            toggleSideColCollapse(miss, count);
        }
      });
    }
  }

  /**
   * Toggles collapsing columns and rows.
   */
  function handleTableRowColCollapse() {
    // Manage column collapse.
    $('.btnColCollapse').on('click', e => {
      const $target = $(e.currentTarget);
      const $cell = $target.parent().parent();
      $cell.toggleClass('collapsed');
      $cell.toggleClass('busy');

      const count = parseInt($cell.attr('colspan'));
      let miss = 0;

      let $prev = $cell.prev();

      while ($prev.length > 0) {
        try {
          miss += parseInt($prev.attr('colspan'))
          $prev = $prev.prev();
        }
        catch (e) {
          break;
        }
      }

      if (count > 0) {
        setTimeout(() => {
          toggleTopColCollapse(miss, count);
          $cell.toggleClass('busy');
        }, 50);
      } else {
        $cell.toggleClass('busy');
      }
    });

    // Manage row collapse.
    $('.btnRowCollapse').on('click', e => {
      const $target = $(e.currentTarget);
      const $cell = $target.parent().parent();
      $cell.toggleClass('collapsed');
      $cell.toggleClass('busy');

      const count = parseInt($cell.attr('rowspan'));
      let miss = 0;

      $('.side-head .level-1').each((index, cell) => {
        const $otherCell = $(cell);

        if ($otherCell.text() !== $cell.text()) {
          miss += parseInt($otherCell.attr('rowspan'));
        } else {
           return false;
        }
      });

      if (count > 0) {
        setTimeout(() => {
          toggleSideColCollapse(miss, count);
          $cell.toggleClass('busy');
        }, 50);
      } else {
        $cell.toggleClass('busy');
      }
    });
  }

  /**
   * Adjusts the table as the window is resized.
   */
  function adjustTable() {
    const bodyWidth = $pivotTable.width() - $sideHead.width();
    const topHeadWrapperCssHeight = $topHead.height();
    const topHeadWrapperCssPaddingLeft = $sideHead.width();
    const topHeadCssWidth = bodyWidth;
    const bodyCssWidth = bodyWidth - 1;
    let sideHeadCssHeight = $window.height() - $topHead.height() - $footer.height() - $menu.height() - 48;
    let bodyCssHeight = sideHeadCssHeight;

    $header = $('.header')

    if ($header.length > 0 && $header.css('display') !== 'none') {
      sideHeadCssHeight = sideHeadCssHeight - $header.height() - 8;
      bodyCssHeight = sideHeadCssHeight;
    }

    $topHeadWrapper.css({
      'height': topHeadWrapperCssHeight,
      'padding-left': topHeadWrapperCssPaddingLeft
    });

    $topHead.css({
      'width': topHeadCssWidth
    });

    $topHeadTable.css({
      'margin-left': '0px'
    });

    $sideHead.css({
      'height': sideHeadCssHeight
    });

    $sideHeadTable.css({
      'margin-top': '0px'
    });

    $pivotBody.css({
      overflow: 'scroll',
      width: bodyCssWidth,
      height: bodyCssHeight
    });

    $pivotBody.scroll(function (e) {
      $topHeadTable.css({
        'margin-left': e.target.scrollLeft * -1
      });

      $sideHeadTable.css({
        'margin-top': e.target.scrollTop * -1
      });
    });
  }

  function UpdateRecordCount() {
    if (!showRecordCount) return;

    const colIdList = csvData.rows[csvData.totalColDepth - 1].map(item => item.id)
    const rowIdList = []

    for (let i = csvData.totalColDepth; i < csvData.rows.length; i++) {
      const row = csvData.rows[i]
      rowIdList.push(row[row.length - 1].id)
    }

    const references = getFilteredReferences(rowIdList, colIdList);
    $recordCount.html('(' + references.length + ')');
  }

  // Call all the methods to initialize the page.
  createSettingsPanel();
  setupStyleSwitch();
  buildTable();
  adjustTable();
  buildLegend();
  handleExpandClick();
  handleFullscreenClick();
  handleReaderClick();
  handleTableRowColCollapse();
  handleAccessiblityClick();

  // Update the record count on first load.
  UpdateRecordCount();

  if(collapseColumnHeaders || collapseRowHeaders) {
    collapseAll();
  }

  if (aboutContent.trim().length === 0) {
    $('.menu-about').hide();
  }
  else {
    handleAboutClick();

    if (aboutPopup) {
      $('.menu-about').trigger('click');
    }
  }

  if (studySubmissionContent.trim().length === 0) {
    $('.menu-studysubmit').hide();
  } else {
    handleStudySubmissionClick();
  }

  // Show the loader.
  const $loader = $('.loader');
  const windowHeight = $window.height() + 200;

  $window.resize(adjustTable);

  // Slide the up after all is done.
  $loader
    .css({
      'top': -windowHeight,
      'bottom': windowHeight
    });

  // Removes the loader after the animation is complete.
  setTimeout(() => {
    $loader.remove();
  }, 2000);
});

  </script>
  <script src="https://unpkg.com/@popperjs/core@2"></script>
  <script src="https://unpkg.com/tippy.js@6"></script>
  <script>
    tippy('[data-tippy-content]');
  </script>
</body>

</html>